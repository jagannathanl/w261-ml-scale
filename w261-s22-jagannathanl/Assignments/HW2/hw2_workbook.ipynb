{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2 - Naive Bayes in Hadoop MR\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Fall 2018`__\n",
    "\n",
    "In the live sessions for week 2 and week 3 you got some practice designing and debugging Hadoop Streaming jobs. In this homework we'll use Hadoop MapReduce to implement your first parallelized machine learning algorithm: Naive Bayes. As you develop your implementation you'll test it on a small dataset that matches the 'Chinese Example' in the _Manning, Raghavan and Shutze_ reading for Week 2. For the main task in this assignment you'll be working with a small subset of the Enron Spam/Ham Corpus. By the end of this assignment you should be able to:\n",
    "* __... describe__ the Naive Bayes algorithm including both training and inference.\n",
    "* __... perform__ EDA on a corpus using Hadoop MR.\n",
    "* __... implement__ parallelized Naive Bayes.\n",
    "* __... constrast__ partial, unordered and total order sort and their implementations in Hadoop Streaming.\n",
    "* __... explain__ how smoothing affects the bias and variance of a Multinomial Naive Bayes model.\n",
    "\n",
    "As always, your work will be graded both on the correctness of your output and on the clarity and design of your code. __Please refer to the `README` for homework submission instructions.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "Before starting, run the following cells to confirm your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/Assignments/HW2\n"
     ]
    }
   ],
   "source": [
    "%cd /media/notebooks/Assignments/HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global vars (paths) - ADJUST AS NEEDED\n",
    "#JAR_FILE = \"/usr/lib/hadoop-mapreduce/hadoop-streaming.jar\"\n",
    "JAR_FILE = \"/usr/lib/hadoop/hadoop-streaming-3.2.2.jar\"\n",
    "HDFS_DIR = \"/user/root/HW2/\"\n",
    "HOME_DIR = \"/media/notebooks/Assignments/HW2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save path for use in Hadoop jobs (-cmdenv PATH={PATH})\n",
    "from os import environ\n",
    "PATH  = environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data path\n",
    "ENRON = \"data/enronemail_1h.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "drwxr-xr-x   - root hadoop          0 2022-01-22 23:35 .sparkStaging\n",
      "drwxr-xr-x   - root hadoop          0 2022-01-30 00:39 HW2\n",
      "drwxr-xr-x   - root hadoop          0 2022-01-19 03:17 demo2\n",
      "drwxr-xr-x   - root hadoop          0 2022-01-19 22:23 demo3\n",
      "mkdir: `/user/root/HW2': File exists\n"
     ]
    }
   ],
   "source": [
    "# make the HDFS directory if it doesn't already exist\n",
    "!hdfs dfs -ls \n",
    "!hdfs dfs -mkdir {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Hadoop MapReduce Key Takeaways.  \n",
    "\n",
    "This assignment will be the only one in which you use Hadoop Streaming to implement a distributed algorithm. The key reason we continue to teach Hadoop streaming is because of the way it forces the programmer to think carefully about what is happening under the hood when you parallelize a calculation. This question will briefly highlight some of the most important concepts that you need to understand about Hadoop Streaming and MapReduce before we move on to Spark next week.   \n",
    "\n",
    "### Q1 Tasks:\n",
    "\n",
    "* __a) short response:__ What \"programming paradigm\" is Hadoop MapReduce based on? What are the main ideas of this programming paradigm and how does MapReduce exemplify these ideas?\n",
    "\n",
    "* __b) short response:__ What is the Hadoop Shuffle? When does it happen? Why is it potentially costly? Describe one specific thing we can we do to mitigate the cost associated with this stage of our Hadoop Streaming jobs.\n",
    "\n",
    "* __c) short response:__ In Hadoop Streaming why do the input and output record format of a combiner script have to be the same? [__`HINT`__ _what level of combining does the framework guarantee? what is the relationship between the record format your mapper emits and the format your reducer expects to receive?_]\n",
    "\n",
    "* __d) short response:__ To what extent can you control the level of parallelization of your Hadoop Streaming jobs? Please be specific.\n",
    "\n",
    "* __e) short response:__ What change in the kind of computing resources available prompted the creation of parallel computation frameworks like Hadoop? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 Student Answers:\n",
    "\n",
    "> __a)__ Hadoop MapReduce paradigm is based out of the following 3 concepts: (1) Functional Programming, (2) Mappers and Reducers, and (3) Execution Framework as discussed in Lin and Dyer book, chapter 2 MapReduce Basics.  \n",
    "\n",
    "> __b)__ After the Map phase and before the beginning of the Reduce phase there is a handoff process, known as shuffle and sort. Shuffle and sort step performs the important tasks of routing, collating records with the same key, and transporting these records from the mapper is prepared and moved to the nodes where the reducer tasks are run. This step can become potentially expensive as the data is transported through the networking layer and also because same keys can be emitted several times by the mappers and if they were not consolidated will take up the network bandwidth unnecessarily. To improve overall efficiency, records from mapper output are sent to the physical node that a reducer will be running on as they are being produced - to avoid flooding the network when all mapper tasks are complete. Combiners can be used to consolidate the records with the same key to a minimum so that they dont flood the network. So, combiners that allow for local aggregation can serve as a optimization step in MapReduce. MapReduce does not use combiners by default.   \n",
    "\n",
    "> __c)__  In any MapReduce program, the reducer input key-value type must match the mapper output key-value type: this implies that the combiner input and output key-value types must match the mapper output key-value type (which is the same as the reducer input key-value type). As mentioned above, combiner is an optional component so when we write reducers that are compatible to the combiner will break when the combiners were not used by the framework. So, it is necessary to match the input and output record format of the combiner.\n",
    "\n",
    "> __d)__  Unlike with a traditional HPC job, the level of parallelism in a Hadoop job is adjustable. The number of map tasks is ultimately determined by the nature of your input data due to how HDFS distributes chunks of data to your mappers. The number of reducers is determined by the scale of the data and desired time to complete the processing. With Hadoop, we can \"suggest\" a number of mappers when we submit the job. Hadoop tries to honors the number of reduce jobs mentioned in the command line more strictly. The following are the parameters that we typically pass to the Hadoop at the command line to control the number of mappers and reducers: \"-D mapred.map.tasks=4\", \"-D mapred.job.reducers=4\".  \n",
    "\n",
    "> __e)__ The manner in which the semiconductor industry had been exploiting Moore’s Law simply ran out of opportunities for improvement: faster clocks, deeper pipelines, superscalar architectures, and other tricks of the trade reached a point of diminishing returns that did not justify continued investment. As a result, the performance of new generated CPUs didn't dramatically increase as it did in the past. This marked the beginning of an entirely new strategy and the dawn of the multi-core era. In this backdrop, widespread need for data-intensive processing, flooding of commodity computing platforms and the plummeeting storage costs drove innovations in distributed computing such as MapReduce—first by Google, and then by Yahoo and the open source community. This in turn created more demand: when organizations learned about the availability of effective data analysis tools for large datasets, they began instrumenting various business processes to gather even more data—driven by the belief that more data leads to deeper insights and greater competitive advantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: MapReduce Design Patterns.  \n",
    "\n",
    "In the last two live sessions and in your readings from Lin & Dyer you encountered a number of techniques for manipulating the logistics of a MapReduce implementation to ensure that the right information is available at the right time and location. In this question we'll review a few of the key techniques you learned.   \n",
    "\n",
    "### Q2 Tasks:\n",
    "\n",
    "* __a) short response:__ What are counters (in the context of Hadoop Streaming)? How are they useful? What kinds of counters does Hadoop provide for you? How do you create your own custom counter?\n",
    "\n",
    "* __b) short response:__ What are composite keys? How are they useful? How are they related to the idea of custom partitioning?\n",
    "\n",
    "* __c) short response:__ What is the order inversion pattern? What problem does it help solve? How do we implement it? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Student Answers:\n",
    "\n",
    "> __a)__ There are two types of Hadoop MapReduce Counters: (1) Built-in Counters, and (2) User-Defined Counters/Custom counters. Hadoop maintains some built-in Hadoop counters for every job and these report various metrics, like, there are counters for the number of bytes and records, which allow us to confirm that the expected amount of input is consumed and the expected amount of output is produced. For e.g. (1) MapReduce Task Counter, (2) FileSystem Counters, (3) FileInput Format Counter, (4) Job Counter to name a few. Hadoop also allows user code to define a set of counters, which are then incremented as desired in the mapper or reducer. These are called user-defined counters.\n",
    "\n",
    "> __b)__ Composite keys are keys that are made up of two or more number of fields that can be used for sequencing or ordering records. By emitting appropriate values for these keys we can control the flow of data in the sort and shuffle part of the MapReduce framework. But if we used multiple reducers, a default partitioner would use the CompositeKey and would assign it to a reducer. With a custom partitioner, we will be able to control the flow by grouping (key1, key2, ...,value) tuples with  same “state” and send it to a same reducer. \n",
    " \n",
    "\n",
    "> __c)__ The order inversion pattern exploits the sorting phase of MapReduce to push data needed for calculations to the reducer ahead of the data that will be manipulated. Often, a reducer needs to compute an aggregate statistic on a set of elements before individual elements can be processed. Normally, this would require two passes over the data, but with the “order inversion” design pattern, the aggregate statistic can be computed in the reducer before the individual elements are encountered. We can add some special characters to the key (e.g. !, *, etc) such that it gets sorted ahead of other elements in the list.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Understanding Total Order Sort\n",
    "\n",
    "The key challenge in distributed computing is to break a problem into a set of sub-problems that can be performed without communicating with each other. Ideally, we should be able to define an arbirtary number of splits and still get the right result, but that is not always possible. Parallelization becomes particularly challenging when we need to make comparisons between records, for example when sorting. Total Order Sort allows us to order large datasets in a way that enables efficient retrieval of results. Before beginning this assignment, make sure you have read and understand the [Total Order Sort Notebook](https://github.com/UCB-w261/main/tree/master/HelpfulResources/TotalSortGuide/_total-sort-guide-spark2.01-JAN27-2017.ipynb). You can skip the first two MRJob sections, but the rest of section III and all of section IV are **very** important (and apply to Hadoop Streaming) so make sure to read them closely. Feel free to read the Spark sections as well but you won't be responsible for that material until later in the course. To verify your understanding, answer the following questions.\n",
    "\n",
    "### Q3 Tasks:\n",
    "\n",
    "* __a) short response:__ What is the difference between a partial sort, an unordered total sort, and a total order sort? From the programmer's perspective, what does total order sort allow us to do that we can't with unordered total? Why is this important with large datasets?\n",
    "\n",
    "* __b) short response:__ Which phase of a MapReduce job is leveraged to implement Total Order Sort? Which default behaviors must be changed. Why must they be changed?\n",
    "\n",
    "* __c) short response:__ Describe in words how to configure a Hadoop Streaming job for the custom sorting and partitioning that is required for Total Order Sort.  \n",
    "\n",
    "* __d) short response:__ Explain why we need to use an inverse hash code function.\n",
    "\n",
    "* __e) short response:__ Where does this function need to be located so that a Total Order Sort can be performed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Student Answers: \n",
    "\n",
    "> __a)__  \n",
    "<div style=\"color: black;\">\n",
    "The bulleted points below describe the partial sort, unordered total sort, and ordered total sort respectively. \n",
    "            <ol type=\"a\">\n",
    "                <li>In partial sort, keys are assigned to buckets without any ordering. Keys are sorted within each bucket.</li>\n",
    "                <li>In unordered total sort, Keys are assigned to buckets according to their numeric value and buckets are not assigned in sorted order.</li>\n",
    "                <li>In ordered total sort, Keys are assigned to buckets according to their numeric value and buckets are assigned in the sorted order.  </li>\n",
    "            </ol>\n",
    "In unordered total sort, buckets are not assigned in that order. In ordered total sort the buckets are as assigned in that order. So in the case of unordered total sort, we have to make sure to order the buckets after the reducer stage. This can be a big deal when the data is huge. We need to run a post processing job to accomplish this ordering. \n",
    "\n",
    "</div>\n",
    "\n",
    "> __b)__ \n",
    "<div style=\"color: black;\">\n",
    "The following components have to be changed to implement the Total Order Sort: \n",
    "            <ol type=\"a\">\n",
    "                <li>The mapper will have to generate inverse hash map key to identify the partition.</li>\n",
    "                <li>The final reducer should drop the partition key so that the output doesn't include the partition key.</li>\n",
    "                <li>We will have to specify the partition and sorting key to customize the streaming job.</li>\n",
    "            </ol>\n",
    "\n",
    "</div>\n",
    "\n",
    "> __c)__ \n",
    "<div style=\"color: black;\">\n",
    "As a first step, we customize the delimiter, number of keys. The following sample illustrate how to set the number of fields and tab as a delimiter:\n",
    "            <ul type=\"a\">\n",
    "                <li>-D stream.num.map.output.key.fields=3</li>\n",
    "                <li>-D streamm.map.output.field.separator=\"\\t\"</li>\n",
    "            </ul>\n",
    "We then customize the sorting key by sepecifying the primary and secondary keys, format of those keys (e.g. numeric or non-numeric), and the order of the sorting if needed (ascending or descending). The following example illustrates how to set the non-numeric first field as the primary key and the second numeric field as the secondary key to be used.\n",
    "            <ul type=\"a\">\n",
    "                <li>-D mapreduce.partition.keycomparator.options=-\"-k1,1 k2.2nr\"</li>\n",
    "            </ul>\n",
    "We can then specify the partition key as follows.\n",
    "            <ul type=\"a\">\n",
    "                <li>-D mapreduce.partition.keypartitioner.options=-\"-k1,1</li>\n",
    "            </ul>\n",
    "We can also customize the number of reducers as follows. There are several comprehensive hadoop stream jobs in this notebook. The example in for problem 4(e) will serve as good example for hadoop streaming.\n",
    "            <ul type=\"a\">\n",
    "                <li>mapreduce.job.reduces=3</li>\n",
    "            </ul>\n",
    "</div>\n",
    "\n",
    "> __d)__ \n",
    "<div style=\"fcolor: black;\"> Hadoop by default assigns the partition key to each record. Hadoop shuffle routes the data to the reducer using this partition key. If in case we need to modify the final output ordering we need to assign our own partition keys to get desired routing. Inverse hash code function/file is the technique with which we assign custom partition keys to control the way it is partitioned and routed. For e.g. partition keys like 'A', 'B', 'C' are sorted as 'B', 'C', 'A' by hadoop. If we want to customize this ordering it is imperitive to reverse engineer this hashing to accomplish our ordering. This approach of reverse engineering this ordering is called inverse hash coding.</div>  \n",
    "\n",
    "> __e)__ \n",
    "<div style=\"color: black;\"> So, in the case of total order sort, each bucket is sorted and buckets are expected to be in a specific order. We exploit the inverse hash code function to achieve this ordering. So, typically the inverse hash function is  located in the mapper so that the output records contain the partition key in them.</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the Data\n",
    "For the main task in this portion of the homework you will train a classifier to determine whether an email represents spam or not. You will train your Naive Bayes model on a 100 record subset of the Enron Spam/Ham corpus available in the HW2 data directory (__`HW2/data/enronemail_1h.txt`__).\n",
    "\n",
    "__Source:__   \n",
    "The original data included about 93,000 emails which were made public after the company's collapse. There have been a number raw and preprocessed versions of this corpus (including those available [here](http://www.aueb.gr/users/ion/data/enron-spam/index.html) and [here](http://www.aueb.gr/users/ion/publications.html)). The subset we will use is limited to emails from 6 Enron employees and a number of spam sources. It is part of [this data set](http://www.aueb.gr/users/ion/data/enron-spam/) which was created by researchers working on personlized Bayesian spam filters. Their original publication is [available here](http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf). __`IMPORTANT!`__ _For this homework please limit your analysis to the 100 email subset which we provide. No need to download or run your analysis on any of the original datasets, those links are merely provided as context._\n",
    "\n",
    "__Preprocessing:__  \n",
    "For their work, Metsis et al. (the authors) appeared to have pre-processed the data, not only collapsing all text to lower-case, but additionally separating \"words\" by spaces, where \"words\" unfortunately include punctuation. As a concrete example, the sentence:  \n",
    ">  `Hey Jon, I hope you don't get lost out there this weekend!`  \n",
    "\n",
    "... would have been reduced by Metsis et al. to the form:  \n",
    "> `hey jon , i hope you don ' t get lost out there this weekend !` \n",
    "\n",
    "... so we have reverted the data back toward its original state, removing spaces so that our sample sentence would now look like:\n",
    "> `hey jon, i hope you don't get lost out there this weekend!`  \n",
    "\n",
    "Thus we have at least preserved contractions and other higher-order lexical forms. However, one must be aware that this reversion is not complete, and that some object (specifically web sites) will be ill-formatted, and that all text is still lower-cased.\n",
    "\n",
    "\n",
    "__Format:__   \n",
    "All messages are collated to a tab-delimited format:  \n",
    "\n",
    ">    `ID \\t SPAM \\t SUBJECT \\t CONTENT \\n`  \n",
    "\n",
    "where:  \n",
    ">    `ID = string; unique message identifier`  \n",
    "    `SPAM = binary; with 1 indicating a spam message`  \n",
    "    `SUBJECT = string; title of the message`  \n",
    "    `CONTENT = string; content of the message`   \n",
    "    \n",
    "Note that either of `SUBJECT` or `CONTENT` may be \"NA\", and that all tab (\\t) and newline (\\n) characters have been removed from both of the `SUBJECT` and `CONTENT` columns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/Assignments/HW2\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t christmas tree farm pictures\tNA\n",
      "0001.1999-12-10.kaminski\t0\t re: rankings\t thank you.\n",
      "0001.2000-01-17.beck\t0\t leadership development pilot\t\" sally:  what timing, ask and you shall receiv\n",
      "0001.2000-06-06.lokay\t0\t\" key dates and impact of upcoming sap implementation over the next few week\n",
      "0001.2001-02-07.kitchen\t0\t key hr issues going forward\t a) year end reviews-report needs generating \n"
     ]
    }
   ],
   "source": [
    "# take a look at the first 100 characters of the first 5 records (RUN THIS CELL AS IS)\n",
    "!head -n 5 /media/notebooks/Assignments/HW2/{ENRON} | cut -c-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 /media/notebooks/Assignments/HW2/data/enronemail_1h.txt\n"
     ]
    }
   ],
   "source": [
    "# see how many messages/lines are in the file \n",
    "#(this number may be off by 1 if the last line doesn't end with a newline)\n",
    "!wc -l /media/notebooks/Assignments/HW2/{ENRON}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/user/root/HW2': File exists\n"
     ]
    }
   ],
   "source": [
    "# make the HDFS directory if it doesn't already exist\n",
    "!hdfs dfs -mkdir {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/user/root/HW2/enron.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "# load the data into HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -copyFromLocal /media/notebooks/Assignments/HW2/{ENRON} {HDFS_DIR}/enron.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 items\n",
      "-rw-r--r--   1 root hadoop     254300 2022-01-30 00:39 /user/root/HW2/NBmodel.txt\n",
      "drwxr-xr-x   - root hadoop          0 2022-01-29 23:10 /user/root/HW2/chinese-output\n",
      "drwxr-xr-x   - root hadoop          0 2022-01-29 19:06 /user/root/HW2/chinese-output-smooth\n",
      "-rw-r--r--   1 root hadoop        119 2022-01-23 00:41 /user/root/HW2/chineseTest.txt\n",
      "-rw-r--r--   1 root hadoop        107 2022-01-23 00:41 /user/root/HW2/chineseTrain.txt\n",
      "drwxr-xr-x   - root hadoop          0 2022-01-30 00:39 /user/root/HW2/custom-partition\n",
      "drwxr-xr-x   - root hadoop          0 2022-01-29 18:12 /user/root/HW2/eda-output\n",
      "drwxr-xr-x   - root hadoop          0 2022-01-29 18:11 /user/root/HW2/eda-sort-output\n",
      "drwxr-xr-x   - root hadoop          0 2022-01-29 19:09 /user/root/HW2/enron-model\n",
      "-rw-r--r--   1 root hadoop     204559 2022-01-21 02:30 /user/root/HW2/enron.txt\n",
      "-rw-r--r--   1 root hadoop      41493 2022-01-28 02:47 /user/root/HW2/enron_test.txt\n",
      "-rw-r--r--   1 root hadoop     163066 2022-01-28 02:47 /user/root/HW2/enron_train.txt\n",
      "drwxr-xr-x   - root hadoop          0 2022-01-28 23:49 /user/root/HW2/eval-unsmoothed-model\n",
      "drwxr-xr-x   - root hadoop          0 2022-01-29 19:14 /user/root/HW2/evaluate-smoothed-model\n",
      "drwxr-xr-x   - root hadoop          0 2022-01-29 19:13 /user/root/HW2/evaluate-unsmoothed-model\n",
      "drwxr-xr-x   - root hadoop          0 2022-01-29 19:11 /user/root/HW2/smooth-model\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4:  Enron Ham/Spam EDA.\n",
    "Before building our classifier, lets get aquainted with our data. In particular, we're interested in which words occur more in spam emails than in legitimate (\"ham\") emails. In this question you'll implement two Hadoop MapReduce jobs to count and sort word occurrences by document class. You'll also learn about two new Hadoop streaming parameters that will allow you to control how the records output from your mappers are partitioned for reducing on separate nodes. \n",
    "\n",
    "__`IMPORTANT NOTE:`__ For this question and all subsequent items, you should include both the subject and the body of the email in your analysis (i.e. concatetate them to get the 'text' of the document).\n",
    "\n",
    "### Q4 Tasks:\n",
    "* __a) code:__ Complete the missing components of the code in __`EnronEDA/mapper.py`__ and __`EnronEDA/reducer.py`__ to create a Hadoop  MapReduce job that counts how many times each word in the corpus occurs in an email for each class. Pay close attention to the data format specified in the docstrings of these scripts _-- there are a number of ways to accomplish this task, we've chosen this format to help illustrate a technique in `part e`_. Run the provided unit tests to confirm that your code works as expected, then run the provided Hadoop Streaming command to apply your analysis to the Enron data.\n",
    "\n",
    "\n",
    "* __b) code + short response:__ How many times does the word \"__assistance__\" occur in each class? (`HINT:` Use a `grep` command to read from the results file you generated in '`a`' and then report the answer in the space provided.)\n",
    "\n",
    "\n",
    "* __c) short response:__ Would it have been possible to add some sorting parameters to the Hadoop streaming command that would cause our `part a` results to be sorted by count? Explain why or why not. (`HINT:` This question demands an understanding of the sequence of the phases of MapReduce.)\n",
    "\n",
    "\n",
    "* __d) code + short response:__ Write a second Hadoop MapReduce job to sort the output of `part a` first by class and then by count. Run your job and save the results to a local file. Then describe in words how you would go about printing the top 10 words in each class given this sorted output. (`HINT 1:` _remember that you can simply pass the `part a` output directory to the input field of this job; `HINT 2:` since this task is just reodering the records from `part a` we don't need to write a mapper or reducer, just use `/bin/cat` for both_)\n",
    "\n",
    "\n",
    "* __e) code:__ A more efficient alternative to '`grep`-ing' for the top 10 words in each class would be to use the Hadoop framework to separate records from each class into its own partition so that we can just read the top lines in each. Rewrite your job from ` part d` to specify 2 reduce tasks and to tell Hadoop to partition based on the second field (which indicates spam/ham in our data). Your code should maintain the secondary sort -- that is each partition should list words from most to least frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 Student Answers:\n",
    "> __b)__ The word assistance appeared 8 times in the class \"spam\" and appeared 2 times in the class \"ham\"\n",
    "\n",
    "> __c)__ No, it is not possible to sort the **part a** output by streaming the data in hadoop. The problem in **part a** is a word counting problem. As the final word count is not known until the reducer sums up all the partial counts. So, we need to route the word count records from this hadoop streaming job to another hadoop streaming process to sort the word count records by the count. \n",
    "\n",
    "> __d)__ This question has two parts to it. (1) Sorting the output from part a: The output from part a has 3 fields per record (word, class, count). We can then stream the output through Hadoop using unix /bin/cat and configuring the shuffle and sort layer to sort by the primary key the class and secondary key the count to accomplish the sorting. (Pleae look the code at the cell dedicated for this section below) (2) How to list top 10 items in each class: As we partitioned the data by the class. The reducer output from reducer 1 and reducer 2 will be in hdfs output directory in parttion 1 and partion 2 respectively. We can pipe the \"!hdfs dfs -cat\" to unix \"head\" to peek at the top 10 items from each partition sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - do your work in the provided scripts then RUN THIS CELL AS IS\n",
    "!chmod a+x EnronEDA/mapper.py\n",
    "!chmod a+x EnronEDA/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\t1\t1\n",
      "body\t1\t1\n",
      "title\t0\t1\n",
      "body\t0\t1\n"
     ]
    }
   ],
   "source": [
    "# part a - unit test EnronEDA/mapper.py (RUN THIS CELL AS IS)\n",
    "!echo -e \"d1\t1\ttitle\tbody\\nd2\t0\ttitle\tbody\" | EnronEDA/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\t1\t1\n",
      "one\t0\t2\n",
      "two\t1\t0\n",
      "two\t0\t1\n"
     ]
    }
   ],
   "source": [
    "# part a - unit test EnronEDA/reducer.py (RUN THIS CELL AS IS)\n",
    "!echo \"one\t1\t1\\none\t0\t1\\none\t0\t1\\ntwo\t0\t1\" | EnronEDA/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/eda-output\n"
     ]
    }
   ],
   "source": [
    "# part a - clear output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/eda-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.2.2.jar] /tmp/streamjob2860073544150334342.jar tmpDir=null\n",
      "2022-01-30 23:30:43,191 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.128.0.2:8032\n",
      "2022-01-30 23:30:43,444 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.128.0.2:10200\n",
      "2022-01-30 23:30:43,903 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.128.0.2:8032\n",
      "2022-01-30 23:30:43,904 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.128.0.2:10200\n",
      "2022-01-30 23:30:44,096 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1643556265243_0001\n",
      "2022-01-30 23:30:44,989 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2022-01-30 23:30:45,050 INFO mapreduce.JobSubmitter: number of splits:9\n",
      "2022-01-30 23:30:45,195 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1643556265243_0001\n",
      "2022-01-30 23:30:45,197 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-01-30 23:30:45,441 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-01-30 23:30:45,441 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-01-30 23:30:45,872 INFO impl.YarnClientImpl: Submitted application application_1643556265243_0001\n",
      "2022-01-30 23:30:45,992 INFO mapreduce.Job: The url to track the job: http://w261-m:8088/proxy/application_1643556265243_0001/\n",
      "2022-01-30 23:30:46,004 INFO mapreduce.Job: Running job: job_1643556265243_0001\n",
      "2022-01-30 23:30:56,134 INFO mapreduce.Job: Job job_1643556265243_0001 running in uber mode : false\n",
      "2022-01-30 23:30:56,135 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-01-30 23:31:03,270 INFO mapreduce.Job:  map 22% reduce 0%\n",
      "2022-01-30 23:31:04,275 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2022-01-30 23:31:10,344 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2022-01-30 23:31:16,395 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2022-01-30 23:31:23,441 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "2022-01-30 23:31:24,447 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-01-30 23:31:26,462 INFO mapreduce.Job: Job job_1643556265243_0001 completed successfully\n",
      "2022-01-30 23:31:26,550 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=369010\n",
      "\t\tFILE: Number of bytes written=3469030\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=258608\n",
      "\t\tHDFS: Number of bytes written=119173\n",
      "\t\tHDFS: Number of read operations=37\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=9\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=9\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=154568256\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=27826452\n",
      "\t\tTotal time spent by all map tasks (ms)=48976\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8817\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=48976\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8817\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=154568256\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=27826452\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=31490\n",
      "\t\tMap output bytes=306018\n",
      "\t\tMap output materialized bytes=369106\n",
      "\t\tInput split bytes=801\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5065\n",
      "\t\tReduce shuffle bytes=369106\n",
      "\t\tReduce input records=31490\n",
      "\t\tReduce output records=10130\n",
      "\t\tSpilled Records=62980\n",
      "\t\tShuffled Maps =18\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=18\n",
      "\t\tGC time elapsed (ms)=1671\n",
      "\t\tCPU time spent (ms)=10410\n",
      "\t\tPhysical memory (bytes) snapshot=5205172224\n",
      "\t\tVirtual memory (bytes) snapshot=48699236352\n",
      "\t\tTotal committed heap usage (bytes)=5093457920\n",
      "\t\tPeak Map Physical memory (bytes)=549830656\n",
      "\t\tPeak Map Virtual memory (bytes)=4428320768\n",
      "\t\tPeak Reduce Physical memory (bytes)=284397568\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4430544896\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=257807\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=119173\n",
      "2022-01-30 23:31:26,550 INFO streaming.StreamJob: Output directory: /user/root/HW2//eda-output\n"
     ]
    }
   ],
   "source": [
    "# part a - Hadoop streaming job (RUN THIS CELL AS IS)\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files EnronEDA/reducer.py,EnronEDA/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -input {HDFS_DIR}/enron.txt \\\n",
    "  -output {HDFS_DIR}/eda-output \\\n",
    "  -numReduceTasks 2 \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - retrieve results from HDFS & copy them into a local file (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/eda-output/part-0000* > EnronEDA/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t1\t8\n",
      "assistance\t0\t2\n"
     ]
    }
   ],
   "source": [
    "# part b - write your grep command here\n",
    "!hdfs dfs -cat {HDFS_DIR}/eda-output/part-0000* | grep 'assistance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/HW2//eda-sort-output': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# part d - clear the output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/eda-sort-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.2.2.jar] /tmp/streamjob2457006673226946770.jar tmpDir=null\n",
      "2022-01-30 23:34:24,183 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.128.0.2:8032\n",
      "2022-01-30 23:34:24,392 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.128.0.2:10200\n",
      "2022-01-30 23:34:24,931 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.128.0.2:8032\n",
      "2022-01-30 23:34:24,931 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.128.0.2:10200\n",
      "2022-01-30 23:34:25,278 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1643556265243_0002\n",
      "2022-01-30 23:34:25,921 INFO mapred.FileInputFormat: Total input files to process : 2\n",
      "2022-01-30 23:34:26,373 INFO mapreduce.JobSubmitter: number of splits:10\n",
      "2022-01-30 23:34:26,507 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1643556265243_0002\n",
      "2022-01-30 23:34:26,508 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-01-30 23:34:26,725 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-01-30 23:34:26,725 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-01-30 23:34:26,788 INFO impl.YarnClientImpl: Submitted application application_1643556265243_0002\n",
      "2022-01-30 23:34:26,831 INFO mapreduce.Job: The url to track the job: http://w261-m:8088/proxy/application_1643556265243_0002/\n",
      "2022-01-30 23:34:26,833 INFO mapreduce.Job: Running job: job_1643556265243_0002\n",
      "2022-01-30 23:34:33,931 INFO mapreduce.Job: Job job_1643556265243_0002 running in uber mode : false\n",
      "2022-01-30 23:34:33,931 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-01-30 23:34:42,042 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2022-01-30 23:34:48,078 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2022-01-30 23:34:49,084 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "2022-01-30 23:34:54,134 INFO mapreduce.Job:  map 90% reduce 0%\n",
      "2022-01-30 23:34:58,157 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2022-01-30 23:35:04,201 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "2022-01-30 23:35:06,211 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "2022-01-30 23:35:07,215 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-01-30 23:35:08,226 INFO mapreduce.Job: Job job_1643556265243_0002 completed successfully\n",
      "2022-01-30 23:35:08,309 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=149581\n",
      "\t\tFILE: Number of bytes written=3514851\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=152951\n",
      "\t\tHDFS: Number of bytes written=129303\n",
      "\t\tHDFS: Number of read operations=45\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=10\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=10\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=160277460\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=43060464\n",
      "\t\tTotal time spent by all map tasks (ms)=50785\n",
      "\t\tTotal time spent by all reduce tasks (ms)=13644\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=50785\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=13644\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=160277460\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=43060464\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10130\n",
      "\t\tMap output records=10130\n",
      "\t\tMap output bytes=129303\n",
      "\t\tMap output materialized bytes=149743\n",
      "\t\tInput split bytes=1010\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10130\n",
      "\t\tReduce shuffle bytes=149743\n",
      "\t\tReduce input records=10130\n",
      "\t\tReduce output records=10130\n",
      "\t\tSpilled Records=20260\n",
      "\t\tShuffled Maps =30\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=30\n",
      "\t\tGC time elapsed (ms)=1804\n",
      "\t\tCPU time spent (ms)=10750\n",
      "\t\tPhysical memory (bytes) snapshot=6052417536\n",
      "\t\tVirtual memory (bytes) snapshot=57558794240\n",
      "\t\tTotal committed heap usage (bytes)=6034030592\n",
      "\t\tPeak Map Physical memory (bytes)=552636416\n",
      "\t\tPeak Map Virtual memory (bytes)=4428369920\n",
      "\t\tPeak Reduce Physical memory (bytes)=282484736\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4431437824\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=151941\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=129303\n",
      "2022-01-30 23:35:08,309 INFO streaming.StreamJob: Output directory: /user/root/HW2//eda-sort-output\n"
     ]
    }
   ],
   "source": [
    "# part d - write your Hadoop streaming job here\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=3 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k3,3nr\" \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input {HDFS_DIR}/eda-output \\\n",
    "  -output {HDFS_DIR}/eda-sort-output \\\n",
    "  -cmdenv PATH={PATH}\\\n",
    "  -numReduceTasks 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/eda-sort-output\n"
     ]
    }
   ],
   "source": [
    "# part e - clear the output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/eda-sort-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.2.2.jar] /tmp/streamjob1220247576543057211.jar tmpDir=null\n",
      "2022-01-30 23:35:17,199 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.128.0.2:8032\n",
      "2022-01-30 23:35:17,408 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.128.0.2:10200\n",
      "2022-01-30 23:35:17,954 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.128.0.2:8032\n",
      "2022-01-30 23:35:17,955 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.128.0.2:10200\n",
      "2022-01-30 23:35:18,316 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1643556265243_0003\n",
      "2022-01-30 23:35:18,604 INFO mapred.FileInputFormat: Total input files to process : 2\n",
      "2022-01-30 23:35:18,654 INFO mapreduce.JobSubmitter: number of splits:10\n",
      "2022-01-30 23:35:18,794 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1643556265243_0003\n",
      "2022-01-30 23:35:18,796 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-01-30 23:35:19,050 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-01-30 23:35:19,050 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-01-30 23:35:19,118 INFO impl.YarnClientImpl: Submitted application application_1643556265243_0003\n",
      "2022-01-30 23:35:19,162 INFO mapreduce.Job: The url to track the job: http://w261-m:8088/proxy/application_1643556265243_0003/\n",
      "2022-01-30 23:35:19,163 INFO mapreduce.Job: Running job: job_1643556265243_0003\n",
      "2022-01-30 23:35:27,268 INFO mapreduce.Job: Job job_1643556265243_0003 running in uber mode : false\n",
      "2022-01-30 23:35:27,269 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-01-30 23:35:34,350 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2022-01-30 23:35:39,392 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "2022-01-30 23:35:40,401 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2022-01-30 23:35:41,410 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "2022-01-30 23:35:44,433 INFO mapreduce.Job:  map 70% reduce 0%\n",
      "2022-01-30 23:35:46,450 INFO mapreduce.Job:  map 90% reduce 0%\n",
      "2022-01-30 23:35:48,460 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2022-01-30 23:35:55,509 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "2022-01-30 23:35:57,519 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-01-30 23:35:58,529 INFO mapreduce.Job: Job job_1643556265243_0003 completed successfully\n",
      "2022-01-30 23:35:58,609 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=149581\n",
      "\t\tFILE: Number of bytes written=3519674\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=152951\n",
      "\t\tHDFS: Number of bytes written=129303\n",
      "\t\tHDFS: Number of read operations=45\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=10\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=10\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=149733264\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=44215560\n",
      "\t\tTotal time spent by all map tasks (ms)=47444\n",
      "\t\tTotal time spent by all reduce tasks (ms)=14010\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=47444\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=14010\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=149733264\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=44215560\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10130\n",
      "\t\tMap output records=10130\n",
      "\t\tMap output bytes=129303\n",
      "\t\tMap output materialized bytes=149743\n",
      "\t\tInput split bytes=1010\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10130\n",
      "\t\tReduce shuffle bytes=149743\n",
      "\t\tReduce input records=10130\n",
      "\t\tReduce output records=10130\n",
      "\t\tSpilled Records=20260\n",
      "\t\tShuffled Maps =30\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=30\n",
      "\t\tGC time elapsed (ms)=1552\n",
      "\t\tCPU time spent (ms)=11370\n",
      "\t\tPhysical memory (bytes) snapshot=5901492224\n",
      "\t\tVirtual memory (bytes) snapshot=57562701824\n",
      "\t\tTotal committed heap usage (bytes)=5685903360\n",
      "\t\tPeak Map Physical memory (bytes)=547954688\n",
      "\t\tPeak Map Virtual memory (bytes)=4427575296\n",
      "\t\tPeak Reduce Physical memory (bytes)=316952576\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4434477056\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=151941\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=129303\n",
      "2022-01-30 23:35:58,609 INFO streaming.StreamJob: Output directory: /user/root/HW2//eda-sort-output\n"
     ]
    }
   ],
   "source": [
    "# part e - write your Hadoop streaming job here\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=3 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k3,3nr\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k2,2\"  \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /bin/cat \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -input {HDFS_DIR}/eda-output \\\n",
    "  -output {HDFS_DIR}/eda-sort-output \\\n",
    "  -cmdenv PATH={PATH}\\\n",
    "  -numReduceTasks 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== part-00000=====\n",
      "\n",
      "the\t0\t549\t\n",
      "to\t0\t398\t\n",
      "ect\t0\t382\t\n",
      "and\t0\t278\t\n",
      "of\t0\t230\t\n",
      "hou\t0\t206\t\n",
      "a\t0\t196\t\n",
      "in\t0\t182\t\n",
      "for\t0\t170\t\n",
      "on\t0\t135\t\n",
      "cat: Unable to write to output stream.\n",
      "\n",
      "===== part-00001=====\n",
      "\n",
      "the\t1\t698\t\n",
      "to\t1\t566\t\n",
      "and\t1\t392\t\n",
      "your\t1\t357\t\n",
      "a\t1\t347\t\n",
      "you\t1\t345\t\n",
      "of\t1\t336\t\n",
      "in\t1\t236\t\n",
      "for\t1\t204\t\n",
      "com\t1\t153\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "# part e - view the top 10 records from each partition (RUN THIS CELL AS IS)\n",
    "for idx in range(2):\n",
    "    print(f\"\\n===== part-0000{idx}=====\\n\")\n",
    "    !hdfs dfs -cat {HDFS_DIR}/eda-sort-output/part-0000{idx} | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Expected output:__\n",
    "<table>\n",
    "<th>part-00000:</th>\n",
    "<th>part-00001:</th>\n",
    "<tr><td><pre>\n",
    "the\t0\t549\t\n",
    "to\t0\t398\t\n",
    "ect\t0\t382\t\n",
    "and\t0\t278\t\n",
    "of\t0\t230\t\n",
    "hou\t0\t206\t\n",
    "a\t0\t196\t\n",
    "in\t0\t182\t\n",
    "for\t0\t170\t\n",
    "on\t0\t135\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "the\t1\t698\t\n",
    "to\t1\t566\t\n",
    "and\t1\t392\t\n",
    "your\t1\t357\t\n",
    "a\t1\t347\t\n",
    "you\t1\t345\t\n",
    "of\t1\t336\t\n",
    "in\t1\t236\t\n",
    "for\t1\t204\t\n",
    "com\t1\t153\n",
    "</pre></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Counters and Combiners.\n",
    "Tuning the number of mappers & reducers is helpful to optimize very large distributed computations. Doing so successfully requires a thorough understanding of the data size at each stage of the job. As you learned in the week3 live session, counters are an invaluable resource for understanding this kind of detail. In this question, we will take the EDA performed in Question 4 as an opportunity to illustrate some related concepts.\n",
    "\n",
    "### Q5 Tasks:\n",
    "* __a) short response:__ Read the Hadoop output from your job in Question 4a to report how many records are emitted by the mappers and how many records are received be the reducers. In the context of word counting what does this number represent practically?\n",
    "\n",
    "* __b) code:__ Note that we wrote the reducer in question 4a such that the input and output record format is identical. This makes it easy to use the same reducer script as a combiner. In the space provided below, write the Hadoop Streaming command to re-run your job from question 4a with this combining added.\n",
    "\n",
    "* __c) short response__: Report the number of records emitted by your mappers in part b and the number of records received by your reducers. Compare your results here to what you saw in part a. Explain.\n",
    "\n",
    "* __d) short response__: Describe a scenario where using a combiner would _NOT_ improve the efficiency of the shuffle stage. Explain. [__`BONUS:`__ how does increasing the number of mappers affect the usefulness of a combiner?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 Student Answers:\n",
    "> __a)__  Mappers emitted 31490 records and reducers received  the same 31490 records. As we partioned the records by class and have specified to run with 2 reducers, all records that belonged to class 0 would have reached the reducer 0 and records that belonged to class 1 would have reached the reducer 1 and they added to 31490 records. No combiners were used to consolidate the records before sending it to the reducer.\n",
    "\n",
    "> __c)__  Mappers emitted 31490 records and reducers received  only 20576 records. As we used the reducer as the combiner, the combiner consolidated the records before sending it to the reducer. So, we have only 20576 records reaching the reducer. This number matched with the number of records output by the combiner. For example, lets assume the word 'for', the counts from mapper 1 and mapper 2 for class '1' would have gone to the combiner 1 and would have collapsed into one record before reaching the reducer and similarly the word count for the word for  class '0' would have collapsed into one record before reaching the reducer.\n",
    "\n",
    "> __d)__  In our previous problem we used our combiner to be a EnronEDA/reducer.py. The reducer.py collapses records with the same key into one by adding their counts. This reduced the number of (word, count) records that was passing through the shuffle. If we replaced this combiner with a simple /bin/cat command in LINUX it will serve as a pass through and so will not reduce any traffic on the shuffle and hence will NOT improve the efficiency.  **BONUS:** Lets go back to our earlier problem of using EnronEDA/reducer.py as both combiner and reducer. When we increase the number of mappers by two times from $m$ to $2*m$ then we will have approximately two times more (word, count) records leaving the mapping layer. However, assuming twice as many combiners $2*c$ are being used to reduce the (word, count) records it may not reduce the number of records through the shuffle to a level when we had $m$ mappers and $c$ combiners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/eda-output\n"
     ]
    }
   ],
   "source": [
    "# part b - clear output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/eda-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.2.2.jar] /tmp/streamjob1945276239039924653.jar tmpDir=null\n",
      "2022-01-30 23:44:47,430 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.128.0.2:8032\n",
      "2022-01-30 23:44:47,676 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.128.0.2:10200\n",
      "2022-01-30 23:44:48,124 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.128.0.2:8032\n",
      "2022-01-30 23:44:48,124 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.128.0.2:10200\n",
      "2022-01-30 23:44:48,297 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1643556265243_0005\n",
      "2022-01-30 23:44:48,997 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2022-01-30 23:44:49,058 INFO mapreduce.JobSubmitter: number of splits:9\n",
      "2022-01-30 23:44:49,196 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1643556265243_0005\n",
      "2022-01-30 23:44:49,198 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-01-30 23:44:49,432 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-01-30 23:44:49,433 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-01-30 23:44:49,489 INFO impl.YarnClientImpl: Submitted application application_1643556265243_0005\n",
      "2022-01-30 23:44:49,521 INFO mapreduce.Job: The url to track the job: http://w261-m:8088/proxy/application_1643556265243_0005/\n",
      "2022-01-30 23:44:49,523 INFO mapreduce.Job: Running job: job_1643556265243_0005\n",
      "2022-01-30 23:44:57,620 INFO mapreduce.Job: Job job_1643556265243_0005 running in uber mode : false\n",
      "2022-01-30 23:44:57,621 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-01-30 23:45:04,731 INFO mapreduce.Job:  map 11% reduce 0%\n",
      "2022-01-30 23:45:05,737 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2022-01-30 23:45:11,805 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2022-01-30 23:45:16,873 INFO mapreduce.Job:  map 78% reduce 0%\n",
      "2022-01-30 23:45:18,886 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2022-01-30 23:45:24,922 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "2022-01-30 23:45:25,927 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-01-30 23:45:27,942 INFO mapreduce.Job: Job job_1643556265243_0005 completed successfully\n",
      "2022-01-30 23:45:28,026 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=271860\n",
      "\t\tFILE: Number of bytes written=3278643\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=258608\n",
      "\t\tHDFS: Number of bytes written=119173\n",
      "\t\tHDFS: Number of read operations=37\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=9\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=9\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=157159332\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=22262424\n",
      "\t\tTotal time spent by all map tasks (ms)=49797\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7054\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=49797\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=7054\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=157159332\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=22262424\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=31490\n",
      "\t\tMap output bytes=306018\n",
      "\t\tMap output materialized bytes=271956\n",
      "\t\tInput split bytes=801\n",
      "\t\tCombine input records=31490\n",
      "\t\tCombine output records=20576\n",
      "\t\tReduce input groups=5065\n",
      "\t\tReduce shuffle bytes=271956\n",
      "\t\tReduce input records=20576\n",
      "\t\tReduce output records=10130\n",
      "\t\tSpilled Records=41152\n",
      "\t\tShuffled Maps =18\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=18\n",
      "\t\tGC time elapsed (ms)=1567\n",
      "\t\tCPU time spent (ms)=11290\n",
      "\t\tPhysical memory (bytes) snapshot=5189365760\n",
      "\t\tVirtual memory (bytes) snapshot=48705253376\n",
      "\t\tTotal committed heap usage (bytes)=5119672320\n",
      "\t\tPeak Map Physical memory (bytes)=561258496\n",
      "\t\tPeak Map Virtual memory (bytes)=4429373440\n",
      "\t\tPeak Reduce Physical memory (bytes)=242110464\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4431028224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=257807\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=119173\n",
      "2022-01-30 23:45:28,026 INFO streaming.StreamJob: Output directory: /user/root/HW2//eda-output\n"
     ]
    }
   ],
   "source": [
    "# part b - write your Hadoop streaming job here\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files EnronEDA/reducer.py,EnronEDA/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -combiner reducer.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -input {HDFS_DIR}/enron.txt \\\n",
    "  -output {HDFS_DIR}/eda-output \\\n",
    "  -numReduceTasks 2 \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: Document Classification Task Overview.\n",
    "The week 2 assigned reading from Chapter 13 of _Introduction to Information Retrieval_ by Manning, Raghavan and Schutze provides a thorough introduction to the document classification task and the math behind Naive Bayes. In this question we'll use the example from Table 13.1 (reproduced below) to 'train' an unsmoothed Multinomial Naive Bayes model and classify a test document by hand.\n",
    "\n",
    "<table>\n",
    "<th>DocID</th>\n",
    "<th>Class</th>\n",
    "<th>Subject</th>\n",
    "<th>Body</th>\n",
    "<tr><td>Doc1</td><td>1</td><td></td><td>Chinese Beijing Chinese</td></tr>\n",
    "<tr><td>Doc2</td><td>1</td><td></td><td>Chinese Chinese Shanghai</td></tr>\n",
    "<tr><td>Doc3</td><td>1</td><td></td><td>Chinese Macao</td></tr>\n",
    "<tr><td>Doc4</td><td>0</td><td></td><td>Tokyo Japan Chinese</td></tr>\n",
    "</table>\n",
    "\n",
    "### Q6 Tasks:\n",
    "* __a) short response:__ Equation 13.3 in Manning, Raghavan and Shutze shows how a Multinomial Naive Bayes model classifies a document. It predicts the class, $c$, for which the estimated conditional probability of the class given the document's contents,  $\\hat{P}(c|d)$, is greatest. In this equation what two pieces of information are required to calculate  $\\hat{P}(c|d)$? Your answer should include both mathematical notatation and verbal explanation.\n",
    "\n",
    "\n",
    "* __b) short response:__ The Enron data includes two classes of documents: `spam` and `ham` (they're actually labeled `1` and `0`). In plain English, explain what  $\\hat{P}(c)$ and   $\\hat{P}(t_{k} | c)$ mean in the context of this data. How will we would estimate these values from a training corpus? How many passes over the data would we need to make to retrieve this information for all classes and all words?\n",
    "\n",
    "\n",
    "* __c) hand calculations:__ Above we've reproduced the document classification example from the textbook (we added an empty subject field to mimic the Enron data format). Remember that the classes in this \"Chinese Example\" are `1` (about China) and `0` (not about China). Calculate the class priors and the conditional probabilities for an __unsmoothed__ Multinomial Naive Bayes model trained on this data. Show the calculations that lead to your result using markdown and $\\LaTeX$ in the space provided or by embedding an image of your hand written work. [`NOTE:` _Your results should NOT match those in the text -- they are training a model with +1 smoothing you are training a model without smoothing_]\n",
    "\n",
    "\n",
    "* __d) hand calculations:__ Use the model you trained to classify the following test document: `Chinese Chinese Chinese Tokyo Japan`. Show the calculations that lead to your result using markdown and   $\\LaTeX$ in the space provided or by embedding an image of your hand written work.\n",
    "\n",
    "\n",
    "* __e) short response:__ Compare the classification you get from this unsmoothed model in `d`/`e` to the results in the textbook's \"Example 1\" which reflects a model with Laplace plus 1 smoothing. How does smoothing affect our inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 Student Answers:\n",
    "> __a)__ The estimated conidtional probability of the class given the document's content is given as follows: $$argmax_{c \\epsilon C}\\hat{P(c|d)} = argmax_{c \\epsilon C}\\hat{P(c)} \\prod_{1 \\le k \\le n_d } \\hat{P(t_k|c)} $$ where, $c, d, t_k, n_d$ stand for class, document, $k^{th}$ token of the vocabulary we used for the classification, and number of documents respectively. So, basically we need the probability of class $P(c)$ and probability of each token in the vocabulary given the class $\\hat{P(t_k|c)}$ .\n",
    "\n",
    "> __b)__ $\\hat{P(c)}$ in the above equation refer to $\\hat{P(ham)}$ and $\\hat{P(spam)}$. $\\hat{P(ham)}$ can be calculated by the ratio of number of hams and the total number of mails. Similarly, the $\\hat{P(spam)}$ is cacluated by the ratio of number of spams and the total number of mails. The probability of a token $t_k$ given the class $\\hat{P(t_k|c)}$ is calcuated by the ratio of joint probability of $P(t_k, c)$ and $P(c)$. The joint probability $P(t_k, c)$ is calcuated by dividing the number of occurences of the token and total tokens in the class $c$.\n",
    "\n",
    "> __c)__ Prior probability for spam is calculated as follows: $$\\hat{P(spam)} = \\frac{N_c}{N} = \\frac{3}{4} = 0.75$$ where $N_c$ refers to the number of documents labeled as class $c$ and $N$ is the total number of documents. Similarly the prior probability for ham is calculated as follows:  $$\\hat{P(ham)} = \\frac{1}{4} = 0.25$$ The Conditional probablities are calculated as follows: $$\\hat{P(Chinese|spam)} = \\frac{5}{8}$$\n",
    "$$\\hat{P(Tokyo|spam)} = \\hat{P(Japan|spam)} = \\frac{0}{8}$$\n",
    "$$\\hat{P(Chinese|ham)} = \\frac{1}{3}$$\n",
    "$$\\hat{P(Tokyo|ham)} = \\hat{P(Japan|ham)} = \\frac{1}{3}$$\n",
    "\n",
    "> __d)__  For a given doument $D_5$: $$P(spam|D_5) = \\hat{P(spam)}\\cdot\\hat{P(Chinese|spam)}^3\\cdot\\hat{P(Tokyo|spam)}\\cdot \\hat{P(Japan|spam)} = (3/4)\\cdot(5/8)^3\\cdot0\\cdot0 = 0$$ and similarly, $$P(ham|D_5) = \\hat{P(ham)}\\cdot\\hat{P(Chinese|ham)}^3\\cdot\\hat{P(Tokyo|ham)}\\cdot \\hat{P(Japan|ham)} = (1/3)\\cdot(1/3)\\cdot(1/3)$$.\n",
    "\n",
    "> __e)__ As $P(spam|D_5)$ < $P(0|D_5)$, our unsmoothed model favours class ham and the smoothing model rightly favors the class spam over class ham.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part d/e - if you didn't write out your calcuations above, embed a picture of them here:\n",
    "from IPython.display import Image\n",
    "#Image(filename=\"path-to-hand-calulations-image.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7: Naive Bayes Inference.\n",
    "In the next two questions you'll write code to parallelize the Naive Bayes calculations that you performed above. We'll do this in two phases: one MapReduce job to perform training and a second MapReduce to perform inference. While in practice we'd need to train a model before we can use it to classify documents, for learning purposes we're going to develop our code in the opposite order. By first focusing on the pieces of information/format we need to perform the classification (inference) task you should find it easier to develop a solid implementation for training phase when you get to question 8 below. In both of these questions we'll continue to use the Chinese example corpus from the textbook to help us test our MapReduce code as we develop it. Below we've reproduced the corpus, test set and model in text format that matches the Enron data.\n",
    "\n",
    "### Q7 Tasks:\n",
    "* __a) short response:__ run the provided cells to create the example files and load them in to HDFS. Then take a closer look at __`NBmodel.txt`__. This text file represents a Naive Bayes model trained (with Laplace +1 smoothing) on the example corpus. What are the 'keys' and 'values' in this file? Which record means something slightly different than the rest? The value field of each record includes two numbers which will be helpful for debugging but which we don't actually need to perform inference -- what are they? [`HINT`: _This file represents the model from Example 13.1 in the textbook, if you're having trouble getting oriented try comparing our file to the numbers in that example._]\n",
    "\n",
    "\n",
    "* __b) short response:__ When performing Naive Bayes in practice instead of multiplying the probabilities (as in equation 13.3) we add their logs (as in equation 13.4). Why do we choose to work with log probabilities? If we had an unsmoothed model, what potential error could arise from this transformation?\n",
    "\n",
    "\n",
    "* __c) short response:__ Documents 6 and 8 in the test set include a word that did not appear in the training corpus (and as a result does not appear in the model). What should we do at inference time when we need a class conditional probability for this word?\n",
    "\n",
    "\n",
    "* __d) short response:__ The goal of our MapReduce job is to stream over the test set and classify each document by peforming the calculation from equation 13.4. To do this we'll load the model file (which contains the probabilities for equation 13.4) into memory on the nodes where we do our mapping. This is called an in-memory join. Does loading a model 'state' like this depart from the functional programming principles? Explain why or why not. From a scability perspective when would this kind of memory use be justified? when would it be unwise?\n",
    "\n",
    "\n",
    "* __e) code:__ Complete the code in __`NaiveBayes/classify_mapper.py`__. Read the docstring carefully to understand how this script should work and the format it should return. Run the provided unit tests to confirm that your script works as expected then write a Hadoop streaming job to classify the Chinese example test set. [`HINT 1:` _you shouldn't need a reducer for this one._ `HINT 2:` _Don't forget to add the model file to the_ `-files` _parameter in your Hadoop streaming job so that it gets shipped to the mapper nodes where it will be accessed by your script._]\n",
    "\n",
    "\n",
    "* __f) short response:__ In our test example and in the Enron data set we have fairly short documents. Since these fit fine in memory on a mapper node we didn't need a reducer and could just do all of our calculations in the mapper. However with much longer documents (eg. books) we might want a higher level of parallelization -- for example we might want to process parts of a document on different nodes. In this hypothetical scenario how would our algorithm design change? What could the mappers still do? What key-value structure would they emit? What would the reducers have to do as a last step?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7 Student Answers:\n",
    "> __a)__  NBModel.txt file, the ClassPriors record is different from other records. Other records display 4 numbers The first two show the word count of the word in the corresponding document category (0 or 1). The next two numbers refer to the smoothened probabilities of the word given the classes 0 or 1 respectively. The First two numbers are mainly for debugging\n",
    "\n",
    "> __b)__  When performing Naive Bayes we end up  multiplying many conditional probabilities. This may result is floating point underflow. To avoid this underflow problem, it is a standard practice to take logarithms of these probabilties and add them instead. However, when we use this technique with unsmoothened models it may lead to floating point error as we may end up taking logorithms of zero numbers.\n",
    "\n",
    "> __c)__ Ignore the word. This means that if we use the logprobability like we have in our model then assign the logprobability value of zero for the word.\n",
    "\n",
    "> __d)__ One of the core principle of functional programming is statelessness of a function. Loading a model state from disk or other external source and there by computing conditional probability is a deviation from this core principle. However, this approach has an inherent advantage of short processing time and low memory usage in some cases. Once a trained and tested model is loaded in memory it is ready for use in production. The memory footprint of the model is far smaller than the data to train and test the model and we don't need to worry about the data anymore. For e.g. models like regression, SVM, etc. the models are far smaller than data itself and it is wise to use this approach. In the case of KNN and NB models the models themselves are large. In case of our NBmodel.txt, we still have to load the conditional probabilities of each word in the vocabulary which takes substantial memory. It will be unwise to use this approach.\n",
    "\n",
    "> __e)__ Complete the coding portion of this question before answering 'f'.\n",
    "\n",
    "> __f)__ In case of longer documents (eg. books) the document would be stored in the hadoop file system as chunks. While it does not make sense to classify a book to be spam or ham. We can assume it to be a book classification problem. When we process this document or book using a mapper that structurally looks like a classify_mapper for e.g. we would be processing different part of the same document in multiple mappers and each mapper will output a record containing doc_id, class, log_prior_ham, log_prior_spam, log_cond_prob_ham_partial, log_cond_prob_spam_partial, pred_class. Please note this record emits log_prior separately as it has to be added in the end after adding all partial conditional probabilites in the reducer. Also note doc_id will used as the partition key so that all partial conditional probabilites from multiple mappers go to the same reducer to get aggregated and finally the prior to calculate the predicted class. So, basically our hadoop streaming process would have multiple mappers each mapper processing a part of the book/document and all mappers will output a record as listed here for each chunk. These records for the same doc_id will all go to one reducer to get aggregated for classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run these cells to create the example corpus and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NaiveBayes/chineseTrain.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile NaiveBayes/chineseTrain.txt\n",
    "D1\t1\t\tChinese Beijing Chinese\n",
    "D2\t1\t\tChinese Chinese Shanghai\n",
    "D3\t1\t\tChinese Macao\n",
    "D4\t0\t\tTokyo Japan Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NaiveBayes/chineseTest.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile NaiveBayes/chineseTest.txt\n",
    "D5\t1\t\tChinese Chinese Chinese Tokyo Japan\n",
    "D6\t1\t\tBeijing Shanghai Trade\n",
    "D7\t0\t\tJapan Macao Tokyo\n",
    "D8\t0\t\tTokyo Japan Trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NBmodel.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile NBmodel.txt\n",
    "beijing\t0.0,1.0,0.111111111111,0.142857142857\n",
    "chinese\t1.0,5.0,0.222222222222,0.428571428571\n",
    "tokyo\t1.0,0.0,0.222222222222,0.0714285714286\n",
    "shanghai\t0.0,1.0,0.111111111111,0.142857142857\n",
    "ClassPriors\t1.0,3.0,0.25,0.75\n",
    "japan\t1.0,0.0,0.222222222222,0.0714285714286\n",
    "macao\t0.0,1.0,0.111111111111,0.142857142857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/user/root/HW2/chineseTrain.txt': File exists\n",
      "copyFromLocal: `/user/root/HW2/chineseTest.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "# load the data files into HDFS\n",
    "!hdfs dfs -copyFromLocal NaiveBayes/chineseTrain.txt {HDFS_DIR}\n",
    "!hdfs dfs -copyFromLocal NaiveBayes/chineseTest.txt {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your work for `part e` starts here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part e - do your work in NaiveBayes/classify_mapper.py first, then run this cell.\n",
    "!chmod a+x NaiveBayes/classify_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\n",
      "\"\"\"\n",
      "Mapper for Naive Bayes Inference.\n",
      "INPUT:\n",
      "    ID \\t true_class \\t subject \\t body \\n\n",
      "OUTPUT:\n",
      "    ID \\t true_class \\t logP(ham|doc) \\t logP(spam|doc) \\t predicted_class\n",
      "SUPPLEMENTAL FILE: \n",
      "    This script requires a trained Naive Bayes model stored \n",
      "    as NBmodel.txt in the current directory. The model should \n",
      "    be a tab separated file whose records look like:\n",
      "        WORD \\t ham_count,spam_count,P(word|ham),P(word|spam)\n",
      "        \n",
      "Instructions:\n",
      "    We have loaded the supplemental file and taken the log of \n",
      "    each conditional probability in the model. We also provide\n",
      "    the code to tokenize the input lines for you. Keep in mind \n",
      "    that each 'line' of this file represents a unique document \n",
      "    that we wish to classify. Fill in the missing code to get\n",
      "    the probability of each class given the words in the document.\n",
      "    Remember that you will need to handle the case where you\n",
      "    encounter a word that is not represented in the model.\n",
      "\"\"\"\n",
      "import os\n",
      "import re\n",
      "import sys\n",
      "import numpy as np\n",
      "\n",
      "# confirm that we have access to the model file\n",
      "assert 'NBmodel.txt' in os.listdir('.'), \"ERROR: can't find NBmodel.txt\"\n",
      "\n",
      "# load the model into a dictionary for easy access\n",
      "MODEL = {}\n",
      "for record in open('NBmodel.txt', 'r').readlines():\n",
      "    model_ = record.split('\\t')\n",
      "    \n",
      "    if len(model_) == 2:\n",
      "        word, payload = record.split('\\t')\n",
      "        # extract conditional probabilities\n",
      "        ham_cProb, spam_cProb = payload.split(',')[2:]\n",
      "    else:\n",
      "        word, class0_cnt, class1_cnt, ham_cProb, spam_cProb = record.split('\\t')\n",
      "\n",
      "    # save their logs as a tuple in our model dictionary\n",
      "    take_log = lambda x: np.log(x) if x != 0 else float(\"-inf\")\n",
      "    MODEL[word] = (take_log(float(ham_cProb)),\n",
      "                   take_log(float(spam_cProb)))\n",
      "\n",
      "# read from standard input\n",
      "for line in sys.stdin:\n",
      "    # parse input and tokenize\n",
      "    docID, _class, subject, body = line.lower().split('\\t')\n",
      "    words = re.findall(r'[a-z]+', subject + ' ' + body)\n",
      "    \n",
      "    # initialize variables that student code should overwrite\n",
      "    logpHam, logpSpam, pred_class = None, None, None\n",
      "    \n",
      "    ################# YOUR CODE HERE ################\n",
      "    # TIP: try using MODEL.get(word, (0,0)) to access the tuple \n",
      "    # of log probabilities without throwing a KeyError!\n",
      "    logpHam, logpSpam = MODEL.get('ClassPriors')\n",
      "    \n",
      "    for w in words:\n",
      "        t = MODEL.get(w)\n",
      "        ham_logcondp, spam_logcondp = t if  t != None else (0, 0)\n",
      "        logpHam = logpHam + ham_logcondp\n",
      "        logpSpam = logpSpam + spam_logcondp\n",
      "\n",
      "    \n",
      "    pred_class = '1' if logpSpam > logpHam else '0'\n",
      "    ################# (END) YOUR CODE ##############\n",
      "    print(f\"{docID}\\t{_class}\\t{logpHam}\\t{logpSpam}\\t{pred_class}\")\n"
     ]
    }
   ],
   "source": [
    "!cat NaiveBayes/classify_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d5  1  -8.90668134500626   -8.10769031284611   1\n",
      "d6  1  -5.780743515794329  -4.179502370564408  1\n",
      "d7  0  -6.591673732011658  -7.511706880737812  0\n",
      "d8  0  -4.394449154674438  -5.565796731681498  0\n"
     ]
    }
   ],
   "source": [
    "# part e - unit test NaiveBayes/classify_mapper.py (RUN THIS CELL AS IS)\n",
    "!cat NaiveBayes/chineseTest.txt | NaiveBayes/classify_mapper.py | column -t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/chinese-output\n"
     ]
    }
   ],
   "source": [
    "# part e - clear the output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/chinese-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.2.2.jar] /tmp/streamjob1830671026684640071.jar tmpDir=null\n",
      "2022-01-30 23:58:01,400 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.128.0.2:8032\n",
      "2022-01-30 23:58:01,654 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.128.0.2:10200\n",
      "2022-01-30 23:58:02,119 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.128.0.2:8032\n",
      "2022-01-30 23:58:02,119 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.128.0.2:10200\n",
      "2022-01-30 23:58:02,290 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1643556265243_0006\n",
      "2022-01-30 23:58:02,624 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2022-01-30 23:58:02,683 INFO mapreduce.JobSubmitter: number of splits:10\n",
      "2022-01-30 23:58:02,843 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1643556265243_0006\n",
      "2022-01-30 23:58:02,845 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-01-30 23:58:03,090 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-01-30 23:58:03,091 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-01-30 23:58:03,168 INFO impl.YarnClientImpl: Submitted application application_1643556265243_0006\n",
      "2022-01-30 23:58:03,217 INFO mapreduce.Job: The url to track the job: http://w261-m:8088/proxy/application_1643556265243_0006/\n",
      "2022-01-30 23:58:03,220 INFO mapreduce.Job: Running job: job_1643556265243_0006\n",
      "2022-01-30 23:58:10,311 INFO mapreduce.Job: Job job_1643556265243_0006 running in uber mode : false\n",
      "2022-01-30 23:58:10,312 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-01-30 23:58:18,422 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2022-01-30 23:58:23,477 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "2022-01-30 23:58:24,487 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2022-01-30 23:58:25,494 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "2022-01-30 23:58:28,533 INFO mapreduce.Job:  map 70% reduce 0%\n",
      "2022-01-30 23:58:30,555 INFO mapreduce.Job:  map 90% reduce 0%\n",
      "2022-01-30 23:58:32,564 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2022-01-30 23:58:37,589 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-01-30 23:58:39,604 INFO mapreduce.Job: Job job_1643556265243_0006 completed successfully\n",
      "2022-01-30 23:58:39,696 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=192\n",
      "\t\tFILE: Number of bytes written=2723686\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1555\n",
      "\t\tHDFS: Number of bytes written=178\n",
      "\t\tHDFS: Number of read operations=35\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=10\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=10\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=160056540\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=7949964\n",
      "\t\tTotal time spent by all map tasks (ms)=50715\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2519\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=50715\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2519\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=160056540\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=7949964\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=4\n",
      "\t\tMap output bytes=178\n",
      "\t\tMap output materialized bytes=246\n",
      "\t\tInput split bytes=950\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=246\n",
      "\t\tReduce input records=4\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=8\n",
      "\t\tShuffled Maps =10\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=10\n",
      "\t\tGC time elapsed (ms)=1415\n",
      "\t\tCPU time spent (ms)=6720\n",
      "\t\tPhysical memory (bytes) snapshot=5269209088\n",
      "\t\tVirtual memory (bytes) snapshot=48701411328\n",
      "\t\tTotal committed heap usage (bytes)=5053612032\n",
      "\t\tPeak Map Physical memory (bytes)=547819520\n",
      "\t\tPeak Map Virtual memory (bytes)=4428296192\n",
      "\t\tPeak Reduce Physical memory (bytes)=236642304\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4430798848\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=605\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=178\n",
      "2022-01-30 23:58:39,696 INFO streaming.StreamJob: Output directory: /user/root/HW2//chinese-output\n"
     ]
    }
   ],
   "source": [
    "# part e - write your Hadooop streaming job here\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files NaiveBayes/classify_mapper.py,NBmodel.txt\\\n",
    "  -mapper classify_mapper.py \\\n",
    "  -input {HDFS_DIR}/chineseTest.txt \\\n",
    "  -output {HDFS_DIR}/chinese-output \\\n",
    "  -cmdenv PATH={PATH} \\\n",
    "  -numReduceTasks 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part e - retrieve test set results from HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/chinese-output/part-000* > NaiveBayes/chineseResults.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d5  1  -8.90668134500626   -8.10769031284611   1\n",
      "d6  1  -5.780743515794329  -4.179502370564408  1\n",
      "d7  0  -6.591673732011658  -7.511706880737812  0\n",
      "d8  0  -4.394449154674438  -5.565796731681498  0\n"
     ]
    }
   ],
   "source": [
    "# part e - take a look (RUN THIS CELL AS IS)\n",
    "!cat NaiveBayes/chineseResults.txt | column -t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<th> Expected output for the test set:</th>\n",
    "<tr align=Left><td><pre>\n",
    "d5\t1\t-8.90668134\t-8.10769031\t1\n",
    "d6\t1\t-5.78074351\t-4.17950237\t1\n",
    "d7\t0\t-6.59167373\t-7.51170688\t0\n",
    "d8\t0\t-4.39444915\t-5.56579673\t0\n",
    "</pre></td><tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8: Naive Bayes Training.\n",
    "In Question 7 we used a model that we had trained by hand. Next we'll develop the code to do that same training in parallel, making it suitable for use with larger corpora (like the Enron emails). The end result of the MapReduce job you write in this question should be a model text file that looks just like the example (`NBmodel.txt`) that we created by hand above.\n",
    "\n",
    "To refresh your memory about the training process take a look at  `6a` and `6b` where you described the pieces of information you'll need to collect in order to encode a Multinomial Naive Bayes model. We now want to retrieve those pieces of information while streaming over a corpus. The bulk of the task will be very similar to the word counting excercises you've already done but you may want to consider a slightly different key-value record structure to efficiently tally counts for each class. \n",
    "\n",
    "The most challenging (interesting?) design question will be how to retrieve the totals (# of documents and # of words in documents for each class). Of course, counting these numbers is easy. The hard part is the timing: you'll need to make sure you have the counts totalled up _before_ you start estimating the class conditional probabilities for each word. It would be best (i.e. most scalable) if we could find a way to do this tallying without storing the whole vocabulary in memory... Use an appropriate MapReduce design pattern to implement this efficiently! \n",
    "\n",
    "\n",
    "### Q8 Tasks:\n",
    "* __a) make a plan:__  Fill in the docstrings for __`NaiveBayes/train_mapper.py`__ and __`NaiveBayes/train_reducer.py`__ to appropriately reflect the format that each script will input/output. [`HINT:` _the input files_ (`enronemail_1h.txt` & `chineseTrain.txt`) _have a prespecified format and your output file should match_ `NBmodel.txt` _so you really only have to decide on an internal format for Hadoop_].\n",
    "\n",
    "\n",
    "* __b) implement it:__ Complete the code in __`NaiveBayes/train_mapper.py`__ and __`NaiveBayes/train_reducer.py`__ so that together they train a Multinomial Naive Bayes model __with no smoothing__. Make sure your end result is formatted correctly (see note above). Test your scripts independently and together (using `chineseTrain.txt` or test input of your own devising). When you are satisfied with your Python code design and run a Hadoop streaming command to run your job in parallel on the __chineseTrain.txt__. Confirm that your trained model matches your hand calculations from Question 6.\n",
    "\n",
    "\n",
    "* __c) short response:__ We saw in Question 6 that adding Laplace smoothing (where the smoothing parameter $k=1$) makes our classifications less sensitve to rare words. However implementing this technique requires access to one additional piece of information that we had not previously used in our Naive Bayes training. What is that extra piece of information? [`HINT:` see equation 13.7 in Manning, Raghavan and Schutze].\n",
    "\n",
    "\n",
    "* __d) short response:__ There are a couple of approaches that we could take to handle the extra piece of information you identified in `c`: 1) if we knew this extra information beforehand, we could provide it to our reducer as a configurable parameter for the vocab size dynamically (_where would we get it in the first place?_). Or 2) we could compute it in the reducer without storing any bulky information in memory but then we'd need some postprocessing or a second MapReduce job to complete the calculation (_why?_). Breifly explain what is non-ideal about each of these options. \n",
    "\n",
    "\n",
    "* __e) code + short response:__ Choose one of the 2 options above. State your choice & reasoning in the space below then use that strategy to complete the code in __`NaiveBayes/train_reducer_smooth.py`__. Test this alternate reducer then write and run a Hadoop streaming job to train an MNB model with smoothing on the Chinese example. Your results should match the model that we provided for you above (and the calculations in the textbook example). __IMPORTANT NOTE:__ For full credit on this question, your code must work with multiple reducers. \n",
    "\n",
    "    - [`HINT:` You will need to implement custom partitioning - [Total Order Sort Notebook](https://github.com/UCB-w261/main/tree/master/HelpfulResources/TotalSortGuide/_total-sort-guide-spark2.01-JAN27-2017.ipynb)] \n",
    "\n",
    "    - [`HINT:` Don't start from scratch with this one -- you can just copy over your reducer code from part `b` and make the needed modifications]. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__IMPORTANT NOTE:__ For full credit on this question, your code must work with multiple reducers. [`HINT:`_You will need to implement custom partitioning - [Total Order Sort Notebook](https://github.com/UCB-w261/main/tree/master/HelpfulResources/TotalSortGuide/_total-sort-guide-spark2.01-JAN27-2017.ipynb)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8 Student Answers:\n",
    "> __ c)__ Suppose we have a multinomial variable with sample counts $c_1, c_2,...c_n$ where $d$ is the vocabulary size. A Laplacian smoothed version of estimated probabilities has the form: $(c_i+k)/(N+d.k)$ , where $k$ is a given positive integer and d the size of the vocabulary. Typically, $k$ is set to 1 to smooth the estimator. So, we need $k$, the input parameter that is provided and $d$ the size of the vocabulary.\n",
    "\n",
    "> __ d)__ As noted in previous question we need the size of the vocabulary to perform Laplacian smoothing when calcuating the conditional probability. As mentioned above, **Option 1:** We could post process the NBmodel.txt file from the unsmoothed hadoop process to calculate the vocabulary size. As this file has the all the words in the vocabulary except for the PostPrior entries which can be ignored and set the vocabulary size as the environment variable or pass the value as command line argument to the hadoop process when running the smoothened hadoop run or pass the file as we can pass the NBmode.txt as a file arugment and than can be parsed at the reducer phase of the smoothened hadoop run. When parsing the file we can count the words as we read in and use it in the probabaility calcuation.  OR **Option 2:** Were we can stream the ouput from first Hadoop streaming job to a second hadoop streaming job which will calcuate the vocabulary count as send it to the mapper using an inverted index pattern. With option 1, when we fail to set the vacabulary size the caculations can be wrong or could potentially break the processing. Another downside would be is that the NBmode.txt file or the post-processed vocabulary count could be stale and as a result the model could be in accurate. In option 2, however we will have to run two hadoop jobs have to be run back-to-back.\n",
    "\n",
    "> __ e)__ We picked option 1, as it is the simplest and least error prone of both the approaches and the pattern is very similar to what we employed in problem 7(e).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== MAPPER DOCSTRING ============\n",
      "Mapper reads in text documents and emits word counts by class.\n",
      "INPUT:                                                    \n",
      "    DocID \\t true_class \\t subject \\t body                \n",
      "OUTPUT:                                                   \n",
      "    partitionKey \\t word \\t class0_partialCount,class1_partialCount       \n",
      "    \n",
      "=========== REDUCER DOCSTRING ============\n",
      "Reducer aggregates word counts by class and emits frequencies.\n",
      "INPUT:                                                    \n",
      "    partitionKey \\t word \\t class0_partialCount \\t class1_partialCount                \n",
      "OUTPUT:\n",
      "    word \\t class0_count \\t class1_count \\t class0_conditional_prob \\t class1_conditional_prob \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# part a - do your work in train_mapper.py and train_reducer.py then RUN THIS CELL AS IS\n",
    "!chmod a+x NaiveBayes/train_mapper.py\n",
    "!chmod a+x NaiveBayes/train_reducer.py\n",
    "!echo \"=========== MAPPER DOCSTRING ============\"\n",
    "!head -n 8 NaiveBayes/train_mapper.py | tail -n 6\n",
    "!echo \"=========== REDUCER DOCSTRING ============\"\n",
    "!head -n 8 NaiveBayes/train_reducer.py | tail -n 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\n",
      "\"\"\"\n",
      "Mapper reads in text documents and emits word counts by class.\n",
      "INPUT:                                                    \n",
      "    DocID \\t true_class \\t subject \\t body                \n",
      "OUTPUT:                                                   \n",
      "    partitionKey \\t word \\t class0_partialCount,class1_partialCount       \n",
      "    \n",
      "\n",
      "Instructions:\n",
      "    You know what this script should do, go for it!\n",
      "    (As a favor to the graders, please comment your code clearly!)\n",
      "    \n",
      "    A few reminders:\n",
      "    1) To make sure your results match ours please be sure\n",
      "       to use the same tokenizing that we have provided in\n",
      "       all the other jobs:\n",
      "         words = re.findall(r'[a-z]+', text-to-tokenize.lower())\n",
      "         \n",
      "    2) Don't forget to handle the various \"totals\" that you need\n",
      "       for your conditional probabilities and class priors.\n",
      "       \n",
      "Partitioning:\n",
      "    In order to send the totals to each reducer, we need to implement\n",
      "    a custom partitioning strategy.\n",
      "    \n",
      "    We will generate a list of keys based on the number of reduce tasks \n",
      "    that we read in from the environment configuration of our job.\n",
      "    \n",
      "    We'll prepend the partition key by hashing the word and selecting the\n",
      "    appropriate key from our list. This will end up partitioning our data\n",
      "    as if we'd used the word as the partition key - that's how it worked\n",
      "    for the single reducer implementation. This is not necessarily \"good\",\n",
      "    as our data could be very skewed. However, in practice, for this\n",
      "    exercise it works well. The next step would be to generate a file of\n",
      "    partition split points based on the distribution as we've seen in \n",
      "    previous exercises.\n",
      "    \n",
      "    Now that we have a list of partition keys, we can send the totals to \n",
      "    each reducer by prepending each of the keys to each total.\n",
      "       \n",
      "\"\"\"\n",
      "\n",
      "import re                                                   \n",
      "import sys                                                  \n",
      "import numpy as np      \n",
      "\n",
      "from operator import itemgetter\n",
      "import os\n",
      "\n",
      "#################### YOUR CODE HERE ###################\n",
      "\n",
      "class1_doc_count, class0_doc_count = 0, 0\n",
      "class1_word_count, class0_word_count = 0, 0\n",
      "\n",
      "def inverse_hash1(c):\n",
      "    c = c.lower()\n",
      "    if ord(c)<ord('i'):\n",
      "        return 0\n",
      "    elif ord(c)<ord('o'):\n",
      "        return 1\n",
      "    else:\n",
      "        return 2\n",
      "    \n",
      "if os.getenv('mapreduce_job_reduces') == None:\n",
      "    N = 3\n",
      "else:\n",
      "    N = int(os.getenv('mapreduce_job_reduces'))\n",
      "    \n",
      "\n",
      "alpha = 'abcdefghijklmnopqrstuvwxyz'\n",
      "n = int(len(alpha)/N)\n",
      "parts = [alpha[i*n-1] for i in range(1, N+1)]\n",
      "\n",
      "def inverse_hash(c):\n",
      "    c = c.lower()\n",
      "    m = len(parts)\n",
      "    for i in range(0, m):\n",
      "        if ord(c)<=ord(parts[i]):\n",
      "            return i\n",
      "    return m-1\n",
      "    \n",
      "def makeIndex(key, num_reducers = N):\n",
      "    \"\"\"\n",
      "    Mimic the Hadoop string-hash function.\n",
      "    \n",
      "    key             the key that will be used for partitioning\n",
      "    num_reducers    the number of reducers that will be configured\n",
      "    \"\"\"\n",
      "    byteof = lambda char: int(format(ord(char), 'b'), 2)\n",
      "    current_hash = 0\n",
      "    for c in key:\n",
      "        current_hash = (current_hash * 31 + byteof(c))\n",
      "    return current_hash % num_reducers\n",
      "\n",
      "def makeKeyFile(num_reducers = N):\n",
      "    KEYS = list(map(chr, range(ord('A'), ord('Z')+1)))[:num_reducers]\n",
      "    partition_keys = sorted(KEYS, key=lambda k: makeIndex(k,num_reducers))\n",
      "\n",
      "    return partition_keys\n",
      "\n",
      "\n",
      "# call your helper function to get partition keys\n",
      "pKeys = makeKeyFile()\n",
      "\n",
      "for line in sys.stdin:\n",
      "    # parse input and tokenize\n",
      "    docID, _class, subject, body = line.lower().split('\\t')\n",
      "    words = re.findall(r'[a-z]+', subject + ' ' + body)\n",
      "    \n",
      "    if _class == '1':\n",
      "        class1_doc_count = class1_doc_count + 1\n",
      "    else:\n",
      "        class0_doc_count = class0_doc_count + 1\n",
      "\n",
      "    for w in words:\n",
      "        if _class == '1':\n",
      "            print(str(inverse_hash(w[:1]))+'\\t'+w+'\\t'+'0'+'\\t'+'1')\n",
      "            class1_word_count = class1_word_count + 1\n",
      "        else:\n",
      "            print(str(inverse_hash(w[:1]))+'\\t'+w+'\\t'+'1'+'\\t'+'0')\n",
      "            class0_word_count = class0_word_count + 1\n",
      "            \n",
      "for i in range(N):\n",
      "    print(str(i)+'\\t'+'*doccount'+'\\t'+str(class0_doc_count)+'\\t'+str(class1_doc_count))\n",
      "    print(str(i)+'\\t'+'*wordcount'+'\\t'+str(class0_word_count)+'\\t'+str(class1_word_count))\n",
      "#################### (END) YOUR CODE ###################"
     ]
    }
   ],
   "source": [
    "!cat NaiveBayes/train_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\n",
      "\"\"\"\n",
      "Reducer aggregates word counts by class and emits frequencies.\n",
      "INPUT:                                                    \n",
      "    partitionKey \\t word \\t class0_partialCount \\t class1_partialCount                \n",
      "OUTPUT:\n",
      "    word \\t class0_count \\t class1_count \\t class0_conditional_prob \\t class1_conditional_prob \n",
      "    \n",
      "    \n",
      "Instructions:\n",
      "    Again, you are free to design a solution however you see \n",
      "    fit as long as your final model meets our required format\n",
      "    for the inference job we designed in Question 8. Please\n",
      "    comment your code clearly and concisely.\n",
      "    \n",
      "    A few reminders: \n",
      "    1) Don't forget to emit Class Priors (with the right key).\n",
      "    2) In python2: 3/4 = 0 and 3/float(4) = 0.75\n",
      "\"\"\"\n",
      "import sys\n",
      "##################### YOUR CODE HERE ####################\n",
      "c0_total = None\n",
      "c1_total = None\n",
      "c0_wordtotal = None\n",
      "c1_wordtotal = None\n",
      "\n",
      "current_word = None\n",
      "ham_count = 0\n",
      "spam_count = 0\n",
      "ham_prob = 0.0\n",
      "spam_prob = 0.0\n",
      "\n",
      "for line in sys.stdin:\n",
      "    # parse input\n",
      "    pk, word, c0_n, c1_n = line.split('\\t')\n",
      "    c0_n = int(c0_n)\n",
      "    c1_n = int(c1_n)\n",
      "    if word == current_word:\n",
      "        spam_count = spam_count + c1_n\n",
      "        ham_count = ham_count + c0_n\n",
      "    else:\n",
      "        if word[0] == '*':\n",
      "            if word == '*doccount':\n",
      "                if c0_total == None:\n",
      "                    c0_total = c0_n\n",
      "                    c1_total = c1_n\n",
      "                else:\n",
      "                    c0_total = c0_total + c0_n\n",
      "                    c1_total = c1_total + c1_n\n",
      "            if word == '*wordcount':\n",
      "                if c0_wordtotal == None:\n",
      "                    c0_wordtotal = c0_n\n",
      "                    c1_wordtotal = c1_n\n",
      "                else:\n",
      "                    c0_wordtotal = c0_wordtotal + c0_n\n",
      "                    c1_wordtotal = c1_wordtotal + c1_n                     \n",
      "        elif current_word == None:\n",
      "            current_word, ham_count, spam_count  = word, c0_n, c1_n\n",
      "        else:\n",
      "            ham_prob = ham_count/float(c0_wordtotal)\n",
      "            spam_prob = spam_count/float(c1_wordtotal)\n",
      "            print(f'{current_word}\\t{ham_count}\\t{spam_count}\\t{ham_prob}\\t{spam_prob}')\n",
      "            current_word, ham_count, spam_count  = word, c0_n, c1_n\n",
      "            \n",
      "if c0_wordtotal != None and c1_wordtotal != None:\n",
      "    ham_prob = ham_count/float(c0_wordtotal)\n",
      "    spam_prob = spam_count/float(c1_wordtotal)        \n",
      "    print(f'{current_word}\\t{ham_count}\\t{spam_count}\\t{ham_prob}\\t{spam_prob}')\n",
      "    print(f'{\"ClassPriors\"}\\t{c0_total}\\t{c1_total}\\t{c0_total/(c0_total+c1_total)}\\t{c1_total/(c0_total+c1_total)}')\n",
      "\n",
      "##################### (END) CODE HERE ####################"
     ]
    }
   ],
   "source": [
    "!cat NaiveBayes/train_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\n",
      "\n",
      "import os\n",
      "import sys                                                  \n",
      "import numpy as np  \n",
      "\n",
      "#################### YOUR CODE HERE ###################\n",
      "# confirm that we have access to the model file\n",
      "assert 'NBmodel.txt' in os.listdir('.'), \"ERROR: can't find NBmodel.txt\"\n",
      "\n",
      "# load the model into a dictionary for easy access\n",
      "uniquewc =0\n",
      "for record in open('NBmodel.txt', 'r').readlines():\n",
      "    word = record.split('\\t')[0]\n",
      "    if word == 'ClassPriors':\n",
      "        continue\n",
      "    uniquewc = uniquewc + 1\n",
      "        \n",
      "        \n",
      "    \n",
      "c0_total = None\n",
      "c1_total = None\n",
      "c0_wordtotal = None\n",
      "c1_wordtotal = None\n",
      "#uniquewc = len(unique_words)\n",
      "\n",
      "current_word = None\n",
      "ham_count = 0\n",
      "spam_count = 0\n",
      "ham_prob = 0.0\n",
      "spam_prob = 0.0\n",
      "\n",
      "for line in sys.stdin:\n",
      "    # parse input\n",
      "    pk, word, c0_n, c1_n = line.split('\\t')\n",
      "    c0_n = int(c0_n)\n",
      "    c1_n = int(c1_n)\n",
      "    if word == current_word:\n",
      "        spam_count = spam_count + c1_n\n",
      "        ham_count = ham_count + c0_n\n",
      "    else:\n",
      "        if word[0] == '*':\n",
      "            if word == '*doccount':\n",
      "                if c0_total == None:\n",
      "                    c0_total = c0_n\n",
      "                    c1_total = c1_n\n",
      "                else:\n",
      "                    c0_total = c0_total + c0_n\n",
      "                    c1_total = c1_total + c1_n\n",
      "            if word == '*wordcount':\n",
      "                if c0_wordtotal == None:\n",
      "                    c0_wordtotal = c0_n\n",
      "                    c1_wordtotal = c1_n\n",
      "                else:\n",
      "                    c0_wordtotal = c0_wordtotal + c0_n\n",
      "                    c1_wordtotal = c1_wordtotal + c1_n                     \n",
      "        elif current_word == None:\n",
      "            current_word, ham_count, spam_count  = word, c0_n, c1_n\n",
      "        else:\n",
      "            ham_prob = (ham_count+1)/(float(c0_wordtotal)+float(uniquewc))\n",
      "            spam_prob = (spam_count+1)/(float(c1_wordtotal)+float(uniquewc))\n",
      "            #print(f'{current_word}\\t{ham_count}\\t{spam_count}\\t{ham_prob}\\t{spam_prob}')\n",
      "            print(f'{current_word}\\t{ham_count},{spam_count},{ham_prob},{spam_prob}')\n",
      "            current_word, ham_count, spam_count  = word, c0_n, c1_n\n",
      "            \n",
      "if c0_wordtotal != None and c1_wordtotal != None:\n",
      "    ham_prob = (ham_count+1)/(float(c0_wordtotal)+float(uniquewc))\n",
      "    spam_prob = (spam_count+1)/(float(c1_wordtotal)+float(uniquewc))        \n",
      "    #print(f'{current_word}\\t{ham_count}\\t{spam_count}\\t{ham_prob}\\t{spam_prob}')\n",
      "    #print(f'{\"ClassPriors\"}\\t{c0_total}\\t{c1_total}\\t{c0_total/(c0_total+c1_total)}\\t{c1_total/(c0_total+c1_total)}')\n",
      "    print(f'{current_word}\\t{ham_count},{spam_count},{ham_prob},{spam_prob}')\n",
      "    print(f'{\"ClassPriors\"}\\t{c0_total},{c1_total},{c0_total/(c0_total+c1_total)},{c1_total/(c0_total+c1_total)}')\n",
      "#################### (END) YOUR CODE ###################"
     ]
    }
   ],
   "source": [
    "!cat NaiveBayes/train_reducer_smooth.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`part b starts here`:__ MNB _without_ Smoothing (training on Chinese Example Corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t*doccount\t1\t3\n",
      "0\t*wordcount\t3\t8\n",
      "0\tbeijing\t0\t1\n",
      "0\tchinese\t0\t1\n",
      "0\tchinese\t0\t1\n",
      "0\tchinese\t0\t1\n",
      "0\tchinese\t0\t1\n",
      "0\tchinese\t0\t1\n",
      "0\tchinese\t1\t0\n",
      "1\t*doccount\t1\t3\n",
      "1\t*wordcount\t3\t8\n",
      "1\tjapan\t1\t0\n",
      "1\tmacao\t0\t1\n",
      "2\t*doccount\t1\t3\n",
      "2\t*wordcount\t3\t8\n",
      "2\tshanghai\t0\t1\n",
      "2\ttokyo\t1\t0\n"
     ]
    }
   ],
   "source": [
    "# part b - write a unit test for your mapper here\n",
    "!cat NaiveBayes/chineseTrain.txt | NaiveBayes/train_mapper.py | sort -k1n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beijing\t0\t1\t0.0\t0.125\n",
      "chinese\t1\t5\t0.3333333333333333\t0.625\n",
      "ClassPriors\t1\t3\t0.25\t0.75\n"
     ]
    }
   ],
   "source": [
    "# part b - write a systems test for your mapper + reducer together here\n",
    "!cat NaiveBayes/chineseTrain.txt | NaiveBayes/train_mapper.py | sort -k1n | sed -n 1,9p | NaiveBayes/train_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "japan\t1\t0\t0.3333333333333333\t0.0\n",
      "macao\t0\t1\t0.0\t0.125\n",
      "ClassPriors\t1\t3\t0.25\t0.75\n"
     ]
    }
   ],
   "source": [
    "# part b - write a systems test for your mapper + reducer together here\n",
    "!cat NaiveBayes/chineseTrain.txt | NaiveBayes/train_mapper.py | sort -k1n | sed -n 10,13p | NaiveBayes/train_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shanghai\t0\t1\t0.0\t0.125\n",
      "tokyo\t1\t0\t0.3333333333333333\t0.0\n",
      "ClassPriors\t1\t3\t0.25\t0.75\n"
     ]
    }
   ],
   "source": [
    "# part b - write a systems test for your mapper + reducer together here\n",
    "!cat NaiveBayes/chineseTrain.txt | NaiveBayes/train_mapper.py | sort -k1n | sed -n 14,17p | NaiveBayes/train_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/chinese-output\n"
     ]
    }
   ],
   "source": [
    "# part b - clear (and name) an output directory in HDFS for your unsmoothed chinese NB model\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/chinese-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.2.2.jar] /tmp/streamjob1010769798146897951.jar tmpDir=null\n",
      "2022-01-31 00:17:23,865 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.128.0.2:8032\n",
      "2022-01-31 00:17:24,108 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.128.0.2:10200\n",
      "2022-01-31 00:17:24,535 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.128.0.2:8032\n",
      "2022-01-31 00:17:24,536 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.128.0.2:10200\n",
      "2022-01-31 00:17:24,699 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1643556265243_0007\n",
      "2022-01-31 00:17:25,028 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2022-01-31 00:17:25,090 INFO mapreduce.JobSubmitter: number of splits:10\n",
      "2022-01-31 00:17:25,242 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1643556265243_0007\n",
      "2022-01-31 00:17:25,244 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-01-31 00:17:25,513 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-01-31 00:17:25,513 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-01-31 00:17:25,573 INFO impl.YarnClientImpl: Submitted application application_1643556265243_0007\n",
      "2022-01-31 00:17:25,607 INFO mapreduce.Job: The url to track the job: http://w261-m:8088/proxy/application_1643556265243_0007/\n",
      "2022-01-31 00:17:25,608 INFO mapreduce.Job: Running job: job_1643556265243_0007\n",
      "2022-01-31 00:17:32,780 INFO mapreduce.Job: Job job_1643556265243_0007 running in uber mode : false\n",
      "2022-01-31 00:17:32,781 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-01-31 00:17:40,888 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2022-01-31 00:17:47,968 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "2022-01-31 00:17:54,028 INFO mapreduce.Job:  map 90% reduce 0%\n",
      "2022-01-31 00:17:57,041 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2022-01-31 00:18:04,204 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "2022-01-31 00:18:05,209 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-01-31 00:18:07,223 INFO mapreduce.Job: Job job_1643556265243_0007 completed successfully\n",
      "2022-01-31 00:18:07,312 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1299\n",
      "\t\tFILE: Number of bytes written=3245483\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1535\n",
      "\t\tHDFS: Number of bytes written=246\n",
      "\t\tHDFS: Number of read operations=45\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=10\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=10\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=164026788\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=44016732\n",
      "\t\tTotal time spent by all map tasks (ms)=51973\n",
      "\t\tTotal time spent by all reduce tasks (ms)=13947\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=51973\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=13947\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=164026788\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=44016732\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=71\n",
      "\t\tMap output bytes=1139\n",
      "\t\tMap output materialized bytes=1461\n",
      "\t\tInput split bytes=960\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce shuffle bytes=1461\n",
      "\t\tReduce input records=71\n",
      "\t\tReduce output records=9\n",
      "\t\tSpilled Records=142\n",
      "\t\tShuffled Maps =30\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=30\n",
      "\t\tGC time elapsed (ms)=1629\n",
      "\t\tCPU time spent (ms)=8660\n",
      "\t\tPhysical memory (bytes) snapshot=6008532992\n",
      "\t\tVirtual memory (bytes) snapshot=57556434944\n",
      "\t\tTotal committed heap usage (bytes)=5949620224\n",
      "\t\tPeak Map Physical memory (bytes)=552640512\n",
      "\t\tPeak Map Virtual memory (bytes)=4427415552\n",
      "\t\tPeak Reduce Physical memory (bytes)=280186880\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4430958592\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=575\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=246\n",
      "2022-01-31 00:18:07,312 INFO streaming.StreamJob: Output directory: /user/root/HW2//chinese-output\n"
     ]
    }
   ],
   "source": [
    "# part b - write your hadoop streaming job\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k1,1nr\"  \\\n",
    "  -D mapreduce_job_reduces=3 \\\n",
    "  -files NaiveBayes/train_mapper.py,NaiveBayes/train_reducer.py \\\n",
    "  -mapper train_mapper.py \\\n",
    "  -reducer train_reducer.py \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -input {HDFS_DIR}/chineseTrain.txt \\\n",
    "  -output {HDFS_DIR}/chinese-output \\\n",
    "  -cmdenv PATH={PATH}\\\n",
    "  -numReduceTasks 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part b - extract your results (i.e. model) to a local file\n",
    "!hdfs dfs -cat {HDFS_DIR}/chinese-output/part-0000* > NaiveBayes/NBmodel.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassPriors\t1\t3\t0.25\t0.75\n",
      "ClassPriors\t1\t3\t0.25\t0.75\n",
      "ClassPriors\t1\t3\t0.25\t0.75\n",
      "beijing\t0\t1\t0.0\t0.125\n",
      "chinese\t1\t5\t0.3333333333333333\t0.625\n",
      "japan\t1\t0\t0.3333333333333333\t0.0\n",
      "macao\t0\t1\t0.0\t0.125\n",
      "shanghai\t0\t1\t0.0\t0.125\n",
      "tokyo\t1\t0\t0.3333333333333333\t0.0\n"
     ]
    }
   ],
   "source": [
    "# part b - print your model so that we can confirm that it matches expected results\n",
    "!cat NaiveBayes/NBmodel.txt | sort -k1n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1\t1\t\tChinese Beijing Chinese\n",
      "D2\t1\t\tChinese Chinese Shanghai\n",
      "D3\t1\t\tChinese Macao\n",
      "D4\t0\t\tTokyo Japan Chinese\n"
     ]
    }
   ],
   "source": [
    "!cat NaiveBayes/chineseTrain.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`part e starts here`:__ MNB _with_ Smoothing (training on Chinese Example Corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod a+x NaiveBayes/train_reducer_smooth.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beijing\t0,1,0.1111111111111111,0.14285714285714285\n",
      "chinese\t1,5,0.2222222222222222,0.42857142857142855\n",
      "ClassPriors\t1,3,0.25,0.75\n"
     ]
    }
   ],
   "source": [
    "# part e - write a unit test for your NEW reducer here\n",
    "!cat NaiveBayes/chineseTrain.txt | NaiveBayes/train_mapper.py | sort -k1n | sed -n 1,9p | NaiveBayes/train_reducer_smooth.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part e - write a systems test for your mapper + reducer together here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/chinese-output-smooth\n"
     ]
    }
   ],
   "source": [
    "# part e - clear (and name) an output directory in HDFS for your SMOOTHED chinese NB model\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/chinese-output-smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.2.2.jar] /tmp/streamjob2836118689315485243.jar tmpDir=null\n",
      "2022-01-31 00:19:12,492 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.128.0.2:8032\n",
      "2022-01-31 00:19:12,734 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.128.0.2:10200\n",
      "2022-01-31 00:19:13,213 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.128.0.2:8032\n",
      "2022-01-31 00:19:13,213 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.128.0.2:10200\n",
      "2022-01-31 00:19:13,391 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1643556265243_0008\n",
      "2022-01-31 00:19:13,731 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2022-01-31 00:19:14,204 INFO mapreduce.JobSubmitter: number of splits:10\n",
      "2022-01-31 00:19:14,335 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1643556265243_0008\n",
      "2022-01-31 00:19:14,336 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-01-31 00:19:14,549 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-01-31 00:19:14,549 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-01-31 00:19:14,607 INFO impl.YarnClientImpl: Submitted application application_1643556265243_0008\n",
      "2022-01-31 00:19:14,639 INFO mapreduce.Job: The url to track the job: http://w261-m:8088/proxy/application_1643556265243_0008/\n",
      "2022-01-31 00:19:14,641 INFO mapreduce.Job: Running job: job_1643556265243_0008\n",
      "2022-01-31 00:19:21,787 INFO mapreduce.Job: Job job_1643556265243_0008 running in uber mode : false\n",
      "2022-01-31 00:19:21,788 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-01-31 00:19:29,894 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2022-01-31 00:19:36,943 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "2022-01-31 00:19:41,989 INFO mapreduce.Job:  map 70% reduce 0%\n",
      "2022-01-31 00:19:42,996 INFO mapreduce.Job:  map 90% reduce 0%\n",
      "2022-01-31 00:19:46,023 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2022-01-31 00:19:52,269 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "2022-01-31 00:19:54,281 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-01-31 00:19:56,299 INFO mapreduce.Job: Job job_1643556265243_0008 completed successfully\n",
      "2022-01-31 00:19:56,383 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1299\n",
      "\t\tFILE: Number of bytes written=3249799\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1535\n",
      "\t\tHDFS: Number of bytes written=379\n",
      "\t\tHDFS: Number of read operations=45\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=10\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=10\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=163638600\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=42211500\n",
      "\t\tTotal time spent by all map tasks (ms)=51850\n",
      "\t\tTotal time spent by all reduce tasks (ms)=13375\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=51850\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=13375\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=163638600\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=42211500\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=71\n",
      "\t\tMap output bytes=1139\n",
      "\t\tMap output materialized bytes=1461\n",
      "\t\tInput split bytes=960\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce shuffle bytes=1461\n",
      "\t\tReduce input records=71\n",
      "\t\tReduce output records=9\n",
      "\t\tSpilled Records=142\n",
      "\t\tShuffled Maps =30\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=30\n",
      "\t\tGC time elapsed (ms)=1843\n",
      "\t\tCPU time spent (ms)=9030\n",
      "\t\tPhysical memory (bytes) snapshot=5921435648\n",
      "\t\tVirtual memory (bytes) snapshot=57552531456\n",
      "\t\tTotal committed heap usage (bytes)=5837422592\n",
      "\t\tPeak Map Physical memory (bytes)=550203392\n",
      "\t\tPeak Map Virtual memory (bytes)=4427923456\n",
      "\t\tPeak Reduce Physical memory (bytes)=281018368\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4430278656\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=575\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=379\n",
      "2022-01-31 00:19:56,384 INFO streaming.StreamJob: Output directory: /user/root/HW2//chinese-output-smooth\n"
     ]
    }
   ],
   "source": [
    "# part e - write your hadoop streaming job\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k1,1nr\"  \\\n",
    "  -D mapreduce_job_reduces=3 \\\n",
    "  -files NaiveBayes/train_mapper.py,NaiveBayes/train_reducer_smooth.py,NaiveBayes/NBmodel.txt \\\n",
    "  -mapper train_mapper.py \\\n",
    "  -reducer train_reducer_smooth.py \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -input {HDFS_DIR}/chineseTrain.txt \\\n",
    "  -output {HDFS_DIR}/chinese-output-smooth \\\n",
    "  -cmdenv PATH={PATH}\\\n",
    "  -numReduceTasks 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part e - extract your results (i.e. model) to a local file\n",
    "!hdfs dfs -cat {HDFS_DIR}/chinese-output-smooth/part-0000* > NaiveBayes/NBmodel_smooth.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beijing\t0,1,0.1111111111111111,0.14285714285714285\n",
      "chinese\t1,5,0.2222222222222222,0.42857142857142855\n",
      "ClassPriors\t1,3,0.25,0.75\n",
      "japan\t1,0,0.2222222222222222,0.07142857142857142\n",
      "macao\t0,1,0.1111111111111111,0.14285714285714285\n",
      "ClassPriors\t1,3,0.25,0.75\n",
      "shanghai\t0,1,0.1111111111111111,0.14285714285714285\n",
      "tokyo\t1,0,0.2222222222222222,0.07142857142857142\n",
      "ClassPriors\t1,3,0.25,0.75\n"
     ]
    }
   ],
   "source": [
    "# part e - print your model (above local file) so that we can confirm that it matches expected results\n",
    "!cat NaiveBayes/NBmodel_smooth.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9: Enron Ham/Spam NB Classifier & Results.\n",
    "\n",
    "Fantastic work. We're finally ready to perform Spam Classification on the Enron Corpus. In this question you'll run the analysis you've developed, report its performance.\n",
    "\n",
    "### Q9 Tasks:\n",
    "* __a) train/test split:__ Run the provided code to split our Enron file into a training set and testing set then load them into HDFS. [`NOTE:` _Make sure you re calculate the vocab size for just the training set!_]\n",
    "\n",
    "* __b) train 2 models:__ Write Hadoop Streaming jobs to train MNB Models on the training set with and without smoothing. Save your models to local files at __`NaiveBayes/Unsmoothed/NBmodel.txt`__ and __`NaiveBayes/Smoothed/NBmodel.txt`__. [`NOTE:` _This naming is important because we wrote our classification task so that it expects a file of that name... if this inelegance frustrates you there is an alternative that would involve a few adjustments to your code [read more about it here](http://www.tnoda.com/blog/2013-11-23)._] Finally run the checks that we provide to confirm that your results are correct.\n",
    "\n",
    "\n",
    "* __c) code:__ Recall that we designed our classification job with just a mapper. An efficient way to report the performance of our models would be to simply add a reducer phase to this job and compute precision and recall right there. Complete the code in __`NaiveBayes/evaluation_reducer.py`__ and then write Hadoop jobs to evaluate your two models on the test set. Report their performance side by side. [`NOTE:` if you need a refresher on precision, recall and F1-score [Wikipedia](https://en.wikipedia.org/wiki/F1_score) is a good resource.]\n",
    "\n",
    "\n",
    "* __d) short response:__ Compare the performance of your two models. What do you notice about the unsmoothed model's predictions? Can you guess why this is happening? Which evaluation measure do you think is most relevant in our use case? [`NOTE:` _Feel free to answer using your common sense but if you want more information on evaluating the classification task checkout_ [this blogpost](https://tryolabs.com/blog/2013/03/25/why-accuracy-alone-bad-measure-classification-tasks-and-what-we-can-do-about-it/\n",
    ") or [this paper](http://www.flinders.edu.au/science_engineering/fms/School-CSEM/publications/tech_reps-research_artfcts/TRRA_2007.pdf\n",
    ")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9 Student Answers:\n",
    "> __d)__ Overall the F1-score of the smoothened model improved substantially from 0.16 to 0.88. **Precision answers the following question: out of all the examples the classifier labeled as positive, what fraction were correct?** On the otherhand, **recall answers the following question: out of all the positive examples what fraction were correct?**.The unsmoothened model gave out 1.0 and 0.0909 as precision and recall respectively. Unsmoothened model has only predicted the classification correctly only 50% times. So clearly, the model didn't do good at all. Tweaking a classifier is a matter of balancing precision and recall. It is possible to get both up: one may choose to optimize a measure that combines precision and recall into a single value, such as the F-measure. F-score is the harmonic mean of of precision and recall. As we noticed in the below example, in our smoothened case the precision was at 0.7857 and the accuracy at 0.85. The smoothened model has balanced the precision and recall score and hence its F1-score was far better than the unsmoothened model. For this use case of identifying the mail as either spam or ham, it is important that we don't classify a ham as a spam as it may have significant business impact. So, we think recall is a critical metric as it measures percentage of correct predictions of spam over total spam in the dataset. As per the metrics listed below smoothened model has a highest recall score over the unsmoothened model with a score of just 0.0909."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test/Train split__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/user/root/HW2/enron_train.txt': File exists\n",
      "copyFromLocal: `/user/root/HW2/enron_test.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "# part a - test/train split (RUN THIS CELL AS IS)\n",
    "!head -n 80 data/enronemail_1h.txt > data/enron_train.txt\n",
    "!tail -n 20 data/enronemail_1h.txt > data/enron_test.txt\n",
    "!hdfs dfs -copyFromLocal data/enron_train.txt {HDFS_DIR}\n",
    "!hdfs dfs -copyFromLocal data/enron_test.txt {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training__ (Enron MNB Model _without smoothing_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/enron-model\n",
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.2.2.jar] /tmp/streamjob7083438271505124556.jar tmpDir=null\n",
      "2022-01-31 00:20:36,269 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.128.0.2:8032\n",
      "2022-01-31 00:20:36,510 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.128.0.2:10200\n",
      "2022-01-31 00:20:36,947 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.128.0.2:8032\n",
      "2022-01-31 00:20:36,947 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.128.0.2:10200\n",
      "2022-01-31 00:20:37,122 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1643556265243_0009\n",
      "2022-01-31 00:20:37,844 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2022-01-31 00:20:37,901 INFO mapreduce.JobSubmitter: number of splits:9\n",
      "2022-01-31 00:20:38,037 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1643556265243_0009\n",
      "2022-01-31 00:20:38,039 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-01-31 00:20:38,262 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-01-31 00:20:38,262 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-01-31 00:20:38,324 INFO impl.YarnClientImpl: Submitted application application_1643556265243_0009\n",
      "2022-01-31 00:20:38,356 INFO mapreduce.Job: The url to track the job: http://w261-m:8088/proxy/application_1643556265243_0009/\n",
      "2022-01-31 00:20:38,358 INFO mapreduce.Job: Running job: job_1643556265243_0009\n",
      "2022-01-31 00:20:46,453 INFO mapreduce.Job: Job job_1643556265243_0009 running in uber mode : false\n",
      "2022-01-31 00:20:46,455 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-01-31 00:20:54,568 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2022-01-31 00:21:00,627 INFO mapreduce.Job:  map 44% reduce 0%\n",
      "2022-01-31 00:21:01,639 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2022-01-31 00:21:05,680 INFO mapreduce.Job:  map 78% reduce 0%\n",
      "2022-01-31 00:21:07,696 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2022-01-31 00:21:14,743 INFO mapreduce.Job:  map 100% reduce 20%\n",
      "2022-01-31 00:21:17,764 INFO mapreduce.Job:  map 100% reduce 60%\n",
      "2022-01-31 00:21:19,779 INFO mapreduce.Job:  map 100% reduce 80%\n",
      "2022-01-31 00:21:21,790 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-01-31 00:21:23,807 INFO mapreduce.Job: Job job_1643556265243_0009 completed successfully\n",
      "2022-01-31 00:21:23,889 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=345382\n",
      "\t\tFILE: Number of bytes written=4183347\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=213073\n",
      "\t\tHDFS: Number of bytes written=187051\n",
      "\t\tHDFS: Number of read operations=52\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=15\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=9\n",
      "\t\tLaunched reduce tasks=5\n",
      "\t\tData-local map tasks=9\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=158043012\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=70744896\n",
      "\t\tTotal time spent by all map tasks (ms)=50077\n",
      "\t\tTotal time spent by all reduce tasks (ms)=22416\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=50077\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=22416\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=158043012\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=70744896\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=80\n",
      "\t\tMap output records=25157\n",
      "\t\tMap output bytes=295038\n",
      "\t\tMap output materialized bytes=345622\n",
      "\t\tInput split bytes=855\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4565\n",
      "\t\tReduce shuffle bytes=345622\n",
      "\t\tReduce input records=25157\n",
      "\t\tReduce output records=4560\n",
      "\t\tSpilled Records=50314\n",
      "\t\tShuffled Maps =45\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=45\n",
      "\t\tGC time elapsed (ms)=2002\n",
      "\t\tCPU time spent (ms)=14470\n",
      "\t\tPhysical memory (bytes) snapshot=5915287552\n",
      "\t\tVirtual memory (bytes) snapshot=61991145472\n",
      "\t\tTotal committed heap usage (bytes)=5910298624\n",
      "\t\tPeak Map Physical memory (bytes)=553152512\n",
      "\t\tPeak Map Virtual memory (bytes)=4428075008\n",
      "\t\tPeak Reduce Physical memory (bytes)=285429760\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4431130624\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=212218\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=187051\n",
      "2022-01-31 00:21:23,889 INFO streaming.StreamJob: Output directory: /user/root/HW2//enron-model\n"
     ]
    }
   ],
   "source": [
    "# part b -  Unsmoothed model (FILL IN THE MISSING CODE BELOW)\n",
    "\n",
    "# clear the output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/enron-model\n",
    "\n",
    "# hadoop command\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k1,1nr\"  \\\n",
    "  -D mapreduce_job_reduces=5 \\\n",
    "  -files NaiveBayes/train_mapper.py,NaiveBayes/train_reducer.py \\\n",
    "  -mapper train_mapper.py \\\n",
    "  -reducer train_reducer.py \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -input {HDFS_DIR}/enron_train.txt \\\n",
    "  -output {HDFS_DIR}/enron-model \\\n",
    "  -cmdenv PATH={PATH}\\\n",
    "  -numReduceTasks 5\n",
    "\n",
    "# save the model locally\n",
    "!rm -rf NaiveBayes/Unsmoothed\n",
    "!mkdir NaiveBayes/Unsmoothed\n",
    "!hdfs dfs -cat {HDFS_DIR}/enron-model/part-000* > NaiveBayes/Unsmoothed/NBmodel.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t2\t4\t0.0001725476662928134\t0.00029682398337785694\n"
     ]
    }
   ],
   "source": [
    "# part b - check your UNSMOOTHED model results (RUN THIS CELL AS IS)\n",
    "!grep assistance NaiveBayes/Unsmoothed/NBmodel.txt\n",
    "# EXPECTED OUTPUT: assistance\t2,4,0.000172547666293,0.000296823983378"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "money\t1\t22\t8.62738331464067e-05\t0.001632531908578213\n"
     ]
    }
   ],
   "source": [
    "# part b - check your UNSMOOTHED model results (RUN THIS CELL AS IS)\n",
    "!grep money NaiveBayes/Unsmoothed/NBmodel.txt\n",
    "# EXPECTED OUTPUT: money\t1,22,8.62738331464e-05,0.00163253190858"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training__ (Enron MNB Model _with Laplace +1 smoothing_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/smooth-model\n",
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.2.2.jar] /tmp/streamjob3202876487256201386.jar tmpDir=null\n",
      "2022-01-31 00:21:49,054 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.128.0.2:8032\n",
      "2022-01-31 00:21:49,302 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.128.0.2:10200\n",
      "2022-01-31 00:21:49,740 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.128.0.2:8032\n",
      "2022-01-31 00:21:49,740 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.128.0.2:10200\n",
      "2022-01-31 00:21:49,908 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1643556265243_0010\n",
      "2022-01-31 00:21:50,284 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2022-01-31 00:21:50,354 INFO mapreduce.JobSubmitter: number of splits:9\n",
      "2022-01-31 00:21:50,512 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1643556265243_0010\n",
      "2022-01-31 00:21:50,514 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-01-31 00:21:50,777 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-01-31 00:21:50,777 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-01-31 00:21:50,840 INFO impl.YarnClientImpl: Submitted application application_1643556265243_0010\n",
      "2022-01-31 00:21:50,873 INFO mapreduce.Job: The url to track the job: http://w261-m:8088/proxy/application_1643556265243_0010/\n",
      "2022-01-31 00:21:50,874 INFO mapreduce.Job: Running job: job_1643556265243_0010\n",
      "2022-01-31 00:21:58,974 INFO mapreduce.Job: Job job_1643556265243_0010 running in uber mode : false\n",
      "2022-01-31 00:21:58,975 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-01-31 00:22:07,092 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2022-01-31 00:22:13,159 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2022-01-31 00:22:20,215 INFO mapreduce.Job:  map 89% reduce 0%\n",
      "2022-01-31 00:22:21,220 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2022-01-31 00:22:27,262 INFO mapreduce.Job:  map 100% reduce 20%\n",
      "2022-01-31 00:22:30,534 INFO mapreduce.Job:  map 100% reduce 60%\n",
      "2022-01-31 00:22:32,549 INFO mapreduce.Job:  map 100% reduce 80%\n",
      "2022-01-31 00:22:34,559 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-01-31 00:22:35,569 INFO mapreduce.Job: Job job_1643556265243_0010 completed successfully\n",
      "2022-01-31 00:22:35,661 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=345382\n",
      "\t\tFILE: Number of bytes written=4188107\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=213073\n",
      "\t\tHDFS: Number of bytes written=254300\n",
      "\t\tHDFS: Number of read operations=52\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=15\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=9\n",
      "\t\tLaunched reduce tasks=5\n",
      "\t\tData-local map tasks=9\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=167851860\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=74418480\n",
      "\t\tTotal time spent by all map tasks (ms)=53185\n",
      "\t\tTotal time spent by all reduce tasks (ms)=23580\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=53185\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=23580\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=167851860\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=74418480\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=80\n",
      "\t\tMap output records=25157\n",
      "\t\tMap output bytes=295038\n",
      "\t\tMap output materialized bytes=345622\n",
      "\t\tInput split bytes=855\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4565\n",
      "\t\tReduce shuffle bytes=345622\n",
      "\t\tReduce input records=25157\n",
      "\t\tReduce output records=4560\n",
      "\t\tSpilled Records=50314\n",
      "\t\tShuffled Maps =45\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=45\n",
      "\t\tGC time elapsed (ms)=1876\n",
      "\t\tCPU time spent (ms)=14940\n",
      "\t\tPhysical memory (bytes) snapshot=6022905856\n",
      "\t\tVirtual memory (bytes) snapshot=62002282496\n",
      "\t\tTotal committed heap usage (bytes)=5983698944\n",
      "\t\tPeak Map Physical memory (bytes)=553484288\n",
      "\t\tPeak Map Virtual memory (bytes)=4428333056\n",
      "\t\tPeak Reduce Physical memory (bytes)=301338624\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4433223680\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=212218\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=254300\n",
      "2022-01-31 00:22:35,662 INFO streaming.StreamJob: Output directory: /user/root/HW2//smooth-model\n"
     ]
    }
   ],
   "source": [
    "# part b -  Smoothed model (FILL IN THE MISSING CODE BELOW)\n",
    "\n",
    "# clear the output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/smooth-model\n",
    "\n",
    "# hadoop command\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k1,1nr\"  \\\n",
    "  -D mapreduce_job_reduces=5 \\\n",
    "  -files NaiveBayes/train_mapper.py,NaiveBayes/train_reducer_smooth.py,NaiveBayes/Unsmoothed/NBmodel.txt \\\n",
    "  -mapper train_mapper.py \\\n",
    "  -reducer train_reducer_smooth.py \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -input {HDFS_DIR}/enron_train.txt \\\n",
    "  -output {HDFS_DIR}/smooth-model \\\n",
    "  -cmdenv PATH={PATH}\\\n",
    "  -numReduceTasks 5\n",
    "\n",
    "# save the model locally\n",
    "!rm -rf NaiveBayes/Smoothed\n",
    "!mkdir NaiveBayes/Smoothed\n",
    "!hdfs dfs -cat {HDFS_DIR}/smooth-model/part-000* > NaiveBayes/Smoothed/NBmodel.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t2,4,0.0001858045336306206,0.00027730020520215184\n"
     ]
    }
   ],
   "source": [
    "# part b - check your SMOOTHED model results (RUN THIS CELL AS IS)\n",
    "!grep assistance NaiveBayes/Smoothed/NBmodel.txt\n",
    "# EXPECTED OUTPUT: assistance\t2,4,0.000185804533631,0.000277300205202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "money\t1,22,0.0001238696890870804,0.0012755809439298986\n"
     ]
    }
   ],
   "source": [
    "# part b - check your SMOOTHED model results (RUN THIS CELL AS IS)\n",
    "!grep money NaiveBayes/Smoothed/NBmodel.txt\n",
    "# EXPECTED OUTPUT: money\t1,22,0.000123869689087,0.00127558094393"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Evaluation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - write your code in NaiveBayes/evaluation_reducer.py then RUN THIS\n",
    "!chmod a+x NaiveBayes/evaluation_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\n",
      "\"\"\"\n",
      "Reducer to calculate precision and recall as part\n",
      "of the inference phase of Naive Bayes.\n",
      "INPUT:\n",
      "    ID \\t true_class \\t P(ham|doc) \\t P(spam|doc) \\t predicted_class\n",
      "OUTPUT:\n",
      "    precision \\t ##\n",
      "    recall \\t ##\n",
      "    accuracy \\t ##\n",
      "    F-score \\t ##\n",
      "         \n",
      "Instructions:\n",
      "    Complete the missing code to compute these^ four\n",
      "    evaluation measures for our classification task.\n",
      "    \n",
      "    Note: if you have no True Positives you will not \n",
      "    be able to compute the F1 score (and maybe not \n",
      "    precision/recall). Your code should handle this \n",
      "    case appropriately feel free to interpret the \n",
      "    \"output format\" above as a rough suggestion. It\n",
      "    may be helpful to also print the counts for true\n",
      "    positives, false positives, etc.\n",
      "\"\"\"\n",
      "import sys\n",
      "\n",
      "# initialize counters\n",
      "FP = 0.0 # false positives\n",
      "FN = 0.0 # false negatives\n",
      "TP = 0.0 # true positives\n",
      "TN = 0.0 # true negatives\n",
      "NDOCS = 0\n",
      "\n",
      "# read from STDIN\n",
      "for line in sys.stdin:\n",
      "    # parse input\n",
      "    docID, class_, pHam, pSpam, pred = line.split()\n",
      "    # emit classification results first\n",
      "    print(line[:-2], class_ == pred)\n",
      "    \n",
      "    # then compute evaluation stats\n",
      "#################### YOUR CODE HERE ###################\n",
      "    NDOCS = NDOCS + 1\n",
      "    if class_ == pred:\n",
      "        if class_ == '1':\n",
      "            TP = TP + 1\n",
      "        else:\n",
      "            TN = TN + 1\n",
      "    else:\n",
      "        if pred == '1':\n",
      "            FP = FP + 1\n",
      "        else:\n",
      "            FN = FN + 1\n",
      "\n",
      "precision = TP/float(TP+FP) if (TP+FP) != 0.0 else 'n/a'    \n",
      "recall = TP/float(TP+FN) if (TP+FN) != 0.0 else 'n/a'\n",
      "accuracy = (TP+TN)/float(TP+FP+TN+FN) if (TP+FP+TN+FN) != 0.0 else 'n/a'\n",
      "f_score = 2*precision*recall/(precision+recall) if (precision != 'n/a' and recall != 'n/a') != 0.0 else 'n/a'\n",
      "print(f'# Documenents:\\t{NDOCS}')\n",
      "print(f'True Positives\\t{TP}')\n",
      "print(f'True Negatives:\\t{TN}')\n",
      "print(f'False Positives\\t{FP}')\n",
      "print(f'False Negatives\\t{FN}')\n",
      "print(f'Accuracy\\t{accuracy:.4f}')\n",
      "print(f'Precision\\t{precision:.4f}')\n",
      "print(f'Recall\\t{recall:.4f}')\n",
      "print(f'F_score\\t{f_score:.4f}')\n",
      "#################### (END) YOUR CODE ###################\n",
      "    "
     ]
    }
   ],
   "source": [
    "!cat NaiveBayes/evaluation_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d5\t1\t-8.90668134500626\t-8.10769031284611\t1\n",
      "d6\t1\t-5.780743515794329\t-4.179502370564408\t1\n",
      "d7\t0\t-6.591673732011658\t-7.511706880737812\t0\n",
      "d8\t0\t-4.394449154674438\t-5.565796731681498\t0\n",
      "d5\t1\t-8.90668134500626\t-8.10769031284611\t True\n",
      "d6\t1\t-5.780743515794329\t-4.179502370564408\t True\n",
      "d7\t0\t-6.591673732011658\t-7.511706880737812\t True\n",
      "d8\t0\t-4.394449154674438\t-5.565796731681498\t True\n",
      "# Documenents:\t4\n",
      "True Positives\t2.0\n",
      "True Negatives:\t2.0\n",
      "False Positives\t0.0\n",
      "False Negatives\t0.0\n",
      "Accuracy\t1.0000\n",
      "Precision\t1.0000\n",
      "Recall\t1.0000\n",
      "F_score\t1.0000\n"
     ]
    }
   ],
   "source": [
    "# part c - unit test your evaluation job on the chinese model (RUN THIS CELL AS IS)\n",
    "!cat NaiveBayes/chineseTest.txt | NaiveBayes/classify_mapper.py \n",
    "!cat NaiveBayes/chineseTest.txt | NaiveBayes/classify_mapper.py | NaiveBayes/evaluation_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/evaluate-unsmoothed-model\n",
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.2.2.jar] /tmp/streamjob924028847229706262.jar tmpDir=null\n",
      "2022-01-31 00:23:20,946 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.128.0.2:8032\n",
      "2022-01-31 00:23:21,200 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.128.0.2:10200\n",
      "2022-01-31 00:23:21,668 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.128.0.2:8032\n",
      "2022-01-31 00:23:21,668 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.128.0.2:10200\n",
      "2022-01-31 00:23:21,842 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1643556265243_0011\n",
      "2022-01-31 00:23:22,173 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2022-01-31 00:23:22,231 INFO mapreduce.JobSubmitter: number of splits:9\n",
      "2022-01-31 00:23:22,372 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1643556265243_0011\n",
      "2022-01-31 00:23:22,374 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-01-31 00:23:22,599 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-01-31 00:23:22,599 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-01-31 00:23:22,658 INFO impl.YarnClientImpl: Submitted application application_1643556265243_0011\n",
      "2022-01-31 00:23:22,711 INFO mapreduce.Job: The url to track the job: http://w261-m:8088/proxy/application_1643556265243_0011/\n",
      "2022-01-31 00:23:22,716 INFO mapreduce.Job: Running job: job_1643556265243_0011\n",
      "2022-01-31 00:23:29,802 INFO mapreduce.Job: Job job_1643556265243_0011 running in uber mode : false\n",
      "2022-01-31 00:23:29,803 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-01-31 00:23:37,919 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2022-01-31 00:23:43,969 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2022-01-31 00:23:50,016 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2022-01-31 00:23:55,144 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-01-31 00:23:57,160 INFO mapreduce.Job: Job job_1643556265243_0011 completed successfully\n",
      "2022-01-31 00:23:57,247 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=818\n",
      "\t\tFILE: Number of bytes written=2488339\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=91491\n",
      "\t\tHDFS: Number of bytes written=1022\n",
      "\t\tHDFS: Number of read operations=32\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=9\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=9\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=159586296\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8344464\n",
      "\t\tTotal time spent by all map tasks (ms)=50566\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2644\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=50566\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2644\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=159586296\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=8344464\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=20\n",
      "\t\tMap output records=20\n",
      "\t\tMap output bytes=772\n",
      "\t\tMap output materialized bytes=866\n",
      "\t\tInput split bytes=846\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce shuffle bytes=866\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=29\n",
      "\t\tSpilled Records=40\n",
      "\t\tShuffled Maps =9\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=9\n",
      "\t\tGC time elapsed (ms)=1194\n",
      "\t\tCPU time spent (ms)=6620\n",
      "\t\tPhysical memory (bytes) snapshot=4847521792\n",
      "\t\tVirtual memory (bytes) snapshot=44272709632\n",
      "\t\tTotal committed heap usage (bytes)=4693426176\n",
      "\t\tPeak Map Physical memory (bytes)=555139072\n",
      "\t\tPeak Map Virtual memory (bytes)=4428460032\n",
      "\t\tPeak Reduce Physical memory (bytes)=237105152\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4431454208\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=90645\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1022\n",
      "2022-01-31 00:23:57,248 INFO streaming.StreamJob: Output directory: /user/root/HW2//evaluate-unsmoothed-model\n"
     ]
    }
   ],
   "source": [
    "# part c - Evaluate the UNSMOOTHED Model Here (FILL IN THE MISSING CODE)\n",
    "\n",
    "# clear output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/evaluate-unsmoothed-model\n",
    "\n",
    "# hadoop job\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files NaiveBayes/classify_mapper.py,NaiveBayes/evaluation_reducer.py,NaiveBayes/Unsmoothed/NBmodel.txt \\\n",
    "  -mapper classify_mapper.py \\\n",
    "  -reducer evaluation_reducer.py \\\n",
    "  -input {HDFS_DIR}/enron_test.txt \\\n",
    "  -output {HDFS_DIR}/evaluate-unsmoothed-model \\\n",
    "  -cmdenv PATH={PATH} \\\n",
    "  -numReduceTasks 1\n",
    "\n",
    "# retrieve results locally\n",
    "!rm -rf NaiveBayes/Unsmoothed/results.txt\n",
    "!hdfs dfs -cat {HDFS_DIR}/evaluate-unsmoothed-model/part-000* > NaiveBayes/Unsmoothed/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/evaluate-smoothed-model\n",
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.2.2.jar] /tmp/streamjob2737056886230107523.jar tmpDir=null\n",
      "2022-01-31 00:24:09,211 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.128.0.2:8032\n",
      "2022-01-31 00:24:09,458 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.128.0.2:10200\n",
      "2022-01-31 00:24:09,883 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.128.0.2:8032\n",
      "2022-01-31 00:24:09,883 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.128.0.2:10200\n",
      "2022-01-31 00:24:10,068 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1643556265243_0012\n",
      "2022-01-31 00:24:10,434 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2022-01-31 00:24:10,492 INFO mapreduce.JobSubmitter: number of splits:9\n",
      "2022-01-31 00:24:10,637 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1643556265243_0012\n",
      "2022-01-31 00:24:10,638 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-01-31 00:24:10,886 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-01-31 00:24:10,886 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-01-31 00:24:10,948 INFO impl.YarnClientImpl: Submitted application application_1643556265243_0012\n",
      "2022-01-31 00:24:10,980 INFO mapreduce.Job: The url to track the job: http://w261-m:8088/proxy/application_1643556265243_0012/\n",
      "2022-01-31 00:24:10,981 INFO mapreduce.Job: Running job: job_1643556265243_0012\n",
      "2022-01-31 00:24:19,075 INFO mapreduce.Job: Job job_1643556265243_0012 running in uber mode : false\n",
      "2022-01-31 00:24:19,076 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-01-31 00:24:26,170 INFO mapreduce.Job:  map 11% reduce 0%\n",
      "2022-01-31 00:24:27,185 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2022-01-31 00:24:33,244 INFO mapreduce.Job:  map 44% reduce 0%\n",
      "2022-01-31 00:24:34,249 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2022-01-31 00:24:40,312 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2022-01-31 00:24:45,342 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-01-31 00:24:47,357 INFO mapreduce.Job: Job job_1643556265243_0012 completed successfully\n",
      "2022-01-31 00:24:47,439 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1349\n",
      "\t\tFILE: Number of bytes written=2489371\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=91491\n",
      "\t\tHDFS: Number of bytes written=1546\n",
      "\t\tHDFS: Number of read operations=32\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=9\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=9\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=161694504\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8170884\n",
      "\t\tTotal time spent by all map tasks (ms)=51234\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2589\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=51234\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2589\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=161694504\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=8170884\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=20\n",
      "\t\tMap output records=20\n",
      "\t\tMap output bytes=1303\n",
      "\t\tMap output materialized bytes=1397\n",
      "\t\tInput split bytes=846\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce shuffle bytes=1397\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=29\n",
      "\t\tSpilled Records=40\n",
      "\t\tShuffled Maps =9\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=9\n",
      "\t\tGC time elapsed (ms)=1632\n",
      "\t\tCPU time spent (ms)=6780\n",
      "\t\tPhysical memory (bytes) snapshot=4844449792\n",
      "\t\tVirtual memory (bytes) snapshot=44273422336\n",
      "\t\tTotal committed heap usage (bytes)=4781506560\n",
      "\t\tPeak Map Physical memory (bytes)=559992832\n",
      "\t\tPeak Map Virtual memory (bytes)=4428279808\n",
      "\t\tPeak Reduce Physical memory (bytes)=233394176\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4430815232\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=90645\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1546\n",
      "2022-01-31 00:24:47,439 INFO streaming.StreamJob: Output directory: /user/root/HW2//evaluate-smoothed-model\n"
     ]
    }
   ],
   "source": [
    "# part c - Evaluate the SMOOTHED Model Here (FILL IN THE MISSING CODE)\n",
    "\n",
    "# clear output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/evaluate-smoothed-model\n",
    "\n",
    "# hadoop job\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files NaiveBayes/classify_mapper.py,NaiveBayes/evaluation_reducer.py,NaiveBayes/Smoothed/NBmodel.txt \\\n",
    "  -mapper classify_mapper.py \\\n",
    "  -reducer evaluation_reducer.py \\\n",
    "  -input {HDFS_DIR}/enron_test.txt \\\n",
    "  -output {HDFS_DIR}/evaluate-smoothed-model \\\n",
    "  -cmdenv PATH={PATH} \\\n",
    "  -numReduceTasks 1\n",
    "\n",
    "# retrieve results locally\n",
    "!rm -rf NaiveBayes/Smoothed/results.txt\n",
    "!hdfs dfs -cat {HDFS_DIR}/evaluate-smoothed-model/part-000* > NaiveBayes/Smoothed/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== UNSMOOTHED MODEL ============\n",
      "# Documenents:\t20\n",
      "True Positives\t1.0\n",
      "True Negatives:\t9.0\n",
      "False Positives\t0.0\n",
      "False Negatives\t10.0\n",
      "Accuracy\t0.5000\n",
      "Precision\t1.0000\n",
      "Recall\t0.0909\n",
      "F_score\t0.1667\n",
      "=========== SMOOTHED MODEL ============\n",
      "# Documenents:\t20\n",
      "True Positives\t11.0\n",
      "True Negatives:\t6.0\n",
      "False Positives\t3.0\n",
      "False Negatives\t0.0\n",
      "Accuracy\t0.8500\n",
      "Precision\t0.7857\n",
      "Recall\t1.0000\n",
      "F_score\t0.8800\n"
     ]
    }
   ],
   "source": [
    "# part c - display results \n",
    "# NOTE: feel free to modify the tail commands to match the format of your results file\n",
    "print('=========== UNSMOOTHED MODEL ============')\n",
    "!tail -n 9 NaiveBayes/Unsmoothed/results.txt\n",
    "print('=========== SMOOTHED MODEL ============')\n",
    "!tail -n 9 NaiveBayes/Smoothed/results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`EXPECTED RESULTS:`__ \n",
    "<table>\n",
    "<th>Unsmoothed Model</th>\n",
    "<th>Smoothed Model</th>\n",
    "<tr>\n",
    "<td><pre>\n",
    "# Documents:\t20\n",
    "True Positives:\t1\n",
    "True Negatives:\t9\n",
    "False Positives:\t0\n",
    "False Negatives:\t10\n",
    "Accuracy\t0.5\n",
    "Precision\t1.0\n",
    "Recall\t0.0909\n",
    "F-Score\t0.1666\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "# Documents:\t20\n",
    "True Positives:\t11\n",
    "True Negatives:\t6\n",
    "False Positives:\t3\n",
    "False Negatives:\t0\n",
    "Accuracy\t0.85\n",
    "Precision\t0.7857\n",
    "Recall\t1.0\n",
    "F-Score\t0.88\n",
    "</pre></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "__`NOTE:`__ _Don't be too disappointed if these seem low to you. We've trained and tested on a very very small corpus... bigger datasets coming soon!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10: Custom Partitioning and Secondary Sort\n",
    "\n",
    "Now that we have our model, we can analyse the results and think about future improvements.\n",
    "\n",
    "### Q10 Tasks:\n",
    "\n",
    "* __a) code + short response:__ Let's look at the top ten words with the highest conditional probability in `Spam` and in `Ham`. We'll do this by writing a Hadoop job that sorts the model file (`NaiveBayes/Smoothed/NBmodel.py`). Normally we'd have to run two jobs -- one that sorts on $P(word|ham)$ and another that sorts on $P(word|spam)$. However if we slighly modify the data format in the model file then we can get the top words in each class with just one job. We've written a mapper that will do just this for you. Read through __`NaiveBayes/model_sort_mapper.py`__ and then briefly explain how this mapper will allow us to partition and sort our model file. Write a Hadoop job that uses our mapper and `/bin/cat` for a reducer to partition and sort. Print out the top 10 words in each class (where 'top' == highest conditional probability).[`HINT:` _this should remind you a lot of what we did in Question 6._]\n",
    "\n",
    "\n",
    "* __b) short response:__ What do you notice about the 'top words' we printed in `a`? How would increasing the smoothing parameter 'k' affect the probabilities for the top words that you identified for 'a'. How would they affect the probabilities of words that occur much more in one class than another? In summary, how does the smoothing parameter 'k' affect the bias and the variance of our model. [`NOTE:` _you do not need to code anything for this task, but if you are struggling with it you could try changing 'k' and see what happens to the test set. We don't recommend doing this exploration with the Enron data because it will be harder to see the impact with such a big vocabulary_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10 Student Answers:\n",
    "> __a)__ We have partitioned using the class and sorted using the conditional probability value of words given the class. As the class happen to be the $3^{rd}$ field and value of the conditional probability is in the $4^{th}$ field. We configured the hadoop's keycomparator and keypartitioner to do the custom partitioning and sorting. (**NOTE**: while our output of this hadoop streaming job matches with the **expected results** provided, the output is not sorted from highest to lowest. This is because hadoop doesn't sort the numbers in scientific notation correctly. While we could get that working by filtering the output using a threshold value we choose not to do that as ours matches the expected output.)\n",
    "\n",
    "> __b)__ Laplace smoothing moves probabilities towards uninformed mean. Suppose we have a multinomial variable with sample counts $c_1, c_2,...c_n$ where $d$ is the vocabulary size. A Laplacian smoothened version of estimated probabilities has the form: $(c_i+k)/(N+d.k)$ , where $k$ is a positive integer. Typically, $k$ is set to 1 to smooth the estimator. If $k$ is $0$ then we have unsmoothed estimator. When large values of k are used then the influence of observed counts will be lower because estimated probabilities for the same number of observations will be lower. So, the higher probabilites will reduce to move towards the mean and similarly, lower probabilities will increase to move towards the mean. A direct consequence is that the variance of the model will tend to be lower and possibly with a larger bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/NBmodel.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r {HDFS_DIR}/NBmodel.txt\n",
    "!hdfs dfs -copyFromLocal NaiveBayes/Smoothed/NBmodel.txt {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 items\n",
      "-rw-r--r--   1 root hadoop     254300 2022-01-31 00:25 /user/root/HW2/NBmodel.txt\n",
      "drwxr-xr-x   - root hadoop          0 2022-01-31 00:18 /user/root/HW2/chinese-output\n",
      "drwxr-xr-x   - root hadoop          0 2022-01-31 00:19 /user/root/HW2/chinese-output-smooth\n",
      "-rw-r--r--   1 root hadoop        119 2022-01-23 00:41 /user/root/HW2/chineseTest.txt\n",
      "-rw-r--r--   1 root hadoop        107 2022-01-23 00:41 /user/root/HW2/chineseTrain.txt\n",
      "drwxr-xr-x   - root hadoop          0 2022-01-30 00:39 /user/root/HW2/custom-partition\n",
      "drwxr-xr-x   - root hadoop          0 2022-01-30 23:45 /user/root/HW2/eda-output\n",
      "drwxr-xr-x   - root hadoop          0 2022-01-30 23:35 /user/root/HW2/eda-sort-output\n",
      "drwxr-xr-x   - root hadoop          0 2022-01-31 00:21 /user/root/HW2/enron-model\n",
      "-rw-r--r--   1 root hadoop     204559 2022-01-21 02:30 /user/root/HW2/enron.txt\n",
      "-rw-r--r--   1 root hadoop      41493 2022-01-28 02:47 /user/root/HW2/enron_test.txt\n",
      "-rw-r--r--   1 root hadoop     163066 2022-01-28 02:47 /user/root/HW2/enron_train.txt\n",
      "drwxr-xr-x   - root hadoop          0 2022-01-28 23:49 /user/root/HW2/eval-unsmoothed-model\n",
      "drwxr-xr-x   - root hadoop          0 2022-01-31 00:24 /user/root/HW2/evaluate-smoothed-model\n",
      "drwxr-xr-x   - root hadoop          0 2022-01-31 00:23 /user/root/HW2/evaluate-unsmoothed-model\n",
      "drwxr-xr-x   - root hadoop          0 2022-01-31 00:22 /user/root/HW2/smooth-model\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/custom-partition\n",
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.2.2.jar] /tmp/streamjob8495792187527972986.jar tmpDir=null\n",
      "2022-01-31 00:25:31,725 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.128.0.2:8032\n",
      "2022-01-31 00:25:31,978 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.128.0.2:10200\n",
      "2022-01-31 00:25:32,447 INFO client.RMProxy: Connecting to ResourceManager at w261-m/10.128.0.2:8032\n",
      "2022-01-31 00:25:32,447 INFO client.AHSProxy: Connecting to Application History server at w261-m/10.128.0.2:10200\n",
      "2022-01-31 00:25:32,619 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1643556265243_0013\n",
      "2022-01-31 00:25:32,918 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2022-01-31 00:25:33,378 INFO mapreduce.JobSubmitter: number of splits:9\n",
      "2022-01-31 00:25:33,518 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1643556265243_0013\n",
      "2022-01-31 00:25:33,520 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-01-31 00:25:33,734 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-01-31 00:25:33,735 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-01-31 00:25:33,792 INFO impl.YarnClientImpl: Submitted application application_1643556265243_0013\n",
      "2022-01-31 00:25:33,834 INFO mapreduce.Job: The url to track the job: http://w261-m:8088/proxy/application_1643556265243_0013/\n",
      "2022-01-31 00:25:33,835 INFO mapreduce.Job: Running job: job_1643556265243_0013\n",
      "2022-01-31 00:25:40,921 INFO mapreduce.Job: Job job_1643556265243_0013 running in uber mode : false\n",
      "2022-01-31 00:25:40,923 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-01-31 00:25:48,036 INFO mapreduce.Job:  map 22% reduce 0%\n",
      "2022-01-31 00:25:49,042 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2022-01-31 00:25:55,108 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2022-01-31 00:26:00,165 INFO mapreduce.Job:  map 78% reduce 0%\n",
      "2022-01-31 00:26:02,181 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2022-01-31 00:26:07,210 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "2022-01-31 00:26:08,262 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-01-31 00:26:10,279 INFO mapreduce.Job: Job job_1643556265243_0013 completed successfully\n",
      "2022-01-31 00:26:10,387 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=777365\n",
      "\t\tFILE: Number of bytes written=4293558\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=287887\n",
      "\t\tHDFS: Number of bytes written=759133\n",
      "\t\tHDFS: Number of read operations=37\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=9\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=9\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=151235520\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=22584336\n",
      "\t\tTotal time spent by all map tasks (ms)=47920\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7156\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=47920\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=7156\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=151235520\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=22584336\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4560\n",
      "\t\tMap output records=9110\n",
      "\t\tMap output bytes=759133\n",
      "\t\tMap output materialized bytes=777461\n",
      "\t\tInput split bytes=819\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=9110\n",
      "\t\tReduce shuffle bytes=777461\n",
      "\t\tReduce input records=9110\n",
      "\t\tReduce output records=9110\n",
      "\t\tSpilled Records=18220\n",
      "\t\tShuffled Maps =18\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=18\n",
      "\t\tGC time elapsed (ms)=1525\n",
      "\t\tCPU time spent (ms)=10950\n",
      "\t\tPhysical memory (bytes) snapshot=5273772032\n",
      "\t\tVirtual memory (bytes) snapshot=48701673472\n",
      "\t\tTotal committed heap usage (bytes)=5303697408\n",
      "\t\tPeak Map Physical memory (bytes)=550830080\n",
      "\t\tPeak Map Virtual memory (bytes)=4427096064\n",
      "\t\tPeak Reduce Physical memory (bytes)=283443200\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4434681856\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=287068\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=759133\n",
      "2022-01-31 00:26:10,388 INFO streaming.StreamJob: Output directory: /user/root/HW2//custom-partition\n"
     ]
    }
   ],
   "source": [
    "# part a - write your Hadoop job here (sort smoothed model on P(word|class))\n",
    "\n",
    "# clear output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/custom-partition\n",
    "\n",
    "# hadoop job\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=4 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k4,4nr -k1,1\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k3,3\" \\\n",
    "  -files NaiveBayes/model_sort_mapper.py \\\n",
    "  -mapper model_sort_mapper.py \\\n",
    "  -reducer /bin/cat \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -input {HDFS_DIR}/NBmodel.txt \\\n",
    "  -output {HDFS_DIR}/custom-partition \\\n",
    "  -cmdenv PATH={PATH} \\\n",
    "  -numReduceTasks 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abn\t0,1,6.19348445435402e-05,0.00011092008208086075\tham\t6.19348445435402e-05\t\n",
      "absenteeism\t0,1,6.19348445435402e-05,0.00011092008208086075\tham\t6.19348445435402e-05\t\n",
      "absolute\t0,1,6.19348445435402e-05,0.00011092008208086075\tham\t6.19348445435402e-05\t\n",
      "absolutely\t0,1,6.19348445435402e-05,0.00011092008208086075\tham\t6.19348445435402e-05\t\n",
      "absorb\t0,1,6.19348445435402e-05,0.00011092008208086075\tham\t6.19348445435402e-05\t\n",
      "abuse\t0,1,6.19348445435402e-05,0.00011092008208086075\tham\t6.19348445435402e-05\t\n",
      "abused\t0,1,6.19348445435402e-05,0.00011092008208086075\tham\t6.19348445435402e-05\t\n",
      "acce\t0,1,6.19348445435402e-05,0.00011092008208086075\tham\t6.19348445435402e-05\t\n",
      "accelerate\t0,1,6.19348445435402e-05,0.00011092008208086075\tham\t6.19348445435402e-05\t\n",
      "accelerated\t0,1,6.19348445435402e-05,0.00011092008208086075\tham\t6.19348445435402e-05\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "# part b - print top words in each class\n",
    "!hdfs dfs -cat {HDFS_DIR}/custom-partition/part-00000 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab\t4,0,0.00030967422271770096,5.546004104043037e-05\tspam\t5.546004104043037e-05\t\n",
      "absent\t1,0,0.0001238696890870804,5.546004104043037e-05\tspam\t5.546004104043037e-05\t\n",
      "accepts\t1,0,0.0001238696890870804,5.546004104043037e-05\tspam\t5.546004104043037e-05\t\n",
      "accomodate\t4,0,0.00030967422271770096,5.546004104043037e-05\tspam\t5.546004104043037e-05\t\n",
      "accomodates\t1,0,0.0001238696890870804,5.546004104043037e-05\tspam\t5.546004104043037e-05\t\n",
      "accompanied\t1,0,0.0001238696890870804,5.546004104043037e-05\tspam\t5.546004104043037e-05\t\n",
      "accounting\t4,0,0.00030967422271770096,5.546004104043037e-05\tspam\t5.546004104043037e-05\t\n",
      "accurate\t1,0,0.0001238696890870804,5.546004104043037e-05\tspam\t5.546004104043037e-05\t\n",
      "achieve\t1,0,0.0001238696890870804,5.546004104043037e-05\tspam\t5.546004104043037e-05\t\n",
      "achieved\t1,0,0.0001238696890870804,5.546004104043037e-05\tspam\t5.546004104043037e-05\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat {HDFS_DIR}/custom-partition/part-00001 | head"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected results:\n",
    "============== PART-00000===============\n",
    "abn\t0,1,6.19348445435402e-05,0.00011092008208086075\tham\t6.19348445435402e-05\t\n",
    "absenteeism\t0,1,6.19348445435402e-05,0.00011092008208086075\tham\t6.19348445435402e-05\t\n",
    "absolute\t0,1,6.19348445435402e-05,0.00011092008208086075\tham\t6.19348445435402e-05\t\n",
    "absolutely\t0,1,6.19348445435402e-05,0.00011092008208086075\tham\t6.19348445435402e-05\t\n",
    "absorb\t0,1,6.19348445435402e-05,0.00011092008208086075\tham\t6.19348445435402e-05\t\n",
    "abuse\t0,1,6.19348445435402e-05,0.00011092008208086075\tham\t6.19348445435402e-05\t\n",
    "abused\t0,1,6.19348445435402e-05,0.00011092008208086075\tham\t6.19348445435402e-05\t\n",
    "acce\t0,1,6.19348445435402e-05,0.00011092008208086075\tham\t6.19348445435402e-05\t\n",
    "accelerate\t0,1,6.19348445435402e-05,0.00011092008208086075\tham\t6.19348445435402e-05\t\n",
    "accelerated\t0,1,6.19348445435402e-05,0.00011092008208086075\tham\t6.19348445435402e-05\t\n",
    "cat: Unable to write to output stream.\n",
    "============== PART-00001===============\n",
    "ab\t4,0,0.00030967422271770096,5.546004104043037e-05\tspam\t5.546004104043037e-05\t\n",
    "absent\t1,0,0.0001238696890870804,5.546004104043037e-05\tspam\t5.546004104043037e-05\t\n",
    "accepts\t1,0,0.0001238696890870804,5.546004104043037e-05\tspam\t5.546004104043037e-05\t\n",
    "accomodate\t4,0,0.00030967422271770096,5.546004104043037e-05\tspam\t5.546004104043037e-05\t\n",
    "accomodates\t1,0,0.0001238696890870804,5.546004104043037e-05\tspam\t5.546004104043037e-05\t\n",
    "accompanied\t1,0,0.0001238696890870804,5.546004104043037e-05\tspam\t5.546004104043037e-05\t\n",
    "accounting\t4,0,0.00030967422271770096,5.546004104043037e-05\tspam\t5.546004104043037e-05\t\n",
    "accurate\t1,0,0.0001238696890870804,5.546004104043037e-05\tspam\t5.546004104043037e-05\t\n",
    "achieve\t1,0,0.0001238696890870804,5.546004104043037e-05\tspam\t5.546004104043037e-05\t\n",
    "achieved\t1,0,0.0001238696890870804,5.546004104043037e-05\tspam\t5.546004104043037e-05\t\n",
    "cat: Unable to write to output stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations, you have completed HW2! Please refer to the readme for submission instructions.\n",
    "\n",
    "If you would like to provide feedback regarding this homework, please use the survey at: https://docs.google.com/forms/d/e/1FAIpQLSce9feiQeSkdP43A0ZYui1tMGIBfLfzb0rmgToQeZD9bXXX8Q/viewform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
