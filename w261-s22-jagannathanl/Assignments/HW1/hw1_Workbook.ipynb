{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1 - Intro to the Map Reduce Paradigm  \n",
    "__ `MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Spring 2018`__\n",
    "\n",
    "Welcome to Machine Learning at Scale! This first homework assignment introduces one of the core strategies in distributed processing: divide and conquer. We'll use the simplest of tasks, word counting, to illustrate the difference between a scalable and non-scalable algorithm. You will be working with the text of _Alice in Wonderland_ to put these ideas into practice using Python and Bash scripting. By the end of this week you should be able to:\n",
    "* ... __describe__ the Bias-Variance tradeoff as it applies to Machine Learning.\n",
    "* ... __explain__ why we consider word counting to be an \"Embarrassingly Parallel\" task.\n",
    "* ... __estimate__ the runtime of embarrassingly parallel tasks using \"back of the envelope\" calculations.\n",
    "* ... __implement__ a Map Reduce algorithm using the Command Line.\n",
    "* ... __set-up__ a Docker container and know why we use them for this course.\n",
    "\n",
    "You will also  become familiar (if you aren't already) with `defaultdict`, `re` and `time` in Python, linux piping and sorting, and Jupyter magic commands `%%writefile` and `%%timeit`. __Please refer to the `README` for detailed submission instructions__.\n",
    "\n",
    "__IMPORTANT:__ If you're not familiar with linux, you should read the following tutorial reagrding **piping** and **redirecting**: https://ryanstutorials.net/linuxtutorial/piping.php You will need to understand the differences to answer some of the later questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Set-Up\n",
    "Before starting your homework run the following cells to confirm your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm you are running Python 3\n",
    "import sys\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a folder for any data you download locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "# NOTE: the contents of this directory will be ignored by git."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Introductions\n",
    "\n",
    "`The Caterpillar and Alice looked at each other for some time in silence: at last the Caterpillar took the hookah out of its mouth, and addressed her in a languid, sleepy voice. \"Who are you?\" said the Caterpillar.`   \n",
    "<div style=\"text-align: right\"> -- Lewis Carroll, _Alice's Adventures in Wonderland_, Chapter 4 </div>\n",
    "\n",
    "\n",
    "__Tell us about yourself! Briefly describe where you live, how far along you are in MIDS, what other classes you are taking and what you want to get out of w261.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Verdana; color: green;\"> I am Jagan, Lakshmipathy. I work for Bank of America and I live in Charlotte, NC. I joined the bank as a Java Developer and later climbed up as a technology lead in the Java Web Development space. For the last two years I joined NLP group within the bank and switched into python development. I am primarily involved in document classification. I am very interested in ML/NLP areas and hence I joined the MIDS program. I completed W201, W203, W205, and W207 so far. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Bias - Variance\n",
    "__In 1-2 sentences explain the bias-variance trade off. Describe what it means to \"decompose\" sources of error. How is this used in machine learning?__ Please use mathematical equation(s) to support your explanation. (Use `$` signs to take advantage of $\\LaTeX$ formatting, eg. `$f(x)$` will look like: $f(x)$). Please also cite any sources that informed your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Verdana; color: green;\"> Let Y be the variable we are trying to predict from the provided data X. If we assume there is a relationship between the two such that $Y=f(X) + \\epsilon$ Where $\\epsilon$ is the error term and it’s normally distributed with a mean of 0. We will make a model $f\\hat(X)$ of $f(X)$ using linear regression or any other modeling technique. The error function $Err(x)$ can be expressed as follows:$$Err(x) = (E[f\\hat(x) -f(x)])^2 + E[(f\\hat(x) -f(x))^2] + \\delta^2$$Where the first term in the RHS is the $Bias^2$ and second term is the variance and the third term is the irreducible error which measures the amount of noise. So, in machine learning if our model is too simple and has very few parameters then it may have high bias and low variance. On the other hand if our model has large number of parameters then it’s going to have high variance and low bias. So the tradeoff is the the right balance between between bias and variance such that it minimizes the total error.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Tokenizing\n",
    "A number of our assignments this term will involve extracting information from text. A common preprocessing step when working with raw files is to 'tokenize' (i.e. extract words from) the text. Within the field of Natural Language Processing a lot of thought goes into what specific tokenizing makes most sense for a given task. For example, you might choose to remove punctuation or to consider punctuation symbols  'tokens' in their own right. __In this question you'll use the Python `re` module to create a tokenizer to use when you perform WordCount on the _Alice In Wonderland_ text.__\n",
    "\n",
    "### Q3 Tasks:\n",
    "\n",
    "* __a) short response:__ In the Naive Bayes algorithm (which we'll implement next week), we'll estimate the _likelihood_ of a word by counting the number of times it appears and dividing by the size of the vocabulary (total number of unique words). Using the text: *\"Alice had an adventure that took alice to wonderland\"*, give a concrete example of how two different tokenizers could cause us to get two different results on this calculation. [`HINT`: _you should not need to read up on Naive Bayes to answer this question_]  \n",
    "  \n",
    "\n",
    "* __b) short response:__ When tokenizing in this assignment we'll remove punctuation and discard numerical digits by making everything lowercase and then capturing only consecutive letters a to z. Suppose __`tokenize(x)`__ is a Python function that performs the desired tokenization. What would __`tokenize(\"By-the-bye, what became of Alice's 12 hats?!\")`__ output? Type the answer in the space provided below.\n",
    "\n",
    "\n",
    "* __c) code:__  Fill in the regular expression pattern in the cell labeled `part c` so that the subsequent call to `re.findall(RE_PATTERN, ...)` returns the tokenization described above. [`HINT`: _we've taken care of the lowercase part for you. If regex is new to you, check out the [`re`  documentation](https://docs.python.org/3/library/re.html) or [this PyMOTW tutorial](https://pymotw.com/2/re/)._]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Student Answers:\n",
    "> __a)__ <span style=\"font-family:Verdana; color: green;\"> In this example, if we choose to tokenize by converting the words by ignoring the case and by not ignoring the case we will get two different probabilities for the word \"alice\".  For the above example the probability of the word \"alice\" when we ignore the case will be 2/8 as there are 8 unique words and the word \"alice\" appeared twice. If we didn't ignore the case then the probability of the word \"alice\" will come out to be 1/9 as there are 9 unique words including the word \"Alice\" and the word \"alice\" appeared only once.</span>\n",
    "\n",
    "> __b)__ <span style=\"font-family:Verdana; color: green;\"> This will be the output of the tokenize() function above: ['by', 'the', 'bye', 'what', 'became', 'of', 'alice', 's', 'hats']</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Part C - Fill in the regular expression\n",
    "RE_PATTERN = re.compile(\"[a-z]+\")          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['by', 'the', 'bye', 'what', 'became', 'of', 'alice', 's', 'hats']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize - DO NOT MODIFY THIS CELL, just run it as is to check your pattern\n",
    "words = re.findall(RE_PATTERN, \"By-the-bye, what became of Alice's 12 hats?!\".lower())\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "` \"Please would you tell me\", said Alice, a little timidly, for she was not quite sure whether it was good manners for her to speak first, \"why your cat grins like that?\"`  \n",
    "<div style=\"text-align: right\">  -- Lewis Carroll, _Alice's Adventures in Wonderland_, Chapter 4  </div>\n",
    "\n",
    "For the main part of this assignment we'll be working with the free plain text version of _Alice's Adventures in Wonderland_ available from Project Gutenberg. __Use the first two cells below to download this text and preview the first few lines.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n"
     ]
    }
   ],
   "source": [
    "# Download Full text \n",
    "# NOTE: feel free to replace 'curl' with 'wget' or equivalent command of your choice.\n",
    "!curl \"http://www.gutenberg.org/files/11/11-0.txt\" -o data/alice.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg eBook of Alice’s Adventures in Wonderland, by Lewis Carroll\r",
      "\r\n",
      "\r",
      "\r\n",
      "This eBook is for the use of anyone anywhere in the United States and\r",
      "\r\n",
      "most other parts of the world at no cost and with almost no restrictions\r",
      "\r\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\r",
      "\r\n",
      "of the Project Gutenberg License included with this eBook or online at\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Take a peak at the first few lines\n",
    "!head -n 6 data/alice.txt\n",
    "# NOTE: If you are working in JupyterLab on Docker you may not see the output \n",
    "# below due to an encoding issue... in that case, use a terminal on Docker to \n",
    "# execute this head command and confirm that the file has downloaded properly, \n",
    "# this encoding issue should not affect your work on subsequent HW items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd also like you to develop a habit of creating small files with simulated data for use in developing, debugging and testing your code. The jupyter magic command `%%writefile` is a convenient way to do this. __Run the following cells to create a test data file for use in our word counting task.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/alice_test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/alice_test.txt\n",
    "This is a small test file. This file is for a test.\n",
    "This small test file has two small lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice_test.txt\r\n"
     ]
    }
   ],
   "source": [
    "# confirm the file was created in the data directory using a grep command:\n",
    "!ls data | grep test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Word Count in Python\n",
    "\n",
    "Over the course of this term you will also become very familiar writing Python programs that read from standard input and using Linux piping commands to run these programs and save their output to file. __In this question you will write a short python script to perform the Word Count task and then run your script on the _Alice in Wonderland_ text__. You can think of this like a 'baseline implementation' that we'll later compare to the parallelized version of the same task.\n",
    "\n",
    "### Q4 Tasks:\n",
    "\n",
    "* __a) code:__ Complete the Python script in the file __`wordCount.py`__. Read the docstrings carefully to be sure you understand the expected behavior of this function. Please do not code outside of the marked location.\n",
    "\n",
    "\n",
    "* __b) testing:__ Run the cell marked `part b` to call your script on the test file we created above. Confirm that your script returns the correct counts for each word by visually comparing the output to the test file. \n",
    "\n",
    "\n",
    "* __c) results:__ When you are confident in your implementation, run the cell marked `part c` to count the number of occurrences of each word in _Alice's Adventures in Wonderland_. In the same cell we'll pipe the output to file. Then use the provided `grep` commands to check your answers.\n",
    "\n",
    "\n",
    "* __d) short response:__ Suppose you decide that you'd really like  a word and its plural (e.g. 'hatter' and 'hatters' or 'person' and 'people') to be counted as the same word. After we have run the wordcount would it be more efficient to post-process your output file or discard your output file and start the analysis over with a new tokenizer? Explain your reasoning briefly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 Student Answers:\n",
    "> __a-c)__ _Complete the coding portions of this question before answering 'e'._ <span style=\"font-family:Verdana; color: green;\"> Coding portions were completed and we are proceeding to the next section.</span>\n",
    "\n",
    "> __d)__ <span style=\"font-family:Verdana; color: green;\"> It will be efficient to postprocess after running the wordcount to recount each word and its plural as one word. The following are some reasons for this approach: (1) We will be able to keep the original count unchanged and we can generate the post processed count and keep it separate from the original count. We will have the copies of both counts for future reference. (2) Rules for post processing may change in the future (e.g. more words may be added, removed, or perhaps plural word may appear different as in the case of 'person' or 'people' etc.). We can rerun the postprocessing with out having to generate the orignal count. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# part a - DO YOUR WORK IN wordCount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\r\n",
      "\"\"\"\r\n",
      "This script reads lines from STDIN and returns a list of\r\n",
      "all words an the count of how many times they occurred.\r\n",
      "\r\n",
      "INPUT:\r\n",
      "    a text file\r\n",
      "OUTPUT FORMAT:\r\n",
      "    word \\t count\r\n",
      "USAGE:\r\n",
      "    python wordCount.py < yourTextFile.txt\r\n",
      "\r\n",
      "Instructions:\r\n",
      "    Fill in the missing code below so that the script\r\n",
      "    prints tab separated word counts to Standard Output.\r\n",
      "    NOTE: we have performed the tokenizing for you, please\r\n",
      "    don't modify the provided code or you may fail unittests.\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "# imports\r\n",
      "import sys\r\n",
      "import re\r\n",
      "from collections import defaultdict\r\n",
      "\r\n",
      "counts = defaultdict(int)\r\n",
      "\r\n",
      "# stream over lines from Standard Input\r\n",
      "for line in sys.stdin:\r\n",
      "\r\n",
      "    # tokenize\r\n",
      "    line = line.strip()\r\n",
      "    words = re.findall(r'[a-z]+', line.lower())\r\n",
      "\r\n",
      "############ YOUR CODE HERE #########\r\n",
      "    for w in words:\r\n",
      "        counts[w] += 1\r\n",
      "for k, v in counts.items():\r\n",
      "    print(k+'\\t'+str(v))\r\n",
      "############ (END) YOUR CODE #########\r\n"
     ]
    }
   ],
   "source": [
    "!cat wordCount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\t3\r\n",
      "is\t2\r\n",
      "a\t2\r\n",
      "small\t3\r\n",
      "test\t3\r\n",
      "file\t3\r\n",
      "for\t1\r\n",
      "has\t1\r\n",
      "two\t1\r\n",
      "lines\t1\r\n"
     ]
    }
   ],
   "source": [
    "# part b - DO NOT MODIFY THIS CELL, just run it as is to test your script\n",
    "!python wordCount.py < data/alice_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - DO NOT MODIFY THIS CELL, just run it as is to perform the word count.\n",
    "!python wordCount.py < data/alice.txt > data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the first 10 words & their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\t1839\r\n",
      "project\t88\r\n",
      "gutenberg\t98\r\n",
      "ebook\t13\r\n",
      "of\t638\r\n",
      "alice\t403\r\n",
      "s\t222\r\n",
      "adventures\t11\r\n",
      "in\t435\r\n",
      "wonderland\t7\r\n"
     ]
    }
   ],
   "source": [
    "!head data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check your results:__ How many times does the word \"alice\" appear in the book? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice\t403\r\n"
     ]
    }
   ],
   "source": [
    "# EXPECTED OUTPUT: 404\n",
    "!grep alice data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check your results:__ How many times does the word \"hatter\" appear in the book? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hatter\t56\r\n",
      "hatters\t1\r\n"
     ]
    }
   ],
   "source": [
    "# EXPECTED OUTPUT: 56\n",
    "!grep hatter data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check your results:__ How many times does the word \"queen\" appear in the book? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen\t76\r\n",
      "queens\t1\r\n"
     ]
    }
   ],
   "source": [
    "# EXPECTED OUTPUT: 76\n",
    "!grep queen data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Unix Sorting Practice\n",
    "Another common task in this course's assignments will be to make strategic use of sorting.     \n",
    "\n",
    "### Q5 Tasks:\n",
    "* __a) short response:__ What is the Big O complexity of the fastest comparison based sorting algorithms? [*HINT: If you need a Big O notation refresher, here's_ a [blog post](https://rob-bell.net/2009/06/a-beginners-guide-to-big-o-notation/), a [cheatsheet](http://bigocheatsheet.com), and a [thorough explanation](http://pages.cs.wisc.edu/~vernon/cs367/notes/3.COMPLEXITY.html).*]\n",
    "\n",
    "* __b) short response:__ What is the default sorting algorithm in MapReduce? What is the Big O complexity of this algorithm? Why do you think this algorithm was chosen? [*HINT: Julius Ceasar! (week 1 slides)*]\n",
    "\n",
    "* __c) code:__ write a unix command to check how many records are in your word count file.\n",
    "\n",
    "* __d) code:__ Write a unix command to sort your word count file alphabetically. Save (i.e. [redirect](https://superuser.com/questions/277324/pipes-vs-redirects)) the results to `data/alice_counts_A-Z.txt`. [*HINT: if Unix sort commands are new to you, start with [this biowize blogpost](https://biowize.wordpress.com/2015/03/13/unix-sort-sorting-with-both-numeric-and-non-numeric-keys/) or [this unixschool tutorial](http://www.theunixschool.com/2012/08/linux-sort-command-examples.html)*]\n",
    "\n",
    "* __e) code:__ Write a unix command to sort your word count file from highest to lowest count. Save (i.e. [redirect](https://superuser.com/questions/277324/pipes-vs-redirects)) your results to `data/alice_counts_sorted.txt`; then run the provided cell to print the top ten words. Compare your output to the expected output we provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 Student Answers:\n",
    "> __a)__ <span style=\"font-family:Verdana; color: green;\"> In computer science, Big $O$ notation is used to compare the worst case time complexity of algorithms. The fastest performing sorting algorithm performs at $O(n log (n))$. </span>\n",
    "\n",
    "> __b)__ <span style=\"font-family:Verdana; color: green;\"> MapReduce has inherently 3 phases in it: (i) Map phase, (ii) Shuffle phase, and (iii) Reduce phase. MapReduce exploits divide and conquer strategy to breakdown the problem into smaller subproblems in the mapping stage and then in the shuffle stage depending on the algorithm we organize the data before we feed it to the final Reduce phase. Merge sort being a divide-and-conquer algorithm and merges two or more sorted sub-sequences into one merged sequence incrementally, it fits perfectly with the MapReduce framework. For e.g. in our above word count problem, we split the document into smaller documents at the mapping stage and the mapper produce the word count for the sub-document as name value pairs. In the shuffle stage we further organize the name value pairs to feed into the final reduce stage to reduce the final count of each word. Merge sort performs at best, on average and worst case at O(n log (n)). The reduce stage of the MapReduce will receive the sorted sequences from each partition in the Map stage. Finally, these individual sorted sequences are merged into one final sorted sequence.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    3006\r\n"
     ]
    }
   ],
   "source": [
    "# part c - write a unix command to check how many records are in your word count file\n",
    "!wc -l < data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part d - unix command to sort your word counts alphabetically \n",
    "!sort data/alice_counts.txt > data/alice_counts_A-Z.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t695\r\n",
      "abide\t2\r\n",
      "able\t1\r\n",
      "about\t102\r\n",
      "above\t3\r\n",
      "absence\t1\r\n",
      "absurd\t2\r\n",
      "accept\t1\r\n",
      "acceptance\t1\r\n",
      "accepted\t2\r\n"
     ]
    }
   ],
   "source": [
    "# part d - DO NOT MODIFY THIS CELL, run it as is to confirm your sort worked\n",
    "!head data/alice_counts_A-Z.txt   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part e - unix command to sort your word counts from highest to lowest count\n",
    "!sort -k 2nr  data/alice_counts.txt > data/alice_counts_sorted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\t1839\r\n",
      "and\t942\r\n",
      "to\t811\r\n",
      "a\t695\r\n",
      "of\t638\r\n",
      "it\t610\r\n",
      "she\t553\r\n",
      "i\t546\r\n",
      "you\t486\r\n",
      "said\t462\r\n"
     ]
    }
   ],
   "source": [
    "# part e - DO NOT MODIFY THIS CELL, run it as is to confirm your sort worked\n",
    "!head data/alice_counts_sorted.txt  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<th>expected output for (d):</th>\n",
    "<th>expected output for (e):</th>\n",
    "<tr><td><pre>\n",
    "a\t695\n",
    "abide\t2\n",
    "able\t1\n",
    "about\t102\n",
    "above\t3\n",
    "absence\t1\n",
    "absurd\t2\n",
    "accept\t1\n",
    "acceptance\t1\n",
    "accepted\t2\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "the\t1837\n",
    "and\t946\n",
    "to\t811\n",
    "a\t695\n",
    "of\t637\n",
    "it\t610\n",
    "she\t553\n",
    "i\t546\n",
    "you\t487\n",
    "said\t462\n",
    "</pre></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: Parallel Word Count (part 1)\n",
    "What would happen if we tried to run our script on a much larger dataset? For one thing, it would take longer to run. However there is a second concern. The Python object that aggregates our counts (`defaultdict`) exists in memory on the machine running this notebook. If the vocabulary is too large for the memory space available we would crash the notebook. The solution? Divide and Conquer! Instead of running the script on the whole dataset at once, we could split our text up in to smaller 'chunks' and process them independently of each other. __In this question you'll use a bash script to \"parallelize\" your Word Count.__\n",
    "\n",
    "\n",
    "### Q6 Tasks:\n",
    "* __a) read provided code:__ The bash script `pWordCount_v1.sh` takes an input file, splits it into a specified number of 'chunks', and then applies an executable of your choice to each chunk. Read through this code and make sure you understand each line before you proceed. [`HINT:` _For now, ignore the 'student code' section -- you'll use that in part c._]\n",
    "\n",
    "\n",
    "* __b) short response:__ Below we've provided the command to use this script to apply your analysis (`wordCount.py`) to the _Alice_ text in 4 parallel processes. We'll redirect the results into a file called `alice_pCounts.txt.` Run this analysis and compare the count for the word 'alice' to your answer from Question 4. Explain what went wrong and describe what we have to add to `pWordCount_v1.sh` to fix the problem.\n",
    "\n",
    "\n",
    "* __c) code:__ We've provided a python script, `aggregateCounts_v1.py`, which reads word counts from standard input and combines any duplicates it encounters. Read through this script to be sure you understand how it is written. Then follow the instructions in `pWordCount_v1.sh` to make a one-line modification so that it accepts `aggregateCounts_v1.py` as a 4th argument and uses this script to combine the chunk-ed word counts. Run the cell below to confirm that you now get the correct results for your 'alice' count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 Student Answers:\n",
    "> __b)__ <span style=\"font-family:Verdana; color: green;\"> As pWordCount_v1.sh, partitions the file alice.txt in to 5 chunks, as the first argument to the script pWordCount_v1 refers to the desired process count (i.e. 4 in this case). Each chunk is sent to a separate process namely the python script wordcount.py (the second argument). Python script computes the word count for that chunk and dumps it to an output file. These output from these processes are basically concatenated as we did not pass the aggregator as an argument to wordcount.py. So each word will appear as many number of times as it is found in those partitions. For e.g. the word \"alice\" appeared 4 times in the alice_pCounts.txt as opposed to one entry for word \"alice\" in alice_counts.txt. This is because the word \"alice\" appeared in 4 chunks and the count in alice_counts.txt add up to the counts in the alice_pCounts.txt</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part b - make sure your scripts are executable (RUN THIS CELL AS IS)\n",
    "!chmod a+x pWordCount_v1.sh\n",
    "!chmod a+x wordCount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part b - parallel word count on Alice text (RUN THIS CELL AS IS)\n",
    "!./pWordCount_v1.sh 4 'data/alice.txt' 'wordCount.py' > 'data/alice_pCounts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice\t113\r\n",
      "alice\t126\r\n",
      "alice\t122\r\n",
      "alice\t42\r\n"
     ]
    }
   ],
   "source": [
    "# part b - check alice count (RUN THIS CELL AS IS)\n",
    "!grep alice data/alice_pCounts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - make sure the aggregateCounts script is executable (RUN THIS CELL AS IS)\n",
    "!chmod a+x aggregateCounts_v1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "# pWordCount.sh\r\n",
      "# Author: James G. Shanahan\r\n",
      "# Usage: pWordCount.sh m testFile.txt mapper.py [reducer.py]\r\n",
      "# Input:\r\n",
      "#   m = number of processes (maps), e.g., 4\r\n",
      "#   inputFile = a text input file\r\n",
      "#   mapper = an executable that reads from STDIN and prints to STDOUT\r\n",
      "#   reducer = (optional) an executable that reads from STDIN and prints \r\n",
      "#             to STDOUT, if no reducer is provided, the framework will\r\n",
      "#             simply stream the mapper output.\r\n",
      "#\r\n",
      "# Instructions:\r\n",
      "#    For Q6a - Read this script and its comments closely. Ignore the\r\n",
      "#              part marked \"Otherwise\" in STEP 3, you'll use that later.\r\n",
      "#    For Q6c - Add a single line of code under '#Q6c' in STEP 3 so that\r\n",
      "#              the script pipes the output of each chunk's word countfiles\r\n",
      "#              into the second executable script provided as an argument,\r\n",
      "#              Note that we saved the script name (which was the 4th arg)\r\n",
      "#              to the variable $reducer. It can be executed by piping the\r\n",
      "#              counts to './$reducer' -- don't forget to redirect the output\r\n",
      "#              of this second script into $data.output\r\n",
      "\r\n",
      "# --------------------------------------------------------------------\r\n",
      "\r\n",
      "usage()\r\n",
      "{\r\n",
      "    echo ERROR: No arguments supplied\r\n",
      "    echo\r\n",
      "    echo To run use\r\n",
      "    echo \"pWordCount.sh m inputFile mapper.py [reducer.py]\"\r\n",
      "    echo Input:\r\n",
      "    echo \"number of processes/maps, EG, 4\"\r\n",
      "    echo \"mapper.py = an executable script to apply to each chunk in parallel\"\r\n",
      "    echo \"reducer.py = an executable script after the parallel processes are complete.\"\r\n",
      "    echo NOTE: if no reducer is supplied, we will simply combine the output files.\r\n",
      "}\r\n",
      "\r\n",
      "\r\n",
      "# print the usage message if this script is called without required args\r\n",
      "if [ $# -lt 3 ]\r\n",
      "  then\r\n",
      "    usage\r\n",
      "    exit 1\r\n",
      "fi\r\n",
      "\r\n",
      "# collect the arguments\r\n",
      "m=$1\r\n",
      "data=$2\r\n",
      "mapper=$3\r\n",
      "\r\n",
      "\r\n",
      "################# STEP 1: Split up the data #################\r\n",
      "# 'wc' determines the number of lines in the data\r\n",
      "# 'perl -pe' regex strips the piped wc output to a number\r\n",
      "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\r\n",
      "\r\n",
      "# determine the lines per chunk for the desired number of processes\r\n",
      "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\r\n",
      "\r\n",
      "# split the original file into chunks by line\r\n",
      "split -l $linesinchunk $data $data.chunk.\r\n",
      "\r\n",
      "\r\n",
      "############## STEP 2: Process in \"Parallel\" #################\r\n",
      "for datachunk in $data.chunk.*; do\r\n",
      "    # redirect the lines of text into the user supplied executable (mapper)\r\n",
      "    # and redirect STDOUT to a temporary file on disk\r\n",
      "    ./$mapper  < $datachunk > $datachunk.counts &\r\n",
      "done\r\n",
      "# wait for the mappers to finish their work\r\n",
      "wait\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "############## STEP 3: Collect the results #################\r\n",
      "# 'ls' makes a list of the temporary count files\r\n",
      "# 'perl -pe' regex replaces line breaks with spaces\r\n",
      "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\r\n",
      "\r\n",
      "# If no 'reducer' executable was provided ...\r\n",
      "if [ $# -eq 3 ]\r\n",
      "  then\r\n",
      "    # combine all the count files into one\r\n",
      "    cat $countfiles > $data.output\r\n",
      "fi\r\n",
      "\r\n",
      "# Otherwise...\r\n",
      "if [ $# -eq 4 ]\r\n",
      "  then\r\n",
      "    reducer=$4\r\n",
      "    ################ YOUR CODE HERE #############\r\n",
      "    #Q6c\r\n",
      "    cat $countfiles | python $reducer > $data.output\r\n",
      "    #python $reducer < $data.output > $data.output\r\n",
      "\r\n",
      "    ################# (END YOUR CODE)###########\r\n",
      "fi\r\n",
      "\r\n",
      "\r\n",
      "############## STEP 4: Final Output #################\r\n",
      "# clean up the data chunks and temporary count files\r\n",
      "\\rm $data.chunk.*\r\n",
      "# display the content of the output file:\r\n",
      "cat $data.output\r\n",
      "\r\n",
      "exit\r\n"
     ]
    }
   ],
   "source": [
    "!cat pWordCount_v1.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - parallel word count on Alice text (RUN THIS CELL AS IS)\n",
    "!./pWordCount_v1.sh 4 'data/alice.txt' \\\n",
    "                   'wordCount.py' \\\n",
    "                   'aggregateCounts_v1.py' \\\n",
    "                   > 'data/alice_pCounts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice\t403\r\n"
     ]
    }
   ],
   "source": [
    "# part c - check alice count (RUN THIS CELL AS IS)\n",
    "!grep alice data/alice_pCounts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "SORT(1)                   BSD General Commands Manual                  SORT(1)\r\n",
      "\r\n",
      "N\bNA\bAM\bME\bE\r\n",
      "     s\bso\bor\brt\bt -- sort or merge records (lines) of text and binary files\r\n",
      "\r\n",
      "S\bSY\bYN\bNO\bOP\bPS\bSI\bIS\bS\r\n",
      "     s\bso\bor\brt\bt [-\b-b\bbc\bcC\bCd\bdf\bfg\bgh\bhi\biR\bRM\bMm\bmn\bnr\brs\bsu\buV\bVz\bz] [-\b-k\bk _\bf_\bi_\be_\bl_\bd_\b1[,_\bf_\bi_\be_\bl_\bd_\b2]] [-\b-S\bS _\bm_\be_\bm_\bs_\bi_\bz_\be] [-\b-T\bT _\bd_\bi_\br]\r\n",
      "          [-\b-t\bt _\bc_\bh_\ba_\br] [-\b-o\bo _\bo_\bu_\bt_\bp_\bu_\bt] [_\bf_\bi_\bl_\be _\b._\b._\b.]\r\n",
      "     s\bso\bor\brt\bt -\b--\b-h\bhe\bel\blp\bp\r\n",
      "     s\bso\bor\brt\bt -\b--\b-v\bve\ber\brs\bsi\bio\bon\bn\r\n",
      "\r\n",
      "D\bDE\bES\bSC\bCR\bRI\bIP\bPT\bTI\bIO\bON\bN\r\n",
      "     The s\bso\bor\brt\bt utility sorts text and binary files by lines.  A line is a\r\n",
      "     record separated from the subsequent record by a newline (default) or NUL\r\n",
      "     '\\0' character (-z option).  A record can contain any printable or\r\n",
      "     unprintable characters.  Comparisons are based on one or more sort keys\r\n",
      "     extracted from each line of input, and are performed lexicographically,\r\n",
      "     according to the current locale's collating rules and the specified com-\r\n",
      "     mand-line options that can tune the actual sorting behavior.  By default,\r\n",
      "     if keys are not given, s\bso\bor\brt\bt uses entire lines for comparison.\r\n",
      "\r\n",
      "     The command line options are as follows:\r\n",
      "\r\n",
      "     -\b-c\bc, -\b--\b-c\bch\bhe\bec\bck\bk, -\b-C\bC, -\b--\b-c\bch\bhe\bec\bck\bk=\b=s\bsi\bil\ble\ben\bnt\bt|\b|q\bqu\bui\bie\bet\bt\r\n",
      "             Check that the single input file is sorted.  If the file is not\r\n",
      "             sorted, s\bso\bor\brt\bt produces the appropriate error messages and exits\r\n",
      "             with code 1, otherwise returns 0.  If -\b-C\bC or -\b--\b-c\bch\bhe\bec\bck\bk=\b=s\bsi\bil\ble\ben\bnt\bt is\r\n",
      "             specified, s\bso\bor\brt\bt produces no output.  This is a \"silent\" version\r\n",
      "             of -\b-c\bc.\r\n",
      "\r\n",
      "     -\b-m\bm, -\b--\b-m\bme\ber\brg\bge\be\r\n",
      "             Merge only.  The input files are assumed to be pre-sorted.  If\r\n",
      "             they are not sorted the output order is undefined.\r\n",
      "\r\n",
      "     -\b-o\bo _\bo_\bu_\bt_\bp_\bu_\bt, -\b--\b-o\bou\but\btp\bpu\but\bt=_\bo_\bu_\bt_\bp_\bu_\bt\r\n",
      "             Print the output to the _\bo_\bu_\bt_\bp_\bu_\bt file instead of the standard out-\r\n",
      "             put.\r\n",
      "\r\n",
      "     -\b-S\bS _\bs_\bi_\bz_\be, -\b--\b-b\bbu\buf\bff\bfe\ber\br-\b-s\bsi\biz\bze\be=_\bs_\bi_\bz_\be\r\n",
      "             Use _\bs_\bi_\bz_\be for the maximum size of the memory buffer.  Size modi-\r\n",
      "             fiers %,b,K,M,G,T,P,E,Z,Y can be used.  If a memory limit is not\r\n",
      "             explicitly specified, s\bso\bor\brt\bt takes up to about 90% of available\r\n",
      "             memory.  If the file size is too big to fit into the memory\r\n",
      "             buffer, the temporary disk files are used to perform the sorting.\r\n",
      "\r\n",
      "     -\b-T\bT _\bd_\bi_\br, -\b--\b-t\bte\bem\bmp\bpo\bor\bra\bar\bry\by-\b-d\bdi\bir\bre\bec\bct\bto\bor\bry\by=_\bd_\bi_\br\r\n",
      "             Store temporary files in the directory _\bd_\bi_\br.  The default path is\r\n",
      "             the value of the environment variable TMPDIR or _\b/_\bv_\ba_\br_\b/_\bt_\bm_\bp if\r\n",
      "             TMPDIR is not defined.\r\n",
      "\r\n",
      "     -\b-u\bu, -\b--\b-u\bun\bni\biq\bqu\bue\be\r\n",
      "             Unique keys.  Suppress all lines that have a key that is equal to\r\n",
      "             an already processed one.  This option, similarly to -\b-s\bs, implies\r\n",
      "             a stable sort.  If used with -\b-c\bc or -\b-C\bC, s\bso\bor\brt\bt also checks that\r\n",
      "             there are no lines with duplicate keys.\r\n",
      "\r\n",
      "     -\b-s\bs      Stable sort.  This option maintains the original record order of\r\n",
      "             records that have an equal key.  This is a non-standard feature,\r\n",
      "             but it is widely accepted and used.\r\n",
      "\r\n",
      "     -\b--\b-v\bve\ber\brs\bsi\bio\bon\bn\r\n",
      "             Print the version and silently exits.\r\n",
      "\r\n",
      "     -\b--\b-h\bhe\bel\blp\bp  Print the help text and silently exits.\r\n",
      "\r\n",
      "     The following options override the default ordering rules.  When ordering\r\n",
      "     options appear independently of key field specifications, they apply\r\n",
      "     globally to all sort keys.  When attached to a specific key (see -\b-k\bk), the\r\n",
      "     ordering options override all global ordering options for the key they\r\n",
      "     are attached to.\r\n",
      "\r\n",
      "     -\b-b\bb, -\b--\b-i\big\bgn\bno\bor\bre\be-\b-l\ble\bea\bad\bdi\bin\bng\bg-\b-b\bbl\bla\ban\bnk\bks\bs\r\n",
      "             Ignore leading blank characters when comparing lines.\r\n",
      "\r\n",
      "     -\b-d\bd, -\b--\b-d\bdi\bic\bct\bti\bio\bon\bna\bar\bry\by-\b-o\bor\brd\bde\ber\br\r\n",
      "             Consider only blank spaces and alphanumeric characters in compar-\r\n",
      "             isons.\r\n",
      "\r\n",
      "     -\b-f\bf, -\b--\b-i\big\bgn\bno\bor\bre\be-\b-c\bca\bas\bse\be\r\n",
      "             Convert all lowercase characters to their uppercase equivalent\r\n",
      "             before comparison, that is, perform case-independent sorting.\r\n",
      "\r\n",
      "     -\b-g\bg, -\b--\b-g\bge\ben\bne\ber\bra\bal\bl-\b-n\bnu\bum\bme\ber\bri\bic\bc-\b-s\bso\bor\brt\bt, -\b--\b-s\bso\bor\brt\bt=\b=g\bge\ben\bne\ber\bra\bal\bl-\b-n\bnu\bum\bme\ber\bri\bic\bc\r\n",
      "             Sort by general numerical value.  As opposed to -\b-n\bn, this option\r\n",
      "             handles general floating points.  It has a more permissive format\r\n",
      "             than that allowed by -\b-n\bn but it has a significant performance\r\n",
      "             drawback.\r\n",
      "\r\n",
      "     -\b-h\bh, -\b--\b-h\bhu\bum\bma\ban\bn-\b-n\bnu\bum\bme\ber\bri\bic\bc-\b-s\bso\bor\brt\bt, -\b--\b-s\bso\bor\brt\bt=\b=h\bhu\bum\bma\ban\bn-\b-n\bnu\bum\bme\ber\bri\bic\bc\r\n",
      "             Sort by numerical value, but take into account the SI suffix, if\r\n",
      "             present.  Sort first by numeric sign (negative, zero, or posi-\r\n",
      "             tive); then by SI suffix (either empty, or `k' or `K', or one of\r\n",
      "             `MGTPEZY', in that order); and finally by numeric value.  The SI\r\n",
      "             suffix must immediately follow the number.  For example, '12345K'\r\n",
      "             sorts before '1M', because M is \"larger\" than K.  This sort\r\n",
      "             option is useful for sorting the output of a single invocation of\r\n",
      "             'df' command with -\b-h\bh or -\b-H\bH options (human-readable).\r\n",
      "\r\n",
      "     -\b-i\bi, -\b--\b-i\big\bgn\bno\bor\bre\be-\b-n\bno\bon\bnp\bpr\bri\bin\bnt\bti\bin\bng\bg\r\n",
      "             Ignore all non-printable characters.\r\n",
      "\r\n",
      "     -\b-M\bM, -\b--\b-m\bmo\bon\bnt\bth\bh-\b-s\bso\bor\brt\bt, -\b--\b-s\bso\bor\brt\bt=\b=m\bmo\bon\bnt\bth\bh\r\n",
      "             Sort by month abbreviations.  Unknown strings are considered\r\n",
      "             smaller than the month names.\r\n",
      "\r\n",
      "     -\b-n\bn, -\b--\b-n\bnu\bum\bme\ber\bri\bic\bc-\b-s\bso\bor\brt\bt, -\b--\b-s\bso\bor\brt\bt=\b=n\bnu\bum\bme\ber\bri\bic\bc\r\n",
      "             Sort fields numerically by arithmetic value.  Fields are supposed\r\n",
      "             to have optional blanks in the beginning, an optional minus sign,\r\n",
      "             zero or more digits (including decimal point and possible thou-\r\n",
      "             sand separators).\r\n",
      "\r\n",
      "     -\b-R\bR, -\b--\b-r\bra\ban\bnd\bdo\bom\bm-\b-s\bso\bor\brt\bt, -\b--\b-s\bso\bor\brt\bt=\b=r\bra\ban\bnd\bdo\bom\bm\r\n",
      "             Sort by a random order.  This is a random permutation of the\r\n",
      "             inputs except that the equal keys sort together.  It is imple-\r\n",
      "             mented by hashing the input keys and sorting the hash values.\r\n",
      "             The hash function is chosen randomly.  The hash function is ran-\r\n",
      "             domized by /\b/d\bde\bev\bv/\b/r\bra\ban\bnd\bdo\bom\bm content, or by file content if it is spec-\r\n",
      "             ified by -\b--\b-r\bra\ban\bnd\bdo\bom\bm-\b-s\bso\bou\bur\brc\bce\be.  Even if multiple sort fields are spec-\r\n",
      "             ified, the same random hash function is used for all of them.\r\n",
      "\r\n",
      "     -\b-r\br, -\b--\b-r\bre\bev\bve\ber\brs\bse\be\r\n",
      "             Sort in reverse order.\r\n",
      "\r\n",
      "     -\b-V\bV, -\b--\b-v\bve\ber\brs\bsi\bio\bon\bn-\b-s\bso\bor\brt\bt\r\n",
      "             Sort version numbers.  The input lines are treated as file names\r\n",
      "             in form PREFIX VERSION SUFFIX, where SUFFIX matches the regular\r\n",
      "             expression \"(.([A-Za-z~][A-Za-z0-9~]*)?)*\".  The files are com-\r\n",
      "             pared by their prefixes and versions (leading zeros are ignored\r\n",
      "             in version numbers, see example below).  If an input string does\r\n",
      "             not match the pattern, then it is compared using the byte compare\r\n",
      "             function.  All string comparisons are performed in C locale, the\r\n",
      "             locale environment setting is ignored.\r\n",
      "\r\n",
      "             Example:\r\n",
      "\r\n",
      "             $ ls sort* | sort -V\r\n",
      "\r\n",
      "             sort-1.022.tgz\r\n",
      "\r\n",
      "             sort-1.23.tgz\r\n",
      "\r\n",
      "             sort-1.23.1.tgz\r\n",
      "\r\n",
      "             sort-1.024.tgz\r\n",
      "\r\n",
      "             sort-1.024.003.\r\n",
      "\r\n",
      "             sort-1.024.003.tgz\r\n",
      "\r\n",
      "             sort-1.024.07.tgz\r\n",
      "\r\n",
      "             sort-1.024.009.tgz\r\n",
      "\r\n",
      "     The treatment of field separators can be altered using these options:\r\n",
      "\r\n",
      "     -\b-b\bb, -\b--\b-i\big\bgn\bno\bor\bre\be-\b-l\ble\bea\bad\bdi\bin\bng\bg-\b-b\bbl\bla\ban\bnk\bks\bs\r\n",
      "             Ignore leading blank space when determining the start and end of\r\n",
      "             a restricted sort key (see -\b-k\bk).  If -\b-b\bb is specified before the\r\n",
      "             first -\b-k\bk option, it applies globally to all key specifications.\r\n",
      "             Otherwise, -\b-b\bb can be attached independently to each _\bf_\bi_\be_\bl_\bd argu-\r\n",
      "             ment of the key specifications.  -\b-b\bb.\r\n",
      "\r\n",
      "     -\b-k\bk _\bf_\bi_\be_\bl_\bd_\b1[,_\bf_\bi_\be_\bl_\bd_\b2], -\b--\b-k\bke\bey\by=_\bf_\bi_\be_\bl_\bd_\b1[,_\bf_\bi_\be_\bl_\bd_\b2]\r\n",
      "             Define a restricted sort key that has the starting position\r\n",
      "             _\bf_\bi_\be_\bl_\bd_\b1, and optional ending position _\bf_\bi_\be_\bl_\bd_\b2 of a key field.  The\r\n",
      "             -\b-k\bk option may be specified multiple times, in which case subse-\r\n",
      "             quent keys are compared when earlier keys compare equal.  The -\b-k\bk\r\n",
      "             option replaces the obsolete options +\b+_\bp_\bo_\bs_\b1 and -\b-_\bp_\bo_\bs_\b2, but the old\r\n",
      "             notation is also supported.\r\n",
      "\r\n",
      "     -\b-t\bt _\bc_\bh_\ba_\br, -\b--\b-f\bfi\bie\bel\bld\bd-\b-s\bse\bep\bpa\bar\bra\bat\bto\bor\br=_\bc_\bh_\ba_\br\r\n",
      "             Use _\bc_\bh_\ba_\br as a field separator character.  The initial _\bc_\bh_\ba_\br is not\r\n",
      "             considered to be part of a field when determining key offsets.\r\n",
      "             Each occurrence of _\bc_\bh_\ba_\br is significant (for example, ``_\bc_\bh_\ba_\br_\bc_\bh_\ba_\br''\r\n",
      "             delimits an empty field).  If -\b-t\bt is not specified, the default\r\n",
      "             field separator is a sequence of blank space characters, and con-\r\n",
      "             secutive blank spaces do _\bn_\bo_\bt delimit an empty field, however, the\r\n",
      "             initial blank space _\bi_\bs considered part of a field when determin-\r\n",
      "             ing key offsets.  To use NUL as field separator, use -\b-t\bt '\\0'.\r\n",
      "\r\n",
      "     -\b-z\bz, -\b--\b-z\bze\ber\bro\bo-\b-t\bte\ber\brm\bmi\bin\bna\bat\bte\bed\bd\r\n",
      "             Use NUL as record separator.  By default, records in the files\r\n",
      "             are supposed to be separated by the newline characters.  With\r\n",
      "             this option, NUL ('\\0') is used as a record separator character.\r\n",
      "\r\n",
      "     Other options:\r\n",
      "\r\n",
      "     -\b--\b-b\bba\bat\btc\bch\bh-\b-s\bsi\biz\bze\be=_\bn_\bu_\bm\r\n",
      "             Specify maximum number of files that can be opened by s\bso\bor\brt\bt at\r\n",
      "             once.  This option affects behavior when having many input files\r\n",
      "             or using temporary files.  The default value is 16.\r\n",
      "\r\n",
      "     -\b--\b-c\bco\bom\bmp\bpr\bre\bes\bss\bs-\b-p\bpr\bro\bog\bgr\bra\bam\bm=_\bP_\bR_\bO_\bG_\bR_\bA_\bM\r\n",
      "             Use PROGRAM to compress temporary files.  PROGRAM must compress\r\n",
      "             standard input to standard output, when called without arguments.\r\n",
      "             When called with argument -\b-d\bd it must decompress standard input to\r\n",
      "             standard output.  If PROGRAM fails, s\bso\bor\brt\bt must exit with error.\r\n",
      "             An example of PROGRAM that can be used here is bzip2.\r\n",
      "\r\n",
      "     -\b--\b-r\bra\ban\bnd\bdo\bom\bm-\b-s\bso\bou\bur\brc\bce\be=_\bf_\bi_\bl_\be_\bn_\ba_\bm_\be\r\n",
      "             In random sort, the file content is used as the source of the\r\n",
      "             'seed' data for the hash function choice.  Two invocations of\r\n",
      "             random sort with the same seed data will use the same hash func-\r\n",
      "             tion and will produce the same result if the input is also iden-\r\n",
      "             tical.  By default, file /\b/d\bde\bev\bv/\b/r\bra\ban\bnd\bdo\bom\bm is used.\r\n",
      "\r\n",
      "     -\b--\b-d\bde\beb\bbu\bug\bg\r\n",
      "             Print some extra information about the sorting process to the\r\n",
      "             standard output.\r\n",
      "\r\n",
      "     -\b--\b-p\bpa\bar\bra\bal\bll\ble\bel\bl\r\n",
      "             Set the maximum number of execution threads.  Default number\r\n",
      "             equals to the number of CPUs.\r\n",
      "\r\n",
      "     -\b--\b-f\bfi\bil\ble\bes\bs0\b0-\b-f\bfr\bro\bom\bm=_\bf_\bi_\bl_\be_\bn_\ba_\bm_\be\r\n",
      "             Take the input file list from the file _\bf_\bi_\bl_\be_\bn_\ba_\bm_\be.  The file names\r\n",
      "             must be separated by NUL (like the output produced by the command\r\n",
      "             \"find ... -print0\").\r\n",
      "\r\n",
      "     -\b--\b-r\bra\bad\bdi\bix\bxs\bso\bor\brt\bt\r\n",
      "             Try to use radix sort, if the sort specifications allow.  The\r\n",
      "             radix sort can only be used for trivial locales (C and POSIX),\r\n",
      "             and it cannot be used for numeric or month sort.  Radix sort is\r\n",
      "             very fast and stable.\r\n",
      "\r\n",
      "     -\b--\b-m\bme\ber\brg\bge\bes\bso\bor\brt\bt\r\n",
      "             Use mergesort.  This is a universal algorithm that can always be\r\n",
      "             used, but it is not always the fastest.\r\n",
      "\r\n",
      "     -\b--\b-q\bqs\bso\bor\brt\bt\r\n",
      "             Try to use quick sort, if the sort specifications allow.  This\r\n",
      "             sort algorithm cannot be used with -\b-u\bu and -\b-s\bs.\r\n",
      "\r\n",
      "     -\b--\b-h\bhe\bea\bap\bps\bso\bor\brt\bt\r\n",
      "             Try to use heap sort, if the sort specifications allow.  This\r\n",
      "             sort algorithm cannot be used with -\b-u\bu and -\b-s\bs.\r\n",
      "\r\n",
      "     -\b--\b-m\bmm\bma\bap\bp  Try to use file memory mapping system call.  It may increase\r\n",
      "             speed in some cases.\r\n",
      "\r\n",
      "     The following operands are available:\r\n",
      "\r\n",
      "     _\bf_\bi_\bl_\be    The pathname of a file to be sorted, merged, or checked.  If no\r\n",
      "             _\bf_\bi_\bl_\be operands are specified, or if a _\bf_\bi_\bl_\be operand is -\b-, the stan-\r\n",
      "             dard input is used.\r\n",
      "\r\n",
      "     A field is defined as a maximal sequence of characters other than the\r\n",
      "     field separator and record separator (newline by default).  Initial blank\r\n",
      "     spaces are included in the field unless -\b-b\bb has been specified; the first\r\n",
      "     blank space of a sequence of blank spaces acts as the field separator and\r\n",
      "     is included in the field (unless -\b-t\bt is specified).  For example, all\r\n",
      "     blank spaces at the beginning of a line are considered to be part of the\r\n",
      "     first field.\r\n",
      "\r\n",
      "     Fields are specified by the -\b-k\bk _\bf_\bi_\be_\bl_\bd_\b1[,_\bf_\bi_\be_\bl_\bd_\b2] command-line option.  If\r\n",
      "     _\bf_\bi_\be_\bl_\bd_\b2 is missing, the end of the key defaults to the end of the line.\r\n",
      "\r\n",
      "     The arguments _\bf_\bi_\be_\bl_\bd_\b1 and _\bf_\bi_\be_\bl_\bd_\b2 have the form _\bm_\b._\bn _\b(_\bm_\b,_\bn _\b> _\b0_\b) and can be\r\n",
      "     followed by one or more of the modifiers b\bb, d\bd, f\bf, i\bi, n\bn, g\bg, M\bM and r\br, which\r\n",
      "     correspond to the options discussed above.  When b\bb is specified it\r\n",
      "     applies only to _\bf_\bi_\be_\bl_\bd_\b1 or _\bf_\bi_\be_\bl_\bd_\b2 where it is specified while the rest of\r\n",
      "     the modifiers apply to the whole key field regardless if they are speci-\r\n",
      "     fied only with _\bf_\bi_\be_\bl_\bd_\b1 or _\bf_\bi_\be_\bl_\bd_\b2 or both.  A _\bf_\bi_\be_\bl_\bd_\b1 position specified by\r\n",
      "     _\bm_\b._\bn is interpreted as the _\bnth character from the beginning of the _\bmth\r\n",
      "     field.  A missing _\b._\bn in _\bf_\bi_\be_\bl_\bd_\b1 means `.1', indicating the first character\r\n",
      "     of the _\bmth field; if the -\b-b\bb option is in effect, _\bn is counted from the\r\n",
      "     first non-blank character in the _\bmth field; _\bm.1b refers to the first non-\r\n",
      "     blank character in the _\bmth field.  1._\bn refers to the _\bnth character from\r\n",
      "     the beginning of the line; if _\bn is greater than the length of the line,\r\n",
      "     the field is taken to be empty.\r\n",
      "\r\n",
      "     _\bnth positions are always counted from the field beginning, even if the\r\n",
      "     field is shorter than the number of specified positions.  Thus, the key\r\n",
      "     can really start from a position in a subsequent field.\r\n",
      "\r\n",
      "     A _\bf_\bi_\be_\bl_\bd_\b2 position specified by _\bm_\b._\bn is interpreted as the _\bnth character\r\n",
      "     (including separators) from the beginning of the _\bmth field.  A missing _\b._\bn\r\n",
      "     indicates the last character of the _\bmth field; _\bm = 0 designates the end\r\n",
      "     of a line.  Thus the option -\b-k\bk _\bv_\b._\bx_\b,_\bw_\b._\by is synonymous with the obsolete\r\n",
      "     option +\b+_\bv_\b-_\b1_\b._\bx_\b-_\b1 -\b-_\bw_\b-_\b1_\b._\by; when _\by is omitted, -\b-k\bk _\bv_\b._\bx_\b,_\bw is synonymous with\r\n",
      "     +\b+_\bv_\b-_\b1_\b._\bx_\b-_\b1 -\b-_\bw_\b._\b0.  The obsolete +\b+_\bp_\bo_\bs_\b1 -\b-_\bp_\bo_\bs_\b2 option is still supported,\r\n",
      "     except for -\b-_\bw_\b._\b0_\bb, which has no -\b-k\bk equivalent.\r\n",
      "\r\n",
      "E\bEN\bNV\bVI\bIR\bRO\bON\bNM\bME\bEN\bNT\bT\r\n",
      "     LC_COLLATE  Locale settings to be used to determine the collation for\r\n",
      "                 sorting records.\r\n",
      "\r\n",
      "     LC_CTYPE    Locale settings to be used to case conversion and classifica-\r\n",
      "                 tion of characters, that is, which characters are considered\r\n",
      "                 whitespaces, etc.\r\n",
      "\r\n",
      "     LC_MESSAGES\r\n",
      "                 Locale settings that determine the language of output mes-\r\n",
      "                 sages that s\bso\bor\brt\bt prints out.\r\n",
      "\r\n",
      "     LC_NUMERIC  Locale settings that determine the number format used in\r\n",
      "                 numeric sort.\r\n",
      "\r\n",
      "     LC_TIME     Locale settings that determine the month format used in month\r\n",
      "                 sort.\r\n",
      "\r\n",
      "     LC_ALL      Locale settings that override all of the above locale set-\r\n",
      "                 tings.  This environment variable can be used to set all\r\n",
      "                 these settings to the same value at once.\r\n",
      "\r\n",
      "     LANG        Used as a last resort to determine different kinds of locale-\r\n",
      "                 specific behavior if neither the respective environment vari-\r\n",
      "                 able, nor LC_ALL are set.\r\n",
      "\r\n",
      "     TMPDIR      Path to the directory in which temporary files will be\r\n",
      "                 stored.  Note that TMPDIR may be overridden by the -\b-T\bT option.\r\n",
      "\r\n",
      "     GNUSORT_NUMERIC_COMPATIBILITY\r\n",
      "                 If defined -\b-t\bt will not override the locale numeric symbols,\r\n",
      "                 that is, thousand separators and decimal separators.  By\r\n",
      "                 default, if we specify -\b-t\bt with the same symbol as the thou-\r\n",
      "                 sand separator or decimal point, the symbol will be treated\r\n",
      "                 as the field separator.  Older behavior was less definite;\r\n",
      "                 the symbol was treated as both field separator and numeric\r\n",
      "                 separator, simultaneously.  This environment variable enables\r\n",
      "                 the old behavior.\r\n",
      "\r\n",
      "     GNUSORT_COMPATIBLE_BLANKS\r\n",
      "                 Use 'space' symbols as field separators (as modern GNU sort\r\n",
      "                 does).\r\n",
      "\r\n",
      "F\bFI\bIL\bLE\bES\bS\r\n",
      "     /var/tmp/.bsdsort.PID.*           Temporary files.\r\n",
      "     /dev/random                       Default seed file for the random sort.\r\n",
      "\r\n",
      "E\bEX\bXI\bIT\bT S\bST\bTA\bAT\bTU\bUS\bS\r\n",
      "     The s\bso\bor\brt\bt utility shall exit with one of the following values:\r\n",
      "\r\n",
      "     0     Successfully sorted the input files or if used with -\b-c\bc or -\b-C\bC, the\r\n",
      "           input file already met the sorting criteria.\r\n",
      "     1     On disorder (or non-uniqueness) with the -\b-c\bc or -\b-C\bC options.\r\n",
      "     2     An error occurred.\r\n",
      "\r\n",
      "S\bSE\bEE\bE A\bAL\bLS\bSO\bO\r\n",
      "     comm(1), join(1), uniq(1)\r\n",
      "\r\n",
      "S\bST\bTA\bAN\bND\bDA\bAR\bRD\bDS\bS\r\n",
      "     The s\bso\bor\brt\bt utility is compliant with the IEEE Std 1003.1-2008 (``POSIX.1'')\r\n",
      "     specification.\r\n",
      "\r\n",
      "     The flags [-\b-g\bgh\bhR\bRM\bMS\bSs\bsT\bTV\bVz\bz] are extensions to the POSIX specification.\r\n",
      "\r\n",
      "     All long options are extensions to the specification, some of them are\r\n",
      "     provided for compatibility with GNU versions and some of them are own\r\n",
      "     extensions.\r\n",
      "\r\n",
      "     The old key notations +\b+_\bp_\bo_\bs_\b1 and -\b-_\bp_\bo_\bs_\b2 come from older versions of s\bso\bor\brt\bt\r\n",
      "     and are still supported but their use is highly discouraged.\r\n",
      "\r\n",
      "H\bHI\bIS\bST\bTO\bOR\bRY\bY\r\n",
      "     A s\bso\bor\brt\bt command first appeared in Version 3 AT&T UNIX.\r\n",
      "\r\n",
      "A\bAU\bUT\bTH\bHO\bOR\bRS\bS\r\n",
      "     Gabor Kovesdan <_\bg_\ba_\bb_\bo_\br_\b@_\bF_\br_\be_\be_\bB_\bS_\bD_\b._\bo_\br_\bg>,\r\n",
      "\r\n",
      "     Oleg Moskalenko <_\bm_\bo_\bm_\b0_\b4_\b0_\b2_\b6_\b7_\b@_\bg_\bm_\ba_\bi_\bl_\b._\bc_\bo_\bm>\r\n",
      "\r\n",
      "N\bNO\bOT\bTE\bES\bS\r\n",
      "     This implementation of s\bso\bor\brt\bt has no limits on input line length (other\r\n",
      "     than imposed by available memory) or any restrictions on bytes allowed\r\n",
      "     within lines.\r\n",
      "\r\n",
      "     The performance depends highly on locale settings, efficient choice of\r\n",
      "     sort keys and key complexity.  The fastest sort is with locale C, on\r\n",
      "     whole lines, with option -\b-s\bs.  In general, locale C is the fastest, then\r\n",
      "     single-byte locales follow and multi-byte locales as the slowest but the\r\n",
      "     correct collation order is always respected.  As for the key specifica-\r\n",
      "     tion, the simpler to process the lines the faster the search will be.\r\n",
      "\r\n",
      "     When sorting by arithmetic value, using -\b-n\bn results in much better perfor-\r",
      "\r\n",
      "     mance than -\b-g\bg so its use is encouraged whenever possible.\r\n",
      "\r\n",
      "BSD                             March 19, 2015                             BSD\r\n"
     ]
    }
   ],
   "source": [
    "!man sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7: Parallel Word Count (part 2)\n",
    "Congratulations, you've just implemented a Map-Reduce algorithm! From here on out, we'll refer to the two python scripts you passed to `pWordCount_v1.sh` as '_mapper_' and '_reducer_'. The bash script itself served as our '_framework_' -- it split up the original input, then ___mapped___ our word counting script on to each chunk, then ___aggregated (a.k.a. reduced)___ the resulting files by piping them into our collation script.  Unfortunately, as you may have realized already, there is a major scalability concern with this particular implementation. __In this question you'll fix our implementation of parallel word count so that it will be scalable.__\n",
    "\n",
    "__HINT:__ MapReduce uses the Merge-Sort algorithm under the hood. Linux `sort` command has a merge option which you can use to simiulate the MapReduce framework. Use the `man sort` command to find more information on this option. \n",
    "\n",
    "### Q7 Tasks:\n",
    "\n",
    "* __a) short response:__ What is the potential scalability problem with the provided implementation of `aggregateCounts_v1.py`? Explain why this supposedly 'parallelized' Word Count wouldn't work on a really large input corpus.  [`HINT:` _see the intro to Q6_]\n",
    "\n",
    "\n",
    "* __b) code:__ Fortunately, a 'strategic sort' can solve this problem. Read the instructions at the top of `pWordCount_v2.sh` carefully then make your changes that alphabetically sort the output from the mappers (`countfiles`) before piping them into the reducer script.\n",
    "\n",
    "\n",
    "* __c) code:__ Write the main part of `aggregateCounts_v2.py` so that it takes advantage of the sorted input to add duplicate counts without storing the whole vocabulary in memory. Refer to the file docstring for more detailed instructions. Confirm that your implementation works by running it on both the test and true data files.\n",
    "\n",
    "\n",
    "* __d) short response:__ If you are paying close attention, this rewritten reducer sets us up for a truly scalable solution, but doesn't get us all the way there. In particular, while we chunked our data so it can be processed by multiple mappers, we're still streaming the entire dataset through one reduce script. If the vocabulary is too large to fit on a single computer, we might split the word counts in half after sorting them, then perform the reducing on two separate machines. Explain what could go wrong with this approach. (For now, ignore the question of how we'd sort a dataset that is too large to fit on a single machine and just focus on what might be wrong about the result of this split-in-half reducing).\n",
    "\n",
    "\n",
    "* __e) short response:__ Can you come up with a different way of splitting up the data that would allow us to perform the reducing on separate machines without needing any postprocessing? This is a theoretical question -- don't worry if you don't know how to implement your idea in a bash script, just describe how you'd want to split the sorted counts into different files to be reduced separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7 Student Answers:\n",
    "\n",
    "> __a)__ <span style=\"font-family:Verdana; color: green;\"> 'Parallelized' Word Count wouldn't work on a really large dataset as we have to maintain a hashmap in the memory to aggregate the count. All words have to be kept in memory in the node where the reducer is running.</span>\n",
    "\n",
    "> __b-c)__ _complete the coding portions of this question before answering d and e._ <span style=\"font-family:Verdana; color: green;\"> Here we modified the pWordCount_v2.sh, such that the input document is partitioned into smaller chunks and mapper does the word count on the partition is was assigned. The output from this mapper is later piped to sort and written down into a sorted sequence of wordcount pairs in to a file. Later these files are passed as a argument to the linux sort function with an option '-m' to pick the merge sort option. The output of the sort is then later piped to the reducer for aggregation. In the reducer, as instructed we eliminated the hashmap we used in the earlier version of the aggregator. Instead we used two variables one for the previous word and its corresponding count. When the current word is differnt from that of the preivous word we immediately write out the word and count and replace the previous word with the current and continue until there are no more words. Finally the last word and its count is written out.</span>\n",
    "\n",
    "> __d)__ <span style=\"font-family:Verdana; color: green;\"> When we split the word count in half at midpoint after sorting. We can pass the first half to the first reducer and the second half to the second reducer. Potentially our midpoint could break the word counts of a particular word such that some counts of the word in the first half and some counts of word in the second half. As result we will have two counts for that word appearing in the output of the first reducer and as well as in the second reducer. So, we have to do a postprocessing to fix that discepency.</span>\n",
    "\n",
    "> __e)__ <span style=\"font-family:Verdana; color: green;\"> Instead of splitting the word count at midpoint afer sorting, we can split at word boundary by picking words starting between [a-m] to the first reducer and words starting between [n-z] to the second reducer. Now the word count records for a word will all be always go to the same reducer and no need for a postprocessing as we did in our previous section. Also, now we will know which output file to look for the word count for a word starting with 'd' for example. It will be in the output of the first reducer. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chmod: aggregateCounts_v2.sh: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# Add permissions to your new files (RUN THIS CELL AS IS)\n",
    "!chmod a+x pWordCount_v2.sh\n",
    "!chmod a+x aggregateCounts_v2.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "# pWordCount.sh\r\n",
      "# Author: James G. Shanahan\r\n",
      "# Usage: pWordCount.sh m testFile.txt mapper.py [reducer.py]\r\n",
      "# Input:\r\n",
      "#   m = number of processes (maps), e.g., 4\r\n",
      "#   inputFile = a text input file\r\n",
      "#   mapper = an executable that reads from STDIN and prints to STDOUT\r\n",
      "#   reducer = (optional) an executable that reads from STDIN and prints \r\n",
      "#             to STDOUT, if no reducer is provided, the framework will\r\n",
      "#             simply stream the mapper output.\r\n",
      "#\r\n",
      "# Instructions:\r\n",
      "#    For Q7b - Ammend this script in STEP 2 and STEP 3 to \r\n",
      "#              alphabtetically sort the contents of each chunk before\r\n",
      "#              piping them into the reducer script and redirecting on to \r\n",
      "# .            $data.output.\r\n",
      "# --------------------------------------------------------------------\r\n",
      "\r\n",
      "usage()\r\n",
      "{\r\n",
      "    echo ERROR: No arguments supplied\r\n",
      "    echo\r\n",
      "    echo To run use\r\n",
      "    echo \"pWordCount.sh m inputFile mapper.py [reducer.py]\"\r\n",
      "    echo Input:\r\n",
      "    echo \"number of processes/maps, EG, 4\"\r\n",
      "    echo \"mapper.py = an executable script to apply to each chunk in parallel\"\r\n",
      "    echo \"reducer.py = an executable script after the parallel processes are complete.\"\r\n",
      "    echo NOTE: if no reducer is supplied, we will simply combine the output files.\r\n",
      "}\r\n",
      "\r\n",
      "\r\n",
      "# print the usage message if this script is called without required args\r\n",
      "if [ $# -lt 3 ]\r\n",
      "  then\r\n",
      "    usage\r\n",
      "    exit 1\r\n",
      "fi\r\n",
      "\r\n",
      "# collect the arguments\r\n",
      "m=$1\r\n",
      "data=$2\r\n",
      "mapper=$3\r\n",
      "\r\n",
      "\r\n",
      "################# STEP 1: Split up the data #################\r\n",
      "# 'wc' determines the number of lines in the data\r\n",
      "# 'perl -pe' regex strips the piped wc output to a number\r\n",
      "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\r\n",
      "\r\n",
      "# determine the lines per chunk for the desired number of processes\r\n",
      "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\r\n",
      "\r\n",
      "# split the original file into chunks by line\r\n",
      "split -l $linesinchunk $data $data.chunk.\r\n",
      "\r\n",
      "\r\n",
      "############## STEP 2: Process in \"Parallel\" #################\r\n",
      "for datachunk in $data.chunk.*; do\r\n",
      "    # redirect the lines of text into the user supplied executable (mapper)\r\n",
      "    # and redirect STDOUT to a temporary file on disk\r\n",
      "    \r\n",
      "    \r\n",
      "    ################ YOUR CODE HERE #############\r\n",
      "    # Modify the line from pWordCount_v1.sh, so that  \r\n",
      "    # each chunk is individually sorted. \r\n",
      "    ./$mapper  < $datachunk | sort > $datachunk.counts &\r\n",
      "    \r\n",
      "    ################# (END YOUR CODE)###########\r\n",
      "done\r\n",
      "# wait for the mappers to finish their work\r\n",
      "wait\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "############## STEP 3: Collect the results #################\r\n",
      "# 'ls' makes a list of the temporary count files\r\n",
      "# 'perl -pe' regex replaces line breaks with spaces\r\n",
      "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\r\n",
      "\r\n",
      "# If no 'reducer' executable was provided ...\r\n",
      "if [ $# -eq 3 ]\r\n",
      "  then\r\n",
      "    # combine all the count files into one\r\n",
      "    cat $countfiles > $data.output\r\n",
      "fi\r\n",
      "\r\n",
      "# Otherwise...\r\n",
      "if [ $# -eq 4 ]\r\n",
      "  then\r\n",
      "    reducer=$4\r\n",
      "    ################ YOUR CODE HERE #############\r\n",
      "    # write one line to merge the individual chunks,\r\n",
      "    # pipe to the reducer script, and redirect to \r",
      "\r\n",
      "    # output file\r\n",
      "    #cat $countfiles | sort | python $reducer > $data.output\r\n",
      "    sort -m $countfiles| python $reducer > $data.output\r\n",
      "\r\n",
      "    ################# (END YOUR CODE)###########\r\n",
      "fi\r\n",
      "\r\n",
      "\r\n",
      "############## STEP 4: Final Output #################\r\n",
      "# clean up the data chunks and temporary count files\r\n",
      "\\rm $data.chunk.*\r\n",
      "# display the content of the output file:\r\n",
      "cat $data.output\r\n",
      "\r\n",
      "exit\r\n"
     ]
    }
   ],
   "source": [
    "!cat pWordCount_v2.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\r\n",
      "\"\"\"\r\n",
      "This script reads word counts from STDIN and aggregates\r\n",
      "the counts for any duplicated words.\r\n",
      "\r\n",
      "INPUT & OUTPUT FORMAT:\r\n",
      "    word \\t count\r\n",
      "USAGE (standalone):\r\n",
      "    python aggregateCounts_v2.py < yourCountsFile.txt\r\n",
      "\r\n",
      "Instructions:\r\n",
      "    For Q7 - Your solution should not use a dictionary or store anything   \r\n",
      "             other than a single total count - just print them as soon as  \r\n",
      "             you've added them. HINT: you've modified the framework script \r\n",
      "             to ensure that the input is alphabetized; how can you \r\n",
      "             use that to your advantage?\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "# imports\r\n",
      "import sys\r\n",
      "\r\n",
      "\r\n",
      "################# YOUR CODE HERE #################\r\n",
      "\r\n",
      "current_key = None\r\n",
      "current_val = None\r\n",
      "\r\n",
      "for line in sys.stdin:\r\n",
      "    # extract words & counts\r\n",
      "    word, count  = line.split()\r\n",
      "\r\n",
      "    if current_key == word:\r\n",
      "        current_value = current_value + int(count)\r\n",
      "    else:\r\n",
      "        if current_key != None:\r\n",
      "            print(\"{}\\t{}\".format(current_key, current_value))\r\n",
      "        current_key = word\r\n",
      "        current_value = int(count)\r\n",
      "            \r\n",
      "if current_key != None:\r\n",
      "    print(\"{}\\t{}\".format(current_key, current_value))\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "################ (END) YOUR CODE #################\r\n"
     ]
    }
   ],
   "source": [
    "!cat aggregateCounts_v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t2\r\n",
      "file\t3\r\n",
      "for\t1\r\n",
      "has\t1\r\n",
      "is\t2\r\n",
      "lines\t1\r\n",
      "small\t3\r\n",
      "test\t3\r\n",
      "this\t3\r\n",
      "two\t1\r\n"
     ]
    }
   ],
   "source": [
    "# part c - test your code on the test file (RUN THIS CELL AS IS)\n",
    "!./pWordCount_v2.sh 4 'data/alice_test.txt' \\\n",
    "                   'wordCount.py' \\\n",
    "                   'aggregateCounts_v2.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - run your code on the Alice file (RUN THIS CELL AS IS)\n",
    "!./pWordCount_v2.sh 4 'data/alice.txt' \\\n",
    "                   'wordCount.py' \\\n",
    "                   'aggregateCounts_v2.py' \\\n",
    "                   > 'data/alice_pCounts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice\t403\r\n"
     ]
    }
   ],
   "source": [
    "# part c - confirm that your 'alice' count is correct (RUN THIS CELL AS IS)\n",
    "!grep alice data/alice_pCounts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8: Scalability Considerations\n",
    "In your reading for Week 2's live session, [Chapter1, section 2](https://lintool.github.io/MapReduceAlgorithms/MapReduce-book-final.pdf) of _Data Intensive Text Processing with MapReduce_, Lin and Dyer discuss a number of \"Big Ideas\" that underlie large scale processing: __scale \"out,\" not \"up\"; assume failures are common; move processing to the data; process data sequentially and avoid random access; hide system-level details from the application developer; and seamless scalability.__ Part of our work this semester will be to interact with these ideas in a practical way, not just a conceptual one.\n",
    "\n",
    "### Q8 Tasks:\n",
    "* __a) short response:__ What do Lin and Dyer consider the two features of an \"ideal algorithm\" from a scalability perspective?\n",
    "\n",
    "\n",
    "* __b) short response:__ The mapper script below (created on the fly using a little Jupyter magic) will help us illustrate the concept of scalability. Run the provided code which passes this mapper script to our parallel computation 'framework' and runs the 'analysis' on the _Alice_ text file. Note that we've omitted a reducer for simplicity. What do you observe about the time it takes to run this \"algorithm\" when we use 1, 2 and 4 partitions? Does it meet Lin and Dyer's criteria?\n",
    "\n",
    "\n",
    "* __c) short response:__ Let's try something similar with your Word Count analysis. Run the provided code to time your implementation with 1, 2, 4 and 8 partitions. What do you observe about the runtimes? Does this match your expectation? Speculate about why we might be seeing these times. What conclusions should we draw about the scalability of our implementation? [`HINT:` _consider the limitations of both your machine and our implementation... there are some competing forces at work, what are they?_]\n",
    "\n",
    "\n",
    "* __d) OPTIONAL:__ Which components of your Map-Reduce algorithm are affected by a change in the number of partitions? Does increasing the number of partitions increase or decrease the total time spent on each of these portions of the task? What tradeoff does this cause?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8 Student Answers:\n",
    "\n",
    "> __a)__ <span style=\"font-family:Verdana; color: green;\"> Following are the two features: (1) In terms of data: given twice the amount of data, the same algorithm should take at most twice as long to run, all else being equal. (2) In terms of resources: given a cluster twice the size, the same algorithm should take no more than half as long to run. </span>\n",
    "\n",
    "> __b)__ <span style=\"font-family:Verdana; color: green;\"> When we run the provided mapper.py code (below) without the reducer. pWordCount_v2 shell script will run $n$ mappers in parallel and will finally wait for all the $n$ mappers to complete. The mapper as shown below scans the input line in the partition one line at a time and sleeps for 1ms. These mappers will run in parallel as long as there is a cpu to run. As my host has 8 cores, pWordCount_v2 script run with 1, 2, and 4 partitions in that order will finish progressively in shorter time (approximately 50% shorter than the previous run). As we noted below, it took 5.09 ms, 2.69 ms, and 1.48 ms respectively for pWordCount_v2 to finish in my host. </span>\n",
    "\n",
    "> __c)__ <span style=\"font-family:Verdana; color: green;\"> As we reviewed the code in pWordCount_v2 script, we read in the input document and partitions into smaller chunks. Later these chunks are passed as an input to the mapper. The mapper does the word count for the partition and streams the (word, count) pairs to the linux sort to basically sort the words alphabatically. Finally these sorted (word, count) pair list is passed to merge sort before it is streamed to the aggregator. Going by this logic, the timing should improve with the number of partitions as long as there are enough CPUs to run the mappers in parallel. However, we will experience slow down as the number of partitions is more than the number of cpus in the host we are running. I got the following timings in my macos laptop (mac mini with M1 chip 8 core, 16GB RAM). The following was the average times as output by the python timeit package 292ms, 281ms, 285ms, 323ms, 392ms, and 549ms for 1, 2, 4, 8, 16, and 32 processes count respectively. We saw a marginal time improvement for process counts 1, and 2. We noted a marginal performance degradation between process counts 2 and 4. This degradation can be attributed to the shuffle wait. The script is waiting for the all the 4 partitions to complete. We saw 13%, 37% and  93% slowdown in the 8, 16, and 32 processes run with respect to the timing for the 4 processes run. This could have been because as there are only 8 cores and any increase in process count will only increase the processing time due to processes waiting in the process queue for their turn to run. Also, the number of (word, count) pairs in the reducer will also increase with the number of partitions as same word can appear in more than one partition.</span>\n",
    "\n",
    "> __d)__ Type your (OPTIONAL) <span style=\"font-family:Verdana; color: green;\">  By reviewing the code, it is easy to realize that our mapper and reducer have a runtime complexity of $O(n)$ where the n the size of the input. The shuffle stage in the script will have to wait for $m$ partitions to complete and later merges the partitions to sorted sequence prior to streaming it to the reducer. So the mapper stage takes $O(n/m)$, where $n$ is the number of lines in the input document and $m$ is the number of partitions. The reducer time complexity is linear with the input and input is given by $O(w*m)$, where $w$ is the number of unique words and $m$ is the number of partitions. Finally, the shuffle time depends on the number of partitions $m$ and the number of words $w$ as well. Overall, all stages in map-reduce algorithm are impacted by the number of partitions. While the timing of mapper reduce with increase in $m$ and the shuffle and reducer timings will increase with the increase in number of partitions $m$. Thus the timings of these components (mapper, shuffle, and reducer) interplay to create a tradeoff.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run the following cells to create the mapper referenced in `part b`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing demo/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demo/mapper.py\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "This mapper reads from STDIN and waits 0.001 seconds per line.\n",
    "Its only purpose is to demonstrate one of the scalability ideas.\n",
    "\"\"\"\n",
    "import sys\n",
    "import time\n",
    "for line in sys.stdin:\n",
    "    time.sleep(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the mapper is executable\n",
    "!chmod a+x demo/mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run the next three cells to apply our demo mapper with 1, 2 and 4 partitions.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.09 s ± 3.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 1 'data/alice.txt' 'demo/mapper.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.69 s ± 2.81 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 2 'data/alice.txt' 'demo/mapper.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.48 s ± 16.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 4 'data/alice.txt' 'demo/mapper.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run the following cells to repeat this process with your word count algorithm.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291 ms ± 9.77 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 1 'data/alice.txt' 'wordCount.py' 'aggregateCounts_v2.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281 ms ± 4.91 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 2 'data/alice.txt' 'wordCount.py' 'aggregateCounts_v2.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283 ms ± 2.23 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 4 'data/alice.txt' 'wordCount.py' 'aggregateCounts_v2.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316 ms ± 3.51 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 8 'data/alice.txt' 'wordCount.py' 'aggregateCounts_v2.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401 ms ± 7.75 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 16 'data/alice.txt' 'wordCount.py' 'aggregateCounts_v2.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "553 ms ± 10.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 32 'data/alice.txt' 'wordCount.py' 'aggregateCounts_v2.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9: Embarrassingly Parallel\n",
    "`\"If any one of them can explain it,\" said Alice, (she had grown so large in the last few minutes that she wasn’t a bit afraid of interrupting him,) \"I’ll give him sixpence. I don’t believe there’s an atom of meaning in it.\"`\n",
    "<div style=\"text-align: right\">  -- Lewis Carroll, _Alice's Adventures in Wonderland_, Chapter 12  </div>\n",
    "\n",
    "### Q9 Tasks:\n",
    "\n",
    "* __a) short response:__ Describe what we mean by 'Embarrassingly Parallel' in terms of word counting. Does this term describe a 'task'? an 'implementation of a task'? \n",
    "\n",
    "* __b) OPTIONAL__: Define this concept in terms of 'associative' and 'commutative' operations. [`HINT:` _Refer to Chapter 2 in DITP_ ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9 Student Answers:\n",
    "\n",
    "> __a)__ <span style=\"font-family:Verdana; color: green;\"> A problem is considered embarassingly parallel if the problem is decomposable into many identical but separate subproblems. From that standpoint, the wordcounting problem can be broken into smaller independent subproblems by breaking the input into $n$ partitions. Words in the each partition can be counted and sorted in the alphabetic order of the words independently and then the output from these mappers will be streamed to a reducer through a shuffle stage and can be merged into one sorted sequence of (word, count) pairs as we did in the pWordCount_v2 python script. Hence, the wordcounting problem is an 'Embarrassingly Parallel' problem. An 'Embarassingly Parallel' problem may or may not be easily implemented by breaking the problem into smaller and independent subproblems. From the defintion of 'Embarassingly Parallel' described here  [https://en.wikipedia.org/wiki/Embarrassingly_parallel], an 'Embarassingly Parallel' problem is one where little or no effort is needed to separate the problem into a number of parallel tasks. A task is a description of steps at an higher level rather than a very prescriptive algorithm of an implementation.</span>\n",
    "\n",
    "> __b)__ <span style=\"font-family:Verdana; color: green;\"> Associativity lends itself to paralleization naturally. For example, if the operation is associative then we can have each thread calculate a partial sum. Finally, the partial sums can be aggregated in to a final sum. If the operation is commutative then the final aggregation can be calculated in any order. Otherwise the partial sums have to be aggregated in that order. Aggregating in any order can be more efficient because it's often difficult to have each thread to finish in that order. For example, the word counting problem above our aggregator_v2 (we dont use the hashmap) is non-commutative. We needed the wordcount in the sorted order. We modified the aggregator and the pWordCount (line 73, pWordCount_v2.sh) to avoid the scalability problem observed in problem 6. This modification makes our aggregator a non-commutative operation.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations, you have completed HW1! Please refer to the readme for submission instructions.\n",
    "\n",
    "If you would like to provide feedback regarding this homework, please use the survey at: https://docs.google.com/forms/d/e/1FAIpQLSce9feiQeSkdP43A0ZYui1tMGIBfLfzb0rmgToQeZD9bXXX8Q/viewform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "297px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "951px",
    "left": "0px",
    "right": "1561px",
    "top": "106px",
    "width": "600px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
